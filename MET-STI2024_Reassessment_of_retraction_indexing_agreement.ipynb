{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6cf5a8a",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03faaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "#import dataframe_image as dfi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.patches as mpatches\n",
    "\n",
    "from datetime import date, datetime as dt\n",
    "import time,datetime\n",
    "import re\n",
    "import unicodedata\n",
    "import ast  # Module to handle literal_eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targeting the retraction_index_path\n",
    "retraction_index_path = os.path.abspath('./retraction-indexing-agreement/')\n",
    "retraction_index_path\n",
    "\n",
    "data_dir = retraction_index_path+'/dataset/' # data directory\n",
    "result_dir = retraction_index_path +'/result/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42802a",
   "metadata": {},
   "source": [
    "#### Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4627c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "con_file = open(retraction_index_path+\"\\config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()\n",
    "\n",
    "# Initializing variable for configuration file\n",
    "my_email = config['my_email']\n",
    "elsevier_api_key = config['Elsevier_APIKEY']\n",
    "elsevier_insttoken = config['insttoken']\n",
    "ieee_xplore_api_key = config['IEEEXplore_APIKEY']\n",
    "wos_api_key = config['WoS_APIKEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9898e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global initializatiion\n",
    "global my_email\n",
    "global elsevier_api_key\n",
    "global elsevier_insttoken\n",
    "global ieee_xplore_api_key\n",
    "global wos_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce51c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3da4c",
   "metadata": {},
   "source": [
    "### Creating Present Union list As At July2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d597d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input the date you retrieve retraction publications for each database\n",
    "\n",
    "Update the date for each database format: YYYY-MM-DD e.g. 2024-02-13\n",
    "\"\"\"\n",
    "\n",
    "getdate = {'scopus': '2024-07-05',\n",
    "            'crossref':'2024-07-03',\n",
    "            'retractionwatch': '2024-07-03',\n",
    "            'webofsciencecore': '2024-07-03'} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525da2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unicode(string: str) -> str:\n",
    "    \"\"\"\n",
    "    It takes a string and passes it through different encoding parameter phases\n",
    "    E.g. '10.\\u200b1105/\\u200btpc.\\u200b010357' ->  '10.1105/tpc.010357'\n",
    "    \n",
    "    :param string: variable to be encoded\n",
    "    :return: the actual string value devoided of encoded character\n",
    "    \"\"\"\n",
    "    \n",
    "    string = unicodedata.normalize('NFKD', string).encode('iso-8859-1', 'ignore').decode('iso-8859-1')\n",
    "    string = unicodedata.normalize('NFKD', string).encode('latin1', 'ignore').decode('latin1')\n",
    "    string = unicodedata.normalize('NFKD', string).encode('cp1252', 'ignore').decode('cp1252')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968415b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in Crossref file and renaming some columns\n",
    "\"\"\"\n",
    "crossref = pd.read_csv(data_dir+'crossref/crossref_retractedpublication_'+\\\n",
    "                     getdate['crossref']+'.csv').drop('Unnamed: 0',axis=1)\n",
    "crossref['source'] = 'Crossref'\n",
    "\n",
    "crossref['DOI']= crossref['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "crossref.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in Retraction Watch items\n",
    "\"\"\"\n",
    "\n",
    "retractionwatch = pd.read_csv(data_dir+'/retractionwatch/retractionwatch_'+\\\n",
    "                     getdate['retractionwatch']+'.csv', encoding='latin1').rename(\n",
    "    columns={'OriginalPaperDOI':'DOI', \n",
    "             'OriginalPaperPubMedID': 'PubMedID', \n",
    "             'OriginalPaperDate': 'Year'})\n",
    "\n",
    "retractionwatch['source']='Retraction Watch'\n",
    "\n",
    "\n",
    "retractionwatch['PubMedID']= retractionwatch['PubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str).str.strip()\n",
    "\n",
    "retractionwatch['RetractionPubMedID']= retractionwatch['RetractionPubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str)\n",
    "\n",
    "retractionwatch['Year']=  pd.to_datetime(retractionwatch['Year'], exact=False).dt.year\n",
    "retractionwatch['RetractionYear']=  pd.to_datetime(retractionwatch['RetractionDate'], exact=False).dt.strftime(\"%Y\").fillna(0).astype(int)\n",
    "\n",
    "# A row in retraction Watch contains '|' from a row in \n",
    "retractionwatch['DOI']= retractionwatch['DOI'].str.replace(r'\\|', '') # \"10.1038/embor.2009.88 |\"\n",
    "\n",
    "retractionwatch['DOI'] = retractionwatch['DOI'].str.lower().fillna('').str.strip().apply(convert_unicode)\n",
    "\n",
    "retractionwatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07accbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in Scopus file and renaming some columns\n",
    "\"\"\"\n",
    "scopus = pd.read_csv(data_dir+'scopus/scopus_retractedpublication_'+\\\n",
    "                     getdate['scopus']+'.csv').rename(\n",
    "    columns={'Authors':'Author',\n",
    "             'Source':'source',\n",
    "             'Titles':'Title',\n",
    "             'Source title':'Journal',\n",
    "             'PubMed ID': 'PubMedID'})\n",
    "\n",
    "\n",
    "\n",
    "# Converting 'Publication_date' to Date to extract the year\n",
    "scopus['Publication_date'] = pd.to_datetime(scopus['Publication_date'])\n",
    "scopus['Year'] = scopus['Publication_date'].dt.year\n",
    "\n",
    "scopus['DOI']= scopus['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "\n",
    "scopus['PubMedID']= scopus['PubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str).str.strip()\n",
    "\n",
    "scopus['source'] = 'Scopus'\n",
    "\n",
    "scopus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading in indexed retracted publications from Web of Science Core\n",
    "\"\"\"\n",
    "\n",
    "webofsciencecore= pd.read_csv(data_dir+f'webofscience/webofsciencecore_retractedpublication_'+\\\n",
    "                     getdate['webofsciencecore']+'.csv')\n",
    "\n",
    "webofsciencecore['source'] = 'WoS_Core'\n",
    "\n",
    "webofsciencecore['DOI']= webofsciencecore['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "\n",
    "# Remove decimal part\n",
    "webofsciencecore['PubMedID'] = webofsciencecore['PubMedID'].fillna('').astype(str).str.split('.').str[0]\n",
    "\n",
    "webofsciencecore.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkindividualdataset(x)\n",
    "    \"\"\"\n",
    "    :param x: dataframe object\n",
    "    Clean and deplicate records based on DOIs.\n",
    "    After removing duplicates, we will return the count and the list of records with DOI, \n",
    "    those without DOIs and duplicated records that will be dropped.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting the DataFrame name\n",
    "    df_name = [name for name, obj in globals().items() if obj is x][0]\n",
    "\n",
    "    \n",
    "    # Step 1: We identify the unique records of each dataset based on DOI.\n",
    "    # 'records_withDOI_hasdup': Identify records that have a valid DOI which should start with '10.'\n",
    "    # 'records_withDOI': Drop the duplicates from the previous line.\n",
    "    \n",
    "    records_withDOI_hasdup= x.loc[x['DOI'].str.startswith('10.', na=False)]\n",
    "    \n",
    "    if df_name == 'pubmed':\n",
    "        records_withDOI = records_withDOI_hasdup.drop_duplicates(subset=['DOI'], keep='last')\n",
    "    else:\n",
    "        records_withDOI = records_withDOI_hasdup.drop_duplicates(subset=['DOI'], keep='first')\n",
    "\n",
    "    \n",
    "    # Step 2: We create two duplicate lists.\n",
    "    # 'duplicated_records_all': Identify ALL duplicated records for reference and download for checking manually.\n",
    "    # 'duplicated_records': Identify duplicated records to drop but keep only the first occurrence of each group of duplicates.\n",
    "    duplicate_records_all = records_withDOI_hasdup.loc[records_withDOI_hasdup.duplicated(subset=['DOI'],keep=False), :]\n",
    "    \n",
    "    \n",
    "    # To check anamolies - to remove record with PubMedID 28202934 and leave that 26511294 with correct date\n",
    "    if df_name == 'pubmed':\n",
    "        duplicate_records = records_withDOI_hasdup.loc[records_withDOI_hasdup.duplicated(subset=['DOI'],keep='last'), :]\n",
    "    else:\n",
    "        duplicate_records = records_withDOI_hasdup.loc[records_withDOI_hasdup.duplicated(subset=['DOI'],keep='first'), :]\n",
    "\n",
    "\n",
    "    \n",
    "    # Step 3: We get the count of records without DOI. Duplicates may exist since we could not use DOI to identify duplicate records\n",
    "    records_withoutDOI = x.loc[~x['DOI'].str.startswith('10.', na=False)]    \n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Return the count and items of each group\n",
    "        if len(x) == len(records_withDOI)+len(records_withoutDOI)+len(duplicate_records):\n",
    "            return[len(x), len(records_withDOI),len(records_withoutDOI),len(duplicate_records), records_withDOI, records_withoutDOI, duplicate_records, duplicate_records_all]\n",
    "            \n",
    "    \n",
    "    except: \n",
    "        return('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_DOI_n_PubMedID(df, source)-> list:\n",
    "    \"\"\"\n",
    "    :param df: DataFrame to work on\n",
    "    :param source: source to look up to determine number of count\n",
    "    \n",
    "    :return: source, # DOI, # PubMedID, # Duplicated record -> list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_DOI= df[(df['DOI'].str.startswith('10')) & (df['source'].str.contains(source))]\n",
    "    \n",
    "    df_nodupDOI= df_DOI.drop_duplicates(subset=['DOI'], keep='first') # DF that has no duplicated DOI\n",
    "    \n",
    "    df_duplicatedDOI= df_DOI[df_DOI.duplicated(subset=['DOI'],keep='last')] # DF that are duplicated\n",
    "    \n",
    "    \n",
    "    df_noDOI= df[(~df['DOI'].str.startswith('10')) & (df['source'].str.contains(source))] # DF that has no DOI\n",
    "    \n",
    "    \n",
    "    nDOI= len(df_nodupDOI) # Numbers of items with unique DOI\n",
    "    nDuplicatedDOI= len(df_duplicatedDOI) # Numbers of items that has duplicated DOI removed\n",
    "    nNoDOI= len(df_noDOI) # Numbers of items without DOI\n",
    "    \n",
    "    \n",
    "    \n",
    "    if 'PubMedID' in df.columns:\n",
    "        \n",
    "        df_PMID= df_DOI[((df_DOI['PubMedID'] != \"\") | ~df_DOI['PubMedID'].isna()) & (df_DOI['source'].str.contains(source))]\n",
    "        \n",
    "        df_nodupPMID= df_PMID.drop_duplicates(subset=['DOI'], keep='first') # DF that has no duplicated PMID\n",
    "        \n",
    "        df_duplicatedPMID= df_PMID[df_PMID.duplicated(subset=['DOI'],keep='last')] # DF that has duplicated PMID\n",
    "        \n",
    "        df_noPMID= df[~(((df['PubMedID'] != \"\") | ~df['PubMedID'].isna()) & (df['source'].str.contains(source)))]\n",
    "        \n",
    "        \n",
    "        nPMID= len(df_nodupPMID) # Numbers of items with unique PMID\n",
    "        nDuplicatedPMID= len(df_duplicatedPMID)  # Numbers of items with duplicated PMID\n",
    "        nNoPMID= len(df_noPMID)  # Numbers of items with without PMID\n",
    "        \n",
    "    else:\n",
    "        nPMID,nDuplicatedPMID,nNoPMID= 0,0,0\n",
    "        \n",
    "        \n",
    "    Total= len(df)\n",
    "        \n",
    "    return  source, Total, nDOI, nNoDOI, nDuplicatedDOI, nPMID, nNoPMID, nDuplicatedPMID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab9668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Counting # DOIs, PubMedID\n",
    "#  return: source,Total, nDOI,nNoDOI,nDuplicatedDOI,nPMID,nNoPMID,nDuplicatedPMID \n",
    "\"\"\"\n",
    "nCrossref= count_DOI_n_PubMedID(crossref,'Crossref')\n",
    "nRW= count_DOI_n_PubMedID(retractionwatch,'Retraction Watch')\n",
    "nScopus= count_DOI_n_PubMedID(scopus,'Scopus')\n",
    "nWoS=count_DOI_n_PubMedID(webofsciencecore,'WoS_Core')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nCrossref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb23f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nRW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed343f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nScopus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nWoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7c604",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aggregate the items from all the sources\n",
    "\"\"\"\n",
    "dbtable = [] # A nested list which stores the records of each group in each source\n",
    "ovtable = [] # Store the count of each group from each source and create a table for viewing\n",
    "\n",
    "dblist= [nCrossref, nRW, nScopus, nWoS]\n",
    "\n",
    "# Query results retrieved\tRecords with DOI\tRecords without DOI removed\tDuplicate records removed\n",
    "# source,Total, nDOI,nNoDOI,nDuplicatedDOI,nPMID,nNoPMID,nDuplicatedPMID\n",
    "for result in dblist:\n",
    "    dbtable.append(result)\n",
    "    \n",
    "    np_results= np.array(dbtable)\n",
    "    \n",
    "# Create a table showing the count of each group\n",
    "overview = pd.DataFrame(np_results[:,[1,2,3,4,5]])\n",
    "overview.columns =['Query_result', 'Records_withDOI', 'Records_withoutDOI', 'Duplicate_DOI_removed', 'DOI_records_withPubMedID']\n",
    "overview['source']= ['Crossref','Retraction Watch','Scopus','Web of Science Core']\n",
    "\n",
    "# Re-order column\n",
    "overview = overview[['source', 'Query_result', 'Records_withDOI', 'Records_withoutDOI', 'Duplicate_DOI_removed', 'DOI_records_withPubMedID']]\n",
    "\n",
    "# Aggregating items in each column\n",
    "overview.loc[len(overview)] = ['Total',overview.Query_result.astype(int).sum(), \n",
    "                               overview.Records_withDOI.astype(int).sum(),\n",
    "                               overview.Records_withoutDOI.astype(int).sum(), \n",
    "                               overview.Duplicate_DOI_removed.astype(int).sum(), \n",
    "                               overview.DOI_records_withPubMedID.astype(int).sum(),] \n",
    "\n",
    "\n",
    "\n",
    "overview\\\n",
    "#         .to_csv(result_dir+'datasources_overview_unionlist_crws.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f972c",
   "metadata": {},
   "source": [
    "###### Exporting Records to Folder\n",
    "- These include DOIs, deduplicated DOIs, and No DOI records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets_from_sources(source_list: list, dbtable: list):\n",
    "    \"\"\"\n",
    "    This function filter records with doi,without doi, and those with duplicated doi\n",
    "    for each source and save each file\n",
    "    \n",
    "    :param source_list: list of dataframe of sources. This is dblist. It is initiated later\n",
    "    :return: It will for each source return: \n",
    "            - records with doi (saved as in this format: 'source_name_recordswithdoi_date')\n",
    "            - records without doi (saved as in this format: 'source_name_recordsnodoi_date')\n",
    "            - duplicated doi records (saved as in this format: 'source_name_duplicatedrecords_date')\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for i in range(len(source_list[:-1])):\n",
    "        source_name = [name for name, obj in globals().items() if obj is source_list[i]][0]\n",
    "\n",
    "        date = getdate[source_name]\n",
    "        \n",
    "        print(f'{source_name} was last updated on {date}')\n",
    "        \n",
    "        if source_name in ['webofsciencecore']:\n",
    "            dbtable[i][4].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+ 'webofscience'+'/'+source_name+'_recordswithdoi_'+date + '.csv')\n",
    "            dbtable[i][5].to_csv(data_dir+ 'webofscience'+'/'+source_name+'_recordsnodoi_' + date + '.csv')\n",
    "            dbtable[i][6].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+ 'webofscience'+'/'+source_name+ '_duplicatedrecords_' + date + '.csv')\n",
    "\n",
    "      \n",
    "        else:\n",
    "            dbtable[i][4].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+source_name+'/'+source_name+'_recordswithdoi_'+date + '.csv')\n",
    "            dbtable[i][5].to_csv(data_dir+ source_name+'/'+source_name+'_recordsnodoi_' + date + '.csv')\n",
    "            dbtable[i][6].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+ source_name+'/'+source_name+ '_duplicatedrecords_' + date + '.csv')\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eabbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving records of items with DOIs, without DOIs, and duplicated records in each of the sources\n",
    "\n",
    "dbtable2 = [] # A nested list which stores the records of each group in each source\n",
    "dblist2 = [crossref,retractionwatch, scopus, webofsciencecore,'']\n",
    "\n",
    "for x in dblist2[:-1]:\n",
    "    dbtable2.append(checkindividualdataset(x))\n",
    "\n",
    "export_datasets_from_sources(dblist2[:], dbtable2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting Records without DOI\n",
    "\"\"\"\n",
    "for record in dbtable:\n",
    "    source = record[0]\n",
    "    if record[0]=='WoS_Core':\n",
    "        source = 'Web of Science'\n",
    "    print(f\"The total # records without DOI in {source} is {record[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f092cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading records lacking DOIs\n",
    "\"\"\"\n",
    "\n",
    "nodoi_crossref_df= pd.read_csv(data_dir+'crossref/crossref_recordsnodoi_'+ getdate['crossref'] + '.csv')\n",
    "nodoi_scopus_df= pd.read_csv(data_dir+'scopus/scopus_recordsnodoi_'+ getdate['scopus'] + '.csv')\n",
    "nodoi_rw_df= pd.read_csv(data_dir+'retractionwatch/retractionwatch_recordsnodoi_'+ getdate['retractionwatch'] + '.csv')\n",
    "nodoi_wos_df= pd.read_csv(data_dir+'webofscience/webofsciencecore_recordsnodoi_'+ getdate['webofsciencecore'] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fe2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating Records with PMIDs lacking DOIs in the newly-added records\n",
    "\n",
    "pmid_nodoi_srw  <- is the PMIDs of records that lack DOIs in union list of Retraction Watch, Web of Sciece, and Scopus. Crossref always has DOI.\n",
    "sti_nodoi_pmids_filtered <- is the PMIDs of records that lack DOIs in STI2023 union list \n",
    "\"\"\"\n",
    "\n",
    "nodoi_scopus_df['PubMedID']= nodoi_scopus_df['PubMedID'].fillna(0).astype(int).astype(str)\n",
    "nodoi_rw_df['PubMedID']= nodoi_rw_df['PubMedID'].fillna(0).astype(int).astype(str)\n",
    "nodoi_wos_df['PubMedID']= nodoi_wos_df['PubMedID'].fillna(0).astype(int).astype(str)\n",
    "\n",
    "\n",
    "pmid_nodoi_srw =set(nodoi_scopus_df[nodoi_scopus_df['PubMedID']!='0']['PubMedID'].tolist()+\\\n",
    "                    nodoi_rw_df[nodoi_rw_df['PubMedID']!='0']['PubMedID'].tolist() +\\\n",
    "                    nodoi_wos_df[nodoi_wos_df['PubMedID']!='0']['PubMedID'].tolist())\n",
    "\n",
    "print(f\"The total # PMIDs of records lacking DOIs is {len(pmid_nodoi_srw)} in the union list \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece90b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Laoding retracted records with DOI\n",
    "\"\"\"\n",
    "crossref_retracted= pd.read_csv(data_dir+'crossref/crossref_recordswithdoi_'+getdate['crossref'] +'.csv')\n",
    "\n",
    "retractionwatch_retracted= pd.read_csv(data_dir+'retractionwatch/retractionwatch_recordswithdoi_'+getdate['retractionwatch'] +'.csv')\n",
    "\n",
    "scopus_retracted = pd.read_csv(data_dir+'scopus/scopus_recordswithdoi_'+getdate['scopus']+'.csv')\n",
    "\n",
    "webofsciencecore_retracted = pd.read_csv(data_dir+'webofscience/webofsciencecore_recordswithdoi_'+getdate['webofsciencecore'] +'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e509191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged into one full list 'The Union List'\n",
    "\n",
    "\"\"\"\n",
    "Select PubMed with doi and \n",
    "Select the columns: 'DOI', 'Author','Title', 'Year', 'Journal', 'source', 'PubMedID' -> 'union_list'\n",
    "\"\"\"\n",
    "retracted_sources= [crossref_retracted, retractionwatch_retracted, scopus_retracted, webofsciencecore_retracted]\n",
    "\n",
    "merged_withdoi = pd.concat(retracted_sources)\n",
    "merged_withdoi = merged_withdoi[['DOI', 'Author','Title', 'Year', 'Journal', 'source', 'PubMedID']].sort_values(by='DOI')\n",
    "\n",
    "# Check if the number of records are consistent before and after merging.\n",
    "if len(merged_withdoi) == (len(crossref_retracted) +len(retractionwatch_retracted)+len(scopus_retracted)+ len(webofsciencecore_retracted)):\n",
    "    print('full record count:', len(merged_withdoi))\n",
    "else:\n",
    "    print('ERROR: Inconsistent Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23564e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_withdoi['source']=merged_withdoi['source'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It merges file from all the sources using their DOIs, and joins the source that of the items\n",
    "Output File: One merged CSV file as final output - \n",
    "\n",
    "Uses date as of today (last update): '2023-07-09 to indicate last update the items were merged\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "unionlist_crws = merged_withdoi.groupby('DOI').agg({'Author':'first', \n",
    "                              'Title': 'last',\n",
    "                              'Year': 'first',\n",
    "                              'Journal': 'last',\n",
    "                              'source':'; '.join, \n",
    "                              'PubMedID':'first'}).reset_index()\n",
    "\n",
    "unionlist_crws['PubMedID']= unionlist_crws['PubMedID'].fillna(0).astype(int).replace(0,'').astype(str)\n",
    "unionlist_crws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get number of unique PMID\n",
    "\"\"\"\n",
    "print(f\"The total # of unique PMID in the union list is {unionlist_crws['PubMedID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83e23e",
   "metadata": {},
   "source": [
    "#### Loading coverage items not indexed for each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f2ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input the date you retrieve retraction publications for each database\n",
    "\n",
    "Update the date for each database format: YYYY-MM-DD e.g. 2024-02-13\n",
    "\"\"\"\n",
    "\n",
    "date_coverage = {'scopus': '2024-08-02',\n",
    "            'crossref':'2024-08-06',\n",
    "            'webofsciencecore': '2024-07-30'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69460aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get coverage for the sources - -\n",
    "Read in items that were covered but not indexed as retracted publication from each sources <- _notindexed\n",
    "\"\"\"\n",
    "\n",
    "crossref_notindexed= pd.read_csv(data_dir+'coverednotindexed/crossref_coverednotindexed_'+date_coverage['crossref']+'.csv').drop('Unnamed: 0',axis=1)\n",
    "crossref_notindexed['source']='Crossref'\n",
    "\n",
    "scopus_notindexed= pd.read_csv(data_dir+'coverednotindexed/scopus_coverednotindexed_'+date_coverage['scopus']+'.csv').drop('Unnamed: 0',axis=1)\n",
    "scopus_notindexed['source']='Scopus'\n",
    "\n",
    "woscore_notindexed= pd.read_csv(data_dir+'coverednotindexed/webofsciencecore_coverednotindexed_'+date_coverage['webofsciencecore']+'.csv')\n",
    "woscore_notindexed['source']='WoS_Core'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89068226",
   "metadata": {},
   "outputs": [],
   "source": [
    "unionlist_indexed= unionlist_crws.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e29604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge all the items covered but not indexed as retracted publication from each source with the union list\n",
    "<- unionlist_covers\n",
    "\"\"\"\n",
    "\n",
    "merged_withdoi2 = pd.concat([unionlist_indexed, crossref_notindexed,scopus_notindexed,\n",
    "                           woscore_notindexed,])\n",
    "\n",
    "unionlist_covers= merged_withdoi2.groupby('DOI').agg({'Author':'first', \n",
    "                              'Title': 'first',\n",
    "                              'Year': 'first', \n",
    "                              'Journal': 'first',\n",
    "                              'PubMedID':'first',\n",
    "                              'source':'; '.join \n",
    "                              }).reset_index()\n",
    "\n",
    "unionlist_covers.fillna('', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde216a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unionlist_covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deduplicate the 'source' in the unionlist_covers (i.e. that entails covered all items and \n",
    "call the source field -> 'source_new'.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Deduplicating source value \n",
    "unionlist_covers['source'] = unionlist_covers['source'].apply(lambda x: '; '.join(sorted(set(x.split('; ')))))\n",
    "\n",
    "# Renaming column\n",
    "unionlist_covers.rename(columns={'source': 'source_new'}, inplace=True)\n",
    "unionlist_covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge unionlist_covers (that details covers items) and unionlist_indexed (that detailed indexed retracted items)\n",
    "Call source in unionlist_indexed -> 'source_old'\n",
    "\"\"\"\n",
    "\n",
    "unionlist_crws_covered_and_indexed = pd.merge(unionlist_covers, unionlist_indexed[['DOI', 'source', ]].rename(columns={'source': 'source_old'}), on='DOI')\n",
    "\n",
    "# Store 'source_old' & 'source_new' value as sorted list\n",
    "unionlist_crws_covered_and_indexed['source_old'] = unionlist_crws_covered_and_indexed['source_old'].apply(lambda x: sorted(x.split('; ')))#.astype(str)\n",
    "unionlist_crws_covered_and_indexed['source_new'] = unionlist_crws_covered_and_indexed['source_new'].apply(lambda x: sorted(x.split('; ')))#.astype(str)\n",
    "\n",
    "unionlist_crws_covered_and_indexed['Year']= unionlist_crws_covered_and_indexed['Year'].astype(int)\n",
    "\n",
    "unionlist_crws_covered_and_indexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a286dcd",
   "metadata": {},
   "source": [
    "#### Calculating RetractionIndexingAgreement Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5afc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating Retraction Indexing Agreement\n",
    "\"\"\"\n",
    "count_sources_covering_item = unionlist_crws_covered_and_indexed.source_new.apply(len)\n",
    "\n",
    "count_sources_indexing_item_as_retracted = unionlist_crws_covered_and_indexed.source_old.apply(len)#str.len()\n",
    "RetractionIndexingDiscrepancy_ITEM = count_sources_indexing_item_as_retracted / count_sources_covering_item\n",
    "\n",
    "# add and show the calculation score to dataframe\n",
    "unionlist_crws_covered_and_indexed['RetractionIndexingAgreement_ITEM(%)'] = ((RetractionIndexingDiscrepancy_ITEM) * 100).astype(int)\n",
    "\n",
    "unionlist_crws_covered_and_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "unionlist_crws_covered_and_indexed.to_csv(result_dir + 'unionlist_crws_ria_2024-07-09.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_source(df: pd.DataFrame, source_name: str)-> tuple:\n",
    "    \"\"\"\n",
    "    :param df: DataFrame to work on\n",
    "    :param source_name: source to lookup to determine number of count\n",
    "    \n",
    "    :return: source, # no_inOLD, no_inNEW -> list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    no_inOLD= len(set(df[(df['source_old'].apply(lambda x: source_name in x))]['DOI'])) #.count()[0]\n",
    "    \n",
    "    no_inNEW= len(set(df[(df['source_new'].apply(lambda x: source_name in x))]['DOI'])) #.count()[0]\n",
    "\n",
    "    \n",
    "    return  no_inOLD,no_inNEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Counting # items indexed and covered in each sources\n",
    "\"\"\"\n",
    "\n",
    "crossref_count= count_source(unionlist_crws_covered_and_indexed, 'Crossref')\n",
    "\n",
    "scopus_count= count_source(unionlist_crws_covered_and_indexed, 'Scopus')\n",
    "\n",
    "webofsciencecore_count= count_source(unionlist_crws_covered_and_indexed, 'WoS_Core')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Total Indexed & Coverage\n",
    "\"\"\"\n",
    "print(f\" In Crossref, the total indexed items is {crossref_count[0]} and the total coverage is {crossref_count[1]}\")\n",
    "print(f\" In Scopus, the total indexed items is {scopus_count[0]} and the total coverage is {scopus_count[1]}\")\n",
    "print(f\" In Web of Science, the total indexed items is {webofsciencecore_count[0]} and the total coverage is {webofsciencecore_count[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating # items not indexed as retracted for each source -> not_covered\n",
    "\"\"\"\n",
    "not_covered= [len(unionlist_crws_covered_and_indexed) - crossref_count[1],\n",
    "              len(unionlist_crws_covered_and_indexed) - scopus_count[1],\n",
    "              len(unionlist_crws_covered_and_indexed) - webofsciencecore_count[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Computing the Retraction Indexing Agreement score for sources (indexed sources only)\n",
    "- Putting results of # indexed, coverednotindexed, not_covered\n",
    "\"\"\"\n",
    "\n",
    "table = pd.DataFrame()\n",
    "table['source']= ['Crossref', 'Scopus', 'Web of Science Core']\n",
    "table['indexed_as_retracted']= [crossref_count[0],scopus_count[0],webofsciencecore_count[0]]\n",
    "\n",
    "table['covered_but_not_indexed_as_retracted'] = [crossref_count[1]-crossref_count[0],\n",
    "                                                 scopus_count[1]-scopus_count[0],\n",
    "                                                 webofsciencecore_count[1]-webofsciencecore_count[0]]\n",
    "\n",
    "table['not_covered'] = not_covered\n",
    "\n",
    "ria_source=[]\n",
    "\n",
    "for i in range(0,3):\n",
    "    D = table['indexed_as_retracted'][i] + table['covered_but_not_indexed_as_retracted'][i]   \n",
    "    ria_source.append(round(((table['indexed_as_retracted'][i]/D)*100), 2))\n",
    "\n",
    "table['RetractionIndexingAgreement_SOURCE(%)'] = [ria_source[0], ria_source[1], ria_source[2]]\n",
    " #set first column as index\n",
    "table = table.sort_values(by='source')\n",
    "table = table.reset_index()\n",
    "table =table.drop('index',axis=1)\n",
    "table\\\n",
    "    #.to_csv(result_dir+'ria_scores_unionlist_crws.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575485fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview\\\n",
    "#         .to_csv(result_dir+'datasources_overview_unionlist_crws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16378361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load STI2023 union list\n",
    "\"\"\"\n",
    "unionlist_sti = pd.read_csv(data_dir+\"/sti2023/2023-09-03_journalcategory_knownretractionlist_updated.csv\").drop(['Unnamed: 0'],axis=1)\n",
    "unionlist_sti['DOI']= unionlist_sti['DOI'].str.lower()\n",
    "unionlist_sti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d51c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading PMIDs of records with no DOI from STI: Crossref (0), Web of Science (276), Retraction Watch (682), Scopus (8): 966\n",
    "\n",
    "sti_nodoi_pmids <- publication records with noDOI from dataset that was used for STI2023 union list\n",
    "\"\"\"\n",
    "\n",
    "with open(data_dir+\"sti2023/nodoi_sti_pmids.txt\",'r') as fn:#.drop(['Unnamed: 0'],axis=1)\n",
    "    sti_nodoi_pmids = fn.read().split('\\n')\n",
    "    sti_nodoi_pmids.remove('')\n",
    "    \n",
    "print(f'The total unique # of records lacking DOIs but having PMIDs is {len(sti_nodoi_pmids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f74035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filtering PMIDs of NoDOI items for STI2023 not in STI2023 Union list - \n",
    "to remove pre-existing PMID from STI2023 union list\n",
    "\"\"\"\n",
    "unionlist_sti['PubMedID']=unionlist_sti['PubMedID'].fillna('0').astype(int).astype(str)\n",
    "\n",
    "# Getting PMIDs in STI2023\n",
    "sti_nodoi_pmids_filtered = set(sti_nodoi_pmids) - set(unionlist_sti['PubMedID'].tolist())\n",
    "print(f'Actual records with noDOI with PMID for STI2023 formation:  {len(sti_nodoi_pmids_filtered )}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2291843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Records with NoDOI but having PMIDs [from Web of Science (276), Retraction Watch (682), Scopus (8)]\n",
    "for STI2023 unionlist in unionlist_crws\n",
    "\"\"\"\n",
    "\n",
    "nodoi_sti_found_df= unionlist_crws_covered_and_indexed[(unionlist_crws_covered_and_indexed['PubMedID'] != '') & \\\n",
    "                                                       (unionlist_crws_covered_and_indexed['PubMedID'].isin(sti_nodoi_pmids_filtered))]\n",
    "\n",
    "print(f'Of 657 NoDOI_withPMID records used for creating STI2023 unionlist, \\\n",
    "only {len(nodoi_sti_found_df)} items were found in Unionlist_CRWS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb901dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Breakdown analysis in Year 2023 & 2024 in STI2023 & Unionlist_CRWS\n",
    "\"\"\"\n",
    "n_sti2023 = unionlist_sti[unionlist_sti['Year']==2023].count().iloc[0]\n",
    "n_ul_crws2023 = unionlist_crws_covered_and_indexed[unionlist_crws_covered_and_indexed['Year'] == 2023].count().iloc[0]\n",
    "n_ul_crws2024 = unionlist_crws_covered_and_indexed[unionlist_crws_covered_and_indexed['Year'] == 2024].count().iloc[0]\n",
    "\n",
    "\n",
    "print(f'The # of DOIs indexed as retracted in STI2023 union list in 2023 (Jan - April): {n_sti2023}')\n",
    "print(f'The # of DOIs indexed as retracted in unionlist_crws in 2023 (Jan - Dec): {n_ul_crws2023}')\n",
    "print(f'The # of DOIs indexed as retracted in unionlist_crws in 2024 (Jan - July): {n_ul_crws2024}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754f78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comparing Total # retracted item in Crossref, Scopus, Retraction Watch, Web of Science Core compared with STI2023\n",
    "Newly added DOIs from STI2023 to unionlist_crws 2023\n",
    "\"\"\"\n",
    "\n",
    "newly_added_crws = unionlist_crws_covered_and_indexed[~unionlist_crws_covered_and_indexed['DOI'].isin(unionlist_sti['DOI'])].copy()\n",
    "newly_added_crws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608696bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finding DOIs in union list 2024 and not in STI union list: Actual newly added DOIs in Unionlist_crws\n",
    "\"\"\"\n",
    "diff_notin_sti_doi= set(unionlist_crws_covered_and_indexed['DOI']) - set(unionlist_sti['DOI'])\n",
    "print(f\"The total DOIs in union list CRWS 2024 and not in STI2023 is: {len(diff_notin_sti_doi)}\")\n",
    "print(f\"{(len(diff_notin_sti_doi)/len(unionlist_sti))*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_added_doi_withpmid = newly_added_crws[newly_added_crws['PubMedID']!='']\n",
    "new_added_doi_withpmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2e8ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Quality Check:\n",
    "\n",
    "Common records in STI2023 and in unionlist_crws\n",
    "\"\"\"\n",
    "\n",
    "common_df= pd.merge(unionlist_crws, unionlist_sti.iloc[:,0], on='DOI', how='inner')\n",
    "common_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting items with PMID in the newly-added_crws\n",
    "\"\"\"\n",
    "new_added_doi_withpmid = newly_added_crws[newly_added_crws['PubMedID']!='']\n",
    "print(f'# Newly-added items with PMIDs are {len(new_added_doi_withpmid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed775a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get items retracted that are older year than Year 2023 in the newly_added_crws items\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Total # newly indexed items in newly_added_crws before Year 2023: {(newly_added_crws[newly_added_crws['Year']<2023].count().iloc[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating DOIs for Sankey Diagram\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Year 2024:\")\n",
    "n_newly_added_in2024 = newly_added_crws[newly_added_crws['Year']>2023].count()[0]\n",
    "print(f\"Items added in Jan to July 2024 is {n_newly_added_in2024}\")\n",
    "\n",
    "n_doi_newly_added_in2023 = len(newly_added_crws[newly_added_crws['Year']==2023]['DOI'].tolist())\n",
    "n_doi_in2023_sti= len(unionlist_sti[unionlist_sti['Year']==2023]['DOI'].tolist())\n",
    "\n",
    "print(f\"\\nYear 2023:\")\n",
    "print(f\"Items added in Jan to Dec 2023 {n_doi_newly_added_in2023}\")\n",
    "print(f\"Items added in Jan to April 2023 is {n_doi_in2023_sti}\")\n",
    "print(f\"Items added in May to Dec 2023 is {n_doi_newly_added_in2023 - n_doi_in2023_sti}\")\n",
    "\n",
    "# 427 - missing DOIs\n",
    "print(f\"Items remaining in STI2023 from Year1940 -Year2022) is \\\n",
    "{len(unionlist_sti) - 427 -  n_doi_in2023_sti}\")\n",
    "\n",
    "# 11 items from NoDOI but PMID in STI2023; \n",
    "print(f\"\\nOther remaining items in present unionlist CRWS: \\\n",
    " {len(unionlist_crws_covered_and_indexed) - n_newly_added_in2024 - n_doi_newly_added_in2023 - len(common_df) - 11}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unionlist_crws_covered_and_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbbd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating the breakdown of found DOIs in unionlist_crws by sources\n",
    "\"\"\"\n",
    "print(f'The total # of records with noDOI of STI2023 now in  unionlist_crws 2024 is {len(nodoi_sti_found_df)}')\n",
    "\n",
    "nodoi_sti_found_source=  [source for source in nodoi_sti_found_df['source_old'].tolist()]\n",
    "source_list=[]\n",
    "for sources in nodoi_sti_found_source:\n",
    "    sources = [source.strip() for source in sources]\n",
    "    source_list.extend(sources)\n",
    "\n",
    "print(f'The breakdown of sources those missing DOIs found are:')\n",
    "Counter(source_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For STI2023\n",
    "DOI in 2023 (Jan - April): 375\n",
    "Missing DOI in STI: 425 \n",
    "Left DOIs between start - Year2022:\n",
    "\"\"\"\n",
    "len(unionlist_sti) - 425 - 375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfd2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation common DOIs between STI2023(<2023) and Unionlist_CRWS\n",
    "sti_till2023= unionlist_sti[unionlist_sti['Year']!=2023]['DOI']\n",
    "common_doi_between_sti_and_ul_crws_before_2023= set(sti_till2023) & set(unionlist_crws['DOI'])\n",
    "len(common_doi_between_sti_and_ul_crws_before_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df445e5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating remaining new DOIs for Unionlist for Sankey diagram\n",
    "len(unionlist_crws) - len(common_df) - 27 - 2603 - 12369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_DOIs_for_RIA(df:pd.DataFrame)-> list():\n",
    "    n_25= df[df['RetractionIndexingAgreement_ITEM(%)']<=25].count()[0]\n",
    "    n_to50= df[(df['RetractionIndexingAgreement_ITEM(%)']>25) & \\\n",
    "               (df['RetractionIndexingAgreement_ITEM(%)']<=50)].count()[0]\n",
    "    \n",
    "    # For RIA 66 & 75\n",
    "    n_to75= df[(df['RetractionIndexingAgreement_ITEM(%)']>50) & \\\n",
    "               (df['RetractionIndexingAgreement_ITEM(%)']<=75)].count()[0]\n",
    "   \n",
    "    # For RIA at 100\n",
    "    n_100= df[df['RetractionIndexingAgreement_ITEM(%)']==100].count()[0]\n",
    "    \n",
    "    n_count= [n_25, n_to50,n_to75,n_100]\n",
    "    \n",
    "    n_frac= []\n",
    "    for cnt in n_count:\n",
    "        res= round(cnt/(len(df))*100,2)\n",
    "        n_frac.append(res)\n",
    "    \n",
    "    table=pd.DataFrame()\n",
    "    table['noDOIs']= n_count\n",
    "    table['Fraction_in_Unionlist(%)'] = n_frac\n",
    "\n",
    "    table['RIA_Score(%)'] = [\"<=25%\",'>25% to 50%','>50% to 75%','100%']\n",
    "    table['Color']= ['red','orange','#1f77b4','green',]\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Filtered exclusively indexed sources - items that indexed and covered in only one source\n",
    "\"\"\"\n",
    "unionlist_crws_filtered= unionlist_crws_covered_and_indexed[~unionlist_crws_covered_and_indexed['source_new'].apply(lambda x: len(x) == 1)].copy()\n",
    "\n",
    "all_excluded= len(unionlist_crws_covered_and_indexed[unionlist_crws_covered_and_indexed['source_new'].apply(lambda x: len(x) == 1)])\n",
    "\n",
    "\"\"\"\n",
    "Calculating # items exclusive-indexed  & covered in STI2023 & newly added union list \n",
    "\"\"\"\n",
    "print(f\"# of exclusively-indexed items excluded in the unionlist is {all_excluded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1afe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Distribution of RetractionIndexingAgreement of the entire Unionlist_CRW (exclusive source filtered)\n",
    "\"\"\"\n",
    "table000 = count_DOIs_for_RIA(unionlist_crws_filtered)\n",
    "table000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retraction Indexing Agreement by Items (in Group) for the Entire Unionlist_CRWS\n",
    "\"\"\"\n",
    "\n",
    "plt.style.use('bmh')\n",
    "fig000, ax000 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "mycolor_000 = dict(zip(table000['RIA_Score(%)'], table000['Color']))\n",
    "\n",
    "# Create the bar plot\n",
    "sns.barplot(y=table000['Fraction_in_Unionlist(%)'], x=table000['RIA_Score(%)'],\n",
    "            ax=ax000, color=\"#1f77b4\",palette=mycolor_000, ci=None)\n",
    "\n",
    "#Add custom legend\n",
    "legend_labels = {\n",
    "    'red': '<=25%',\n",
    "    'orange': '>25% to 50%',\n",
    "    '#1f77b4': '>50% to 75%',\n",
    "    'green': '100%'\n",
    "}\n",
    "\n",
    "legend_handles = [mpatches.Patch(color=color, label=label) for color, label in legend_labels.items()]\n",
    "\n",
    "ax000.legend(handles=legend_handles, title=\"RIA Score Range\", loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Label bars with noDOIs values\n",
    "for bar, noDOIs, frc in zip(ax000.patches, table000['noDOIs'],table000['Fraction_in_Unionlist(%)']):\n",
    "    ax000.annotate(f\"{noDOIs} Items ({frc}%)\", #f\"{noDOIs} / {len(unionlist3)}\"\n",
    "                      xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                      xytext=(0, 3),  # 3 points vertical offset\n",
    "                      textcoords=\"offset points\",\n",
    "                      ha='center', va='bottom')\n",
    "\n",
    "# Set labels and title\n",
    "ax000.set_xlabel(\"RetractionIndexingAgreement_Score(%)\")\n",
    "ax000.set_ylabel(\"Fraction of IDs in Unionlist(%)\")\n",
    "# ax3_01_1.set_title(\"Percentage of DOIs with their RIA_Score in Unionlist\", size=13)\n",
    "\n",
    "fig000.patch.set_facecolor('#ccd9ff') \n",
    "plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Distribution of RetractionIndexingAgreement of the new Unionlist_CRWS (exclusive source filtered)\n",
    "\"\"\"\n",
    "newly_added_crws_filtered = unionlist_crws_filtered[unionlist_crws_filtered['DOI'].isin(newly_added_crws['DOI'])]\n",
    "\n",
    "table001= count_DOIs_for_RIA(newly_added_crws_filtered)\n",
    "table001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Distribution of RetractionIndexingAgreement(RIA) of the STI2023 (exclusive source filtered)\n",
    "- We obtain STI2023 (excluding exclusive indexed & covered items) from Unionlist_CRWS -- using present RIA \n",
    "\"\"\"\n",
    "sti2023_filtered = unionlist_crws_filtered[unionlist_crws_filtered['DOI'].isin(unionlist_sti['DOI'])]\n",
    "\n",
    "table002= count_DOIs_for_RIA(sti2023_filtered)\n",
    "table002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retraction Indexing Agreement by Items (in Group) for the newly added DOIs(from Unionlist_CRWS) \n",
    "to STI2023 unionlist\n",
    "\"\"\"\n",
    "\n",
    "plt.style.use('bmh')\n",
    "fig001, ax001 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "mycolor_001 = dict(zip(table001['RIA_Score(%)'], table001['Color']))\n",
    "\n",
    "# Create the bar plot\n",
    "sns.barplot(y=table001['Fraction_in_Unionlist(%)'], x=table001['RIA_Score(%)'],\n",
    "            ax=ax001, color=\"#1f77b4\",palette=mycolor_001, ci=None)\n",
    "\n",
    "#Add custom legend\n",
    "legend_labels = {\n",
    "    'red': '<=25%',\n",
    "    'orange': '>25% to 50%',\n",
    "    '#1f77b4': '>50% to 75%',\n",
    "    'green': '100%',\n",
    "\n",
    "}\n",
    "\n",
    "legend_handles = [mpatches.Patch(color=color, label=label) for color, label in legend_labels.items()]\n",
    "\n",
    "ax001.legend(handles=legend_handles, title=\"RIA Score Range\", loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Label bars with noDOIs values\n",
    "for bar, noDOIs, frc in zip(ax001.patches, table001['noDOIs'],table001['Fraction_in_Unionlist(%)']):\n",
    "    ax001.annotate(f\"{noDOIs} Items ({frc}%)\", #f\"{noDOIs} / {len(unionlist3)}\"\n",
    "                      xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                      xytext=(0, 3),  # 3 points vertical offset\n",
    "                      textcoords=\"offset points\",\n",
    "                      ha='center', va='bottom')\n",
    "\n",
    "# Set labels and title\n",
    "ax001.set_xlabel(\"RetractionIndexingAgreement_Score(%)\")\n",
    "ax001.set_ylabel(\"Fraction of IDs in Unionlist(%)\")\n",
    "# ax3_01_1.set_title(\"Percentage of DOIs with their RIA_Score in Unionlist\", size=13)\n",
    "\n",
    "fig001.patch.set_facecolor('#ccd9ff') \n",
    "plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retraction Indexing Agreement by Items (in Group) for STI2023\n",
    "\"\"\"\n",
    "\n",
    "plt.style.use('bmh')\n",
    "fig002, ax002 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "mycolor_002 = dict(zip(table002['RIA_Score(%)'], table002['Color']))\n",
    "\n",
    "# Create the bar plot\n",
    "sns.barplot(y=table002['Fraction_in_Unionlist(%)'], x=table002['RIA_Score(%)'],\n",
    "            ax=ax002, color=\"#1f77b4\",palette=mycolor_002, ci=None)\n",
    "\n",
    "#Add custom legend\n",
    "#Add custom legend\n",
    "legend_labels = {\n",
    "    'red': '<=25%',\n",
    "    'orange': '>25% to 50%',\n",
    "    '#1f77b4': '>50% to 75%',\n",
    "    'green': '100%',\n",
    "}\n",
    "\n",
    "legend_handles = [mpatches.Patch(color=color, label=label) for color, label in legend_labels.items()]\n",
    "\n",
    "ax002.legend(handles=legend_handles, title=\"RIA Score Range\", loc=\"upper left\")\n",
    "\n",
    "\n",
    "# Label bars with noDOIs values\n",
    "for bar, noDOIs, frc in zip(ax002.patches, table002['noDOIs'],table002['Fraction_in_Unionlist(%)']):\n",
    "    ax002.annotate(f\"{noDOIs} IDs ({frc}%)\", #f\"{noDOIs} / {len(unionlist3)}\"\n",
    "                      xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                      xytext=(0, 3), # 3 points vertical offset\n",
    "                      textcoords=\"offset points\",\n",
    "                      ha='center', va='bottom')\n",
    "\n",
    "# Set labels and title\n",
    "ax002.set_xlabel(\"RetractionIndexingAgreement_Score(%)\")\n",
    "ax002.set_ylabel(\"Fraction of IDs in Unionlist(%)\")\n",
    "# ax3_01_1.set_title(\"Percentage of DOIs with their RIA_Score in Unionlist\", size=13)\n",
    "\n",
    "fig002.patch.set_facecolor('#ccd9ff') \n",
    "plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Subplot for table000 & table0001\n",
    "\"\"\"\n",
    "\n",
    "plt.style.use('bmh')\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "\n",
    "# Plotting table002 -  STI2023\n",
    "mycolor_002 = dict(zip(table000['RIA_Score(%)'], table000['Color']))\n",
    "sns.barplot(y=table002['Fraction_in_Unionlist(%)'], x=table000['RIA_Score(%)'], \n",
    "            ax=ax1, palette=mycolor_000, ci=None)\n",
    "ax1.set_title('April 2023 union list',fontsize=15)\n",
    "ax1.set_xlabel(\"Retraction Indexing Agreement Score (%)\")\n",
    "ax1.set_ylabel(\"Fraction of IDs in Unionlist (%)\")\n",
    "ax1.tick_params(axis='y', labelsize=14) \n",
    "\n",
    "\n",
    "\n",
    "# Plotting table001 - Newly added items to STI2023\n",
    "mycolor_001 = dict(zip(table001['RIA_Score(%)'], table001['Color']))\n",
    "sns.barplot(y=table001['Fraction_in_Unionlist(%)'], x=table001['RIA_Score(%)'], \n",
    "            ax=ax2, palette=mycolor_001, ci=None)\n",
    "ax2.set_title('Items added to the union list: May 2023 to July 2024',fontsize=15)\n",
    "ax2.set_xlabel(\"Retraction Indexing Agreement Score (%)\", fontsize=12)\n",
    "ax2.set_ylabel(\"\")\n",
    "# ax2.yaxis.set_visible(True)\n",
    "ax2.tick_params(axis='x', labelsize=14) \n",
    "ax2.set_xticks([])\n",
    "\n",
    "# Remove y-axis labels and ticks from the second subplot\n",
    "ax2.set_yticklabels([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "# Find the maximum y-value for setting the same y-axis scale\n",
    "max_y = max(table000['Fraction_in_Unionlist(%)'].max(), table001['Fraction_in_Unionlist(%)'].max())\n",
    "\n",
    "# Set the same y-axis limits and ticks for both subplots\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_ylim(0, max_y + 10)  # Add some padding at the top\n",
    "    ax.set_yticks(range(0, int(max_y) + 11, 10))\n",
    "    ax.set_xticks([])\n",
    "\n",
    "\n",
    "\n",
    "# Create a combined legend\n",
    "legend_labels = {\n",
    "    'red': '<=25%',\n",
    "    'orange': '>25% to 50%',\n",
    "    '#1f77b4': '>50% to 75%',\n",
    "    'green': '100%'\n",
    "}\n",
    "\n",
    "legend_handles = [mpatches.Patch(color=color, label=label) for color, label in legend_labels.items()]\n",
    "\n",
    "# Place the combined legend outside the top of the plot\n",
    "fig.legend(handles=legend_handles, title=\"Retraction Indexing Agreement Score Range\", title_fontsize=15,\\\n",
    "           loc=\"upper center\", bbox_to_anchor=(0.5, 1.10), ncol=4,prop={'size': 15})\n",
    "\n",
    "# Label bars with noDOIs values for table000\n",
    "for bar, noDOIs, frc in zip(ax1.patches, table002['noDOIs'], table002['Fraction_in_Unionlist(%)']):\n",
    "    ax1.annotate(f\"{noDOIs} Items ({frc}%)\",\n",
    "                  xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                  xytext=(0, 3),\n",
    "                  textcoords=\"offset points\",fontsize=12.5, \n",
    "                  ha='center', va='bottom')\n",
    "\n",
    "# Label bars with noDOIs values for table001\n",
    "for bar, noDOIs, frc in zip(ax2.patches, table001['noDOIs'], table001['Fraction_in_Unionlist(%)']):\n",
    "    ax2.annotate(f\"{noDOIs} Items ({frc}%)\",\n",
    "                  xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                  xytext=(0, 3),\n",
    "                  textcoords=\"offset points\",fontsize=12.5, \n",
    "                  ha='center', va='bottom')\n",
    "\n",
    "# Customize figure appearance\n",
    "fig.patch.set_facecolor('#ccd9ff')\n",
    "plt.tight_layout(pad=1.0, rect=[0, 0, 1, 1])  # Adjust layout to make room for legend\n",
    "\n",
    "# plt.savefig(result_dir+'unionlist_ria_proportion_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e7e9a",
   "metadata": {},
   "source": [
    "### Investigating Missing DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31582c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The Actual # DOIs in STI2023 found in unionlist_crws 2024 is {len(common_df)}')\n",
    "\n",
    "missing_sti_dois = set(unionlist_sti['DOI']) - set(common_df['DOI'])\n",
    "print(f'The # missing DOIs of STI2023 not found in unionlist_crws 2024 is {len(missing_sti_dois)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Quality Check:\n",
    "Missing records in STI2023 when cross-matched with unionlist_crws 2024\n",
    "\"\"\"\n",
    "\n",
    "missing_df= unionlist_sti[unionlist_sti['DOI'].isin(missing_sti_dois)]\n",
    "\n",
    "# missing_df.to_csv(result_dir+'missing_doi_insti2023.csv',index=False)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig00, ax00 = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Group and count DOIs by year\n",
    "missing_t = missing_df.groupby('Year')['DOI'].count().reset_index()\n",
    "\n",
    "# Plot the lineplot\n",
    "sns.lineplot(data=missing_t, x='Year', y='DOI', marker='o', ax=ax00, color='black', linestyle='--')\n",
    "\n",
    "# Shade the area under the line plot\n",
    "ax00.fill_between(missing_t['Year'], missing_t['DOI'], color='darkgray', alpha=1, label = 'missing DOI')\n",
    "\n",
    "# Set the x-axis ticks to show every year\n",
    "ax00.set_xticks(missing_t['Year'])\n",
    "ax00.set_xticklabels(missing_t['Year'], rotation=45)\n",
    "\n",
    "# Add titles and labels\n",
    "ax00.set_title('Publication year distribution of numbers of missing DOIs in STI2023 union list ')\n",
    "ax00.set_xlabel('Year')\n",
    "ax00.set_ylabel('#DOI')\n",
    "ax00.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and count DOIs\n",
    "sti_year_t = unionlist_sti.groupby('Year')['DOI'].count().reset_index()\n",
    "ul_year_t = common_df.groupby('Year')['DOI'].count().reset_index()\n",
    "\n",
    "ul_year_t= ul_year_t.iloc[:-1,:].copy() # Remove 2024 year because of plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting trends of retracted publications in STI2023 indicating missing DOIs - 1\n",
    "\"\"\"\n",
    "\n",
    "fig01, ax01 = plt.subplots(figsize=(35, 20))\n",
    "\n",
    "# Plot the line plots\n",
    "sns.lineplot(data=sti_year_t, x='Year', y='DOI', marker='o', ax=ax01, color='black', label='STI Year')\n",
    "sns.lineplot(data=ul_year_t, x='Year', y='DOI', marker='o', ax=ax01, color='grey', label='UL Year')\n",
    "\n",
    "# Ensure x-axis is the same for both plots\n",
    "ax01.set_xlim(min(sti_year_t['Year'].min(), ul_year_t['Year'].min()), max(sti_year_t['Year'].max(), ul_year_t['Year'].max()))\n",
    "\n",
    "# Fill the area between the lines\n",
    "# Ensure both DataFrames cover the same years\n",
    "combined_years = sorted(set(sti_year_t['Year']).union(ul_year_t['Year']))\n",
    "sti_year_t = sti_year_t.set_index('Year').reindex(combined_years, fill_value=0).reset_index()\n",
    "ul_year_t = ul_year_t.set_index('Year').reindex(combined_years, fill_value=0).reset_index()\n",
    "\n",
    "plt.fill_between(sti_year_t['Year'], sti_year_t['DOI'], ul_year_t['DOI'], where=(sti_year_t['DOI'] > ul_year_t['DOI']),\n",
    "                 color='red', alpha=1, label='Missing DOIs in STI2023)')\n",
    "\n",
    "# Set the x-axis ticks to show every year\n",
    "ax01.set_xticks(combined_years)\n",
    "ax01.set_xticklabels(combined_years, rotation=45)\n",
    "\n",
    "# Increase font size of tick labels\n",
    "ax01.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# Add titles and labels\n",
    "ax01.set_title('Number of DOIs Over Years',fontsize=16)\n",
    "ax01.set_xlabel('Year',  fontsize=16)\n",
    "ax01.set_ylabel('#DOI')\n",
    "ax01.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting trends of retracted publications in STI2023 indicating missing DOIs - 2\n",
    "\"\"\"\n",
    "\n",
    "fig01, ax01 = plt.subplots(figsize=(35, 20))\n",
    "# Plot the line plots\n",
    "sns.lineplot(data=sti_year_t, x='Year', y='DOI', marker='o', ax=ax01, color='black', label='STI Year')\n",
    "sns.lineplot(data=ul_year_t, x='Year', y='DOI', marker='o', ax=ax01, color='grey', label='UL Year')\n",
    "\n",
    "# Ensure x-axis is the same for both plots\n",
    "ax01.set_xlim(min(sti_year_t['Year'].min(), ul_year_t['Year'].min()), max(sti_year_t['Year'].max(), ul_year_t['Year'].max()))\n",
    "\n",
    "# Fill the area between the lines\n",
    "# Ensure both DataFrames cover the same years\n",
    "combined_years = sorted(set(sti_year_t['Year']).union(ul_year_t['Year']))\n",
    "sti_year_t = sti_year_t.set_index('Year').reindex(combined_years, fill_value=0).reset_index()\n",
    "ul_year_t = ul_year_t.set_index('Year').reindex(combined_years, fill_value=0).reset_index()\n",
    "\n",
    "plt.fill_between(sti_year_t['Year'], sti_year_t['DOI'], ul_year_t['DOI'], where=(sti_year_t['DOI'] > ul_year_t['DOI']),\n",
    "                 color='darkred', alpha=1, label='Missing DOIs in STI2023)')\n",
    "\n",
    "plt.fill_between(sti_year_t['Year'], sti_year_t['DOI'], ul_year_t['DOI'], where=(sti_year_t['DOI'] < ul_year_t['DOI']),\n",
    "                 color='darkred', alpha=1, )\n",
    "# Set the x-axis ticks to show every year\n",
    "ax01.set_xticks(combined_years)\n",
    "ax01.set_xticklabels(combined_years, rotation=45)\n",
    "\n",
    "# Increase font size of tick labels\n",
    "ax01.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# Add titles and labels\n",
    "ax01.set_title('Number of DOIs Over Years',fontsize=16)\n",
    "ax01.set_xlabel('Year',  fontsize=16)\n",
    "ax01.set_ylabel('#DOI')\n",
    "ax01.legend()\n",
    "\n",
    "\n",
    "###\n",
    "fig00, ax00 \n",
    "ax00= fig01.add_axes([0.2, 0.2, 0.38, 0.6],  frameon=True,)  #[0.18, 0.5, 0.55, 0.35] [left, bottom, width, height]\n",
    "for spine in ax00.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    \n",
    "# Plot the lineplot\n",
    "sns.lineplot(data=missing_t, x='Year', y='DOI', marker='o', ax=ax00, color='black', linestyle='--')\n",
    "\n",
    "# Shade the area under the line plot\n",
    "ax00.fill_between(missing_t['Year'], missing_t['DOI'], color='lightcoral', alpha=1, label = 'missing DOI')\n",
    "\n",
    "# color='lightcoral' - light red\n",
    "# Set the x-axis ticks to show every year\n",
    "ax00.set_xticks(missing_t['Year'])\n",
    "ax00.set_xticklabels(missing_t['Year'], rotation=45)\n",
    "\n",
    "# Add titles and labels\n",
    "ax00.set_title('Publication year distribution of numbers of missing DOIs in STI2023 union list ')\n",
    "ax00.set_xlabel('Year')\n",
    "ax00.set_ylabel('#DOI')\n",
    "ax00.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Examining changes between STI2023 and the unionlist_CRWS2024 unionlists\n",
    "\"\"\"\n",
    "\n",
    "combined_df= pd.merge(sti_year_t,ul_year_t, how='inner',on='Year')\n",
    "\n",
    "combined_df.rename(columns={'DOI_x': 'DOI_STI', 'DOI_y':'DOI_Present'}, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07636d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating the breakdown of missing DOIs in STI2023 union list\n",
    "\n",
    "Extract missing DOIs for each sources\n",
    "\"\"\"\n",
    "\n",
    "# missing_df[missing_df['source_old'].apply(lambda col: col if 'WoS_Core' in col)]\n",
    "msDOI_wos= missing_df[missing_df['source'].str.contains('Web of Science')]['DOI'].tolist()\n",
    "msDOI_crossref= missing_df[missing_df['source'].str.contains('Crossref')]['DOI'].tolist()\n",
    "msDOI_scopus= missing_df[missing_df['source'].str.contains('Scopus')]['DOI'].tolist()\n",
    "msDOI_rw= missing_df[missing_df['source'].str.contains('Retraction Watch')]['DOI'].tolist()\n",
    "\n",
    "print(f'# total missing DOIs in STI2023 is {len(missing_df)} DOIs')\n",
    "print(f'# total missing DOIs in Crossref is {len(msDOI_crossref)} DOIs')\n",
    "print(f'# total missing DOIs in Retraction Watch is {len(msDOI_rw)} DOIs')\n",
    "print(f'# total missing DOIs in Scopus is {len(msDOI_scopus)} DOIs')\n",
    "print(f'# total missing DOIs in Web of Science is {len(msDOI_wos)} DOIs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622db4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "msDOI_wos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f05470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write missing DOIs in STI2023 in files \n",
    "\"\"\"\n",
    "\n",
    "# Missing DOIs in Crossref\n",
    "with open(result_dir+\"sti_missingdoi_incrossref.txt\", \"w\") as file:\n",
    "    for item in msDOI_crossref:\n",
    "        file.write(item + \"\\n\")\n",
    "\n",
    "# Missing DOIs in Retraction Watch\n",
    "with open(result_dir+\"sti_missingdoi_inretractionwatch.txt\", \"w\") as file:\n",
    "    for item in msDOI_rw:\n",
    "        file.write(item + \"\\n\")\n",
    "        \n",
    "# Missing DOIs in Scopus\n",
    "with open(result_dir+\"sti_missingdoi_inscopus.txt\", \"w\") as file:\n",
    "    for item in msDOI_scopus:\n",
    "        file.write(item + \"\\n\")\n",
    "        \n",
    "# Missing DOIs in Web of Science\n",
    "with open(result_dir+\"sti_missingdoi_inwos.txt\", \"w\") as file:\n",
    "    for item in msDOI_wos:\n",
    "        file.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_items(pmids:list, cut:int)-> list[list]:\n",
    "    \"\"\"\n",
    "    It divides the list of items into batches for processing. \n",
    "    :param pmids: list of items \n",
    "    :param cut: maximum number of records to assign to a batch\n",
    "    \n",
    "    :return: list of list of batches of pmids\n",
    "    \"\"\"\n",
    "    pmids_batches=[]\n",
    "    \n",
    "    while len(pmids) >= cut:\n",
    "        selected_pmids= pmids[:cut]\n",
    "        pmids_batches.append(selected_pmids)\n",
    "#         print(selected_pmids)    \n",
    "        pmids = pmids[cut:]\n",
    "\n",
    "    if pmids:\n",
    "        pmids_batches.append(pmids)\n",
    "#         print(pmids)\n",
    "\n",
    "    return pmids_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901a69a",
   "metadata": {},
   "source": [
    "#### Checking missing DOIs coverage in Crossref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Works, Etiquette\n",
    "my_etiquette = Etiquette('Retraction Indexing Assessment', 'version2', 'no url', my_email)\n",
    "#my_etiquette = Etiquette('My Project Name', 'My Project version', 'My Project URL', 'My contact email')\n",
    "works = Works(etiquette=my_etiquette)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae7c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut= 50\n",
    "check_doi_in_crossref= msDOI_crossref \n",
    "\n",
    "check_doi_in_crossref_batches = batch_items(check_doi_in_crossref, cut)\n",
    "\n",
    "print(f'The total items to search in Crossref is {len(check_doi_in_crossref)}, which are divided into {len(check_doi_in_crossref_batches)} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a937fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check DOI Coverage in Crossref\n",
    "\"\"\"\n",
    "\n",
    "new = Works()\n",
    "start = time.time()\n",
    "\n",
    "crossref_covered_doi=[]\n",
    "\n",
    "for i in tqdm(check_doi_in_crossref):\n",
    "    try:\n",
    "        for j in new.filter(doi = i).select('DOI'):\n",
    "            find = j['DOI']\n",
    "            if i == find:\n",
    "                crossref_covered_doi.append(i)\n",
    "\n",
    "    except Exception:\n",
    "            pass\n",
    "    time.sleep(0.10)\n",
    "        #print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99991b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The total # missing DOIs found in Crossref is {len(crossref_covered_doi)}')\n",
    "print (f'The missing DOIs now not covered in Crossref is {len(set(msDOI_crossref) - set(crossref_covered_doi))} DOI')\n",
    "set(msDOI_crossref) - set(crossref_covered_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4946bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All the missing DOIs in Crossef were found - # already written to file -> sti_missingdoi_incrossref\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c44dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Investigate found missing DOIs in Crossref\n",
    "\"\"\"\n",
    "\n",
    "crossref_covered_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOIs that starts with '10.1016/j.ijdevneu.2015.' ... '10.1016/j.ijdevneu.2015.04.317'\n",
    "c = 0\n",
    "for doi in crossref_covered_doi:\n",
    "    if doi.startswith('10.1016/j.ijdevneu.2015.'):\n",
    "        c+=1 \n",
    "print(f\"The total # DOI that starts with '10.1016/j.ijdevneu.2015.04....': {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf15ff3",
   "metadata": {},
   "source": [
    "#### Checking missing DOIs coverage in Scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Break the list of DOIs to search into batches of a maximum of 25 items in each batch. \n",
    "The limit per page for Scopus is 25 items at a time.\n",
    "\n",
    "Input:\n",
    "    cut: maximum number items in a batch\n",
    "\"\"\"\n",
    "scopus_cut= 25\n",
    "check_doi_in_scopus= msDOI_scopus \n",
    "\n",
    "check_doi_in_scopus_batches = batch_items(check_doi_in_scopus, scopus_cut) \n",
    "\n",
    "\n",
    "print(f'The total items to search in Scopus is {len(check_doi_in_scopus)}, which are divided into {len(check_doi_in_scopus_batches)} batches')\n",
    "print(f'The items list is divided into lists in which each list contains {scopus_cut} records maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checking DOIs coverage in Scopus\n",
    "\"\"\"\n",
    "\n",
    "scopus_covered_doi = []\n",
    "scopus_unresolved_doi = []\n",
    "\n",
    "# Set your API key\n",
    "api_key = elsevier_api_key\n",
    "elsevier_insttoken = elsevier_insttoken\n",
    "\n",
    "url_base = \"https://api.elsevier.com/content/search/scopus\" \n",
    "headers = {\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-Insttoken':elsevier_insttoken}\n",
    "\n",
    "\n",
    "for batch in tqdm(check_doi_in_scopus_batches[:]):\n",
    "    \n",
    "    # Putting search format 'DOI(10.xxxx/xxxxxxxxxxxx) OR DOI(...)'\n",
    "    check_now = ' OR '.join(f'DOI({doi})' for doi in batch) \n",
    "    params= {\"query\": check_now}\n",
    "        \n",
    "    response = requests.get(url_base,\n",
    "                        headers= headers,\n",
    "                    params = params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "\n",
    "        store=[]\n",
    "            \n",
    "        totalresult= int(results['search-results'].get('opensearch:totalResults',0))\n",
    "    \n",
    "        if totalresult > 0:\n",
    "        \n",
    "            try:\n",
    "                for result in results['search-results']['entry']:\n",
    "                    try:\n",
    "                        store.append(result['prism:doi'])\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                        scopus_unresolved_doi.extend(batch)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    else:\n",
    "        scopus_unresolved_doi.extend(batch)\n",
    "\n",
    "    scopus_covered_doi.extend(store)\n",
    "    \n",
    "    time.sleep(0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b11ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The total # missing DOIs covered in Scopus is {len(scopus_covered_doi)}')\n",
    "print (f'The missing DOIs now not covered in Scopus is {len(set(msDOI_scopus) - set(scopus_covered_doi))} DOI')\n",
    "set(msDOI_scopus) - set(scopus_covered_doi)\n",
    "# scopus_unresolved_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All the missing DOIs in Scopus were found - # already written to file -> sti_missingdoi_inscopus\n",
    "except {'10.1063/1.5032937'}\n",
    "\"\"\"\n",
    "scopus_covered_doi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327f680",
   "metadata": {},
   "source": [
    "#### Checking missing DOIs coverage in Web of Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wos_check_DOI(DOIs_lists: list, database:str):\n",
    "    \"\"\"\n",
    "    It checks the coverage of DOI in the Web of Science\n",
    "    \n",
    "    :param DOIs_lists: list of list of DOIs preformatted to query form for Web of Science use\n",
    "    :param database: specific the database to search\n",
    "    \n",
    "    :return: list of list [available DOIs, DOIs that ran into error]\n",
    "    \n",
    "    \"\"\"\n",
    "    c=0\n",
    "    wos_covered_doi = []\n",
    "    wos_unresolved_doi = []\n",
    "\n",
    "    # Define your API key\n",
    "    WoS_api_key = wos_api_key\n",
    "\n",
    "\n",
    "    # Set the headers with the API key\n",
    "    headers = {\n",
    "            'X-ApiKey': WoS_api_key,\n",
    "            'charset': 'UTF-8',\n",
    "            'Encoding': 'UTF-8',\n",
    "            'content-type':'text/xml'\n",
    "            }\n",
    "#     base_url = 'https://api.clarivate.com/apis/wos-starter/v1/documents' \n",
    "    url = 'https://api.clarivate.com/apis/wos-starter/v1/documents'\n",
    "\n",
    "    for batch in tqdm(DOIs_lists):\n",
    "        check_now = ' OR '.join(f'DO=({doi})' for doi in batch) \n",
    "#         print(check_now)\n",
    "\n",
    "        params = {\n",
    "            'db': database, #'WOK',#'WOS',\n",
    "            'q': check_now,\n",
    "            'limit': 50\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        # Make the API request\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "        # Check if the request was successful (200 status code)\n",
    "        if response.status_code == 200:\n",
    "#             print('success')\n",
    "            # Extract the response content as JSON\n",
    "            data = response.json()\n",
    "            # Print the DOI details\n",
    "            try:\n",
    "                dois_result = data['hits']\n",
    "                for doi in dois_result:\n",
    "                    try:\n",
    "                        if doi.get('identifiers'):\n",
    "                            identifiers = doi.get('identifiers','')\n",
    "                            DOI= identifiers.get('doi','')\n",
    "                            wos_covered_doi.append(DOI)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            except Exception:\n",
    "                pass\n",
    "                wos_unresolved_doi.append(check_now)\n",
    "\n",
    "        else:\n",
    "            # If the request was not successful, print the error message\n",
    "            print(f\"Request failed with status code: {response.status_code} in batch: {c}\")\n",
    "            #print(response.text)\n",
    "\n",
    "        time.sleep(0.15)\n",
    "        #print('batch: ',c)\n",
    "        c+=1 \n",
    "        \n",
    "    return [wos_covered_doi,wos_unresolved_doi]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Break the list of DOIs to search into batches of a maximum of 50 items in each batch. \n",
    "The limit per page for Web of Science is 50 items at a time.\n",
    "\n",
    "Input:\n",
    "    cut: maximum number items in a batch\n",
    "\"\"\"\n",
    "wos_cut= 50\n",
    "check_doi_in_woscore = msDOI_wos #woscoreDOI_notindexed['DOI'].tolist()# change doilist_wos to notindexedasretracted_source\n",
    "\n",
    "check_doi_in_woscore_batches = batch_items(check_doi_in_woscore, wos_cut)\n",
    "\n",
    "print(f'The total items to search in Web of Science Core is {len(check_doi_in_woscore)}, which are divided into {len(check_doi_in_woscore_batches)} batches')\n",
    "print(f'The items list is divided into lists in which each list contains {wos_cut} records maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c1503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check DOIs Coverage in Web of Science Core\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "\n",
    "woscore_results= wos_check_DOI(check_doi_in_woscore_batches, 'WOS')\n",
    "\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d025fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting DOIs that are still missing\n",
    "notfound_doi_wos= list(set(msDOI_wos) - set(woscore_results[0]))\n",
    "\n",
    "print(f'The total # missing DOIs found in Web of Science is {len(woscore_results[0])}')\n",
    "print (f'The missing DOIs now not covered in Web of Science is {len(notfound_doi_wos)} DOI')\n",
    "notfound_doi_wos[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d36094",
   "metadata": {},
   "outputs": [],
   "source": [
    "woscore_results[0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing DOIs that were found in Web of Science\n",
    "with open(result_dir+\"sti_missingdoi_inwos_found.txt\", \"w\") as file:\n",
    "    for item in woscore_results[0]:\n",
    "        file.write(item + \"\\n\")\n",
    "        \n",
    "\n",
    "# Missing DOIs that were not found in Web of Science\n",
    "with open(result_dir+\"sti_missingdoi_inwos_notfound.txt\", \"w\") as file:\n",
    "    for item in notfound_doi_wos:\n",
    "        file.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032b041",
   "metadata": {},
   "source": [
    "#### Checking missing DOIs coverage in Retraction Watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633939fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load Retraction Watch - downloaded on Aug 20, 2024\n",
    "\"\"\"\n",
    "\n",
    "rw_df= pd.read_csv(data_dir+'/retractionwatch/retractionwatch_aug20_2024.csv')\n",
    "rw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_df['OriginalPaperDOI']= rw_df['OriginalPaperDOI'].str.lower()\n",
    "rw_df['RetractionDOI'] = rw_df['RetractionDOI'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing DOIs in Retraction Watch Original DOI\n",
    "rw_df[rw_df['OriginalPaperDOI'].isin(msDOI_rw)].count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0939ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing DOIs in Retraction Watch RetractionDOI  (i.e. Retraction Notices)\n",
    "rw_df[rw_df['RetractionDOI'].isin(msDOI_rw)].count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing  missing DOIs covered in Retraction Watch\n",
    "missingdoi_foundin_rw= rw_df[rw_df['RetractionDOI'].isin(msDOI_rw)]['RetractionDOI'].tolist()\n",
    "missingdoi_foundin_rw[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a421ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing  missing DOIs not covered in Retraction Watch\n",
    "missingdoi_notfoundin_rw = list(set(msDOI_rw) - set(missingdoi_foundin_rw))\n",
    "missingdoi_notfoundin_rw[:15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79593b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing DOIs that were found in Retraction Watch\n",
    "with open(result_dir+\"sti_missingdoi_inrw_found.txt\", \"w\") as file:\n",
    "    for item in missingdoi_foundin_rw:\n",
    "        file.write(item + \"\\n\")\n",
    "        \n",
    "\n",
    "# Missing DOIs that were not found in Retraction Watch\n",
    "with open(result_dir+\"sti_missingdoi_inrwatch_notfound.txt\", \"w\") as file:\n",
    "    for item in missingdoi_notfoundin_rw :\n",
    "        file.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd37330",
   "metadata": {},
   "source": [
    "#### Investigate any overlap in missing DOIs and RQ4 DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "rq4_df = pd.read_excel(retraction_index_path+'/Spreadsheet for RQ4_main category.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14232b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rq4_missing_dois= missing_df[missing_df['DOI'].isin(rq4_df['DOI'].str.lower())]['DOI'].tolist()\n",
    "rq4_missing_dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcfbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rq4_df[rq4_df['DOI'].isin(rq4_missing_dois)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
