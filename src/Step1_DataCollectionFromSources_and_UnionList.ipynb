{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3d7b21",
   "metadata": {},
   "source": [
    "# Step1a: Data Collection from Sources\n",
    "- The following codes collect retracted items from the sources:\n",
    "    - BIOSIS Citation Index (BCI)\n",
    "    - Biological Abstracts (BIOABS)\n",
    "    - Compendex\n",
    "    - Current Contents Connect (CCC)\n",
    "    - Crossref\n",
    "    - GeoBASE\n",
    "    - Medline\n",
    "    - PubMed\n",
    "    - Scopus\n",
    "    - Retraction Watch\n",
    "    - Web of Science Core (WoS_Core)\n",
    "    \n",
    "\n",
    "####  Input File:  \n",
    "   - Retracted publication records will be collected from API EXCEPT for Retraction Watch       \n",
    "       \n",
    "\n",
    "\n",
    "#### Output File: \n",
    "   - engineeringvillage/compendex_retractedpublication_{date}.csv\n",
    "   - engineeringvillage/geobase_retractedpublication_{date}.csv\n",
    "   - crossref/crossref_retractedpublication_{date}.csv\n",
    "   - pubmed/pubmed_retractedpublication_{date}.csv\n",
    "   - scopus/scopus_retractedpublication_{date}.csv\n",
    "   - webofscience/bci_retractedpublication_{date}.csv\n",
    "   - webofscience/bioabs_retractedpublication_{date}.csv\n",
    "   - webofscience/ccc_retractedpublication_{date}.csv\n",
    "   - webofscience/medline_retractedpublication_{date}.csv\n",
    "   - webofscience/webofsciencecore_retractedpublication_{date}.csv\n",
    "\n",
    "###### Uncomment the line of code \"....to_csv(..)\"  to save file to your local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d187319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time,datetime\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from crossref.restful import Works, Etiquette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set up Today's date to label your files\n",
    "Uncomment line 5 to set the date. In case the download extends to another day, set the date accordingly\n",
    "\"\"\"\n",
    "today = str(datetime.date.today())\n",
    "#today = '2024-02-13'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a91fd7",
   "metadata": {},
   "source": [
    "### Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Targeting the retraction_index_path\n",
    "\"\"\"\n",
    "\n",
    "retraction_index_path = os.path.abspath('.\\.')\n",
    "retraction_index_path\n",
    "\n",
    "data_dir = retraction_index_path+'/data/' # data directory\n",
    "result_dir = retraction_index_path+'/result/'\n",
    "\n",
    "# Create folder for all the sources in data directory (data_dir)\n",
    "sources_dir = ['crossref','engineeringvillage', 'pubmed', 'retractionwatch', 'scopus','unionlist','webofscience']\n",
    "for source in sources_dir:\n",
    "    if not os.path.exists(data_dir+source):\n",
    "        os.mkdir(data_dir+source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ed18b",
   "metadata": {},
   "source": [
    "### Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "con_file = open(retraction_index_path+\"/config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()\n",
    "\n",
    "# Initializing variable for configuration file\n",
    "my_email = config['my_email']\n",
    "elsevier_api_key = config['Elsevier_APIKEY']\n",
    "elsevier_insttoken = config['insttoken']\n",
    "ieee_xplore_api_key = config['IEEEXplore_APIKEY']\n",
    "wos_api_key = config['WoS_APIKEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable declarations\n",
    "global my_email\n",
    "global elsevier_api_key\n",
    "global elsevier_insttoken\n",
    "global ieee_xplore_api_key\n",
    "global wos_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de00c29",
   "metadata": {},
   "source": [
    "# PubMed API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c08e63",
   "metadata": {},
   "source": [
    "#### Fetching Retracted Publications from the PubMed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681925f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pmids(term:str,mindate:int, maxdate:int)->list:\n",
    "    \"\"\"\n",
    "    It retrieves pmids for a given search term.\n",
    "    \n",
    "    :param term: search term\n",
    "    :param mindate: the year to start the search\n",
    "    :param maxdate: the year to end the search\n",
    "    \n",
    "    :return: list of all pmids of the records retrieved\n",
    "    \"\"\"\n",
    "    \n",
    "    api_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    \n",
    "    # Step 1: Search for retracted papers\n",
    "    global my_email\n",
    "    \n",
    "    email = my_email # Supply your email\n",
    "    \n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": term,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": 10000,  # Maximum number of results per request\n",
    "        \"mindate\": mindate,\n",
    "        \"maxdate\": maxdate\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the PubMed API to search for retracted papers\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    total_results = int(data[\"esearchresult\"][\"count\"])\n",
    "    pmids = data[\"esearchresult\"][\"idlist\"]\n",
    "    return total_results, pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_pmids(term:str,start_year:int, end_year:int, interval_year:int)->list:\n",
    "    \"\"\"\n",
    "    It retrieves pmids for a given search term over a period of time using retrieve_pmids function. \n",
    "    It re-iterates at a defined interval year because upto 10,000 records maximum can retrieved \n",
    "    from PubMed at time. Check: https://www.ncbi.nlm.nih.gov/books/NBK25501/ for detail\n",
    "    \n",
    "    :param term: search term\n",
    "    :param from_year: the year to start the search\n",
    "    :param to_year: the year to end the search\n",
    "    \n",
    "    :return: list of all pmids of the records retrieved\n",
    "    \"\"\"\n",
    "    all_pmids = []\n",
    "    total_pmids_count = 0\n",
    "    current_year = end_year\n",
    "    \n",
    "    # Iterate over the years with a stipulated year interval of 10,000 records maximum limitation\n",
    "    for year in range(start_year, end_year +1, interval_year):\n",
    "        end_year = year + interval_year-1\n",
    "        if (current_year - year) < interval_year:\n",
    "            end_year = current_year\n",
    "\n",
    "        \n",
    "        count, pmids_per_interval = retrieve_pmids(term,year,end_year)\n",
    "        total_pmids_count+=count\n",
    "        all_pmids+=pmids_per_interval\n",
    "        \n",
    "        print(f'{year} - {end_year}: {count} total number of retrieved pmids')\n",
    "        \n",
    "    return total_pmids_count,all_pmids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea102b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_retraction_notice_metadata(soup:bs)->list:\n",
    "    \"\"\"\n",
    "    It extracts data from the XML metadata for retraction notice\n",
    "    \n",
    "    :param soup: article in Beautifulsoup XML format\n",
    "    \n",
    "    :return: list of extracted data from the XML metadata\n",
    "    \"\"\" \n",
    "    #initialize all variables\n",
    "    rn_pmid, doi = '',''   # retraction notice paper's pmid and doi\n",
    "    pub_year,year,month, day = '','9999','99','99'   # publication of the retracted paper\n",
    "    jour_abrv,jour_title, title = '','',''  # journal title, abbreviation and article title of the retracted paper\n",
    "    pub_type = '' # publication type of the retracted paper such as letter, article etc.\n",
    "    retractionOf = None\n",
    "     \n",
    "    try:\n",
    "\n",
    "        # extract pmid\n",
    "        if soup.PMID: # pmid\n",
    "            pmid = soup.PMID.string\n",
    "        elif soup.ArticleIdList.find(IdType=\"pubmed\"):\n",
    "            pmid = soup.ArticleIdList.find(IdType=\"pubmed\").string\n",
    "            #print(pmid)\n",
    "\n",
    "            # extract doi\n",
    "        if soup.ArticleIdList.find(IdType=\"doi\") is not None:\n",
    "            doi = soup.ArticleIdList.find(IdType=\"doi\").string\n",
    "\n",
    "            # fetching publication year\n",
    "        if soup.ArticleDate:\n",
    "            if soup.ArticleDate.Year is not None:\n",
    "                year = soup.ArticleDate.Year.string\n",
    "\n",
    "            if soup.ArticleDate.Month is not None:\n",
    "                month = soup.ArticleDate.Month.string\n",
    "\n",
    "            if soup.ArticleDate.Day is not None:\n",
    "                day = soup.ArticleDate.Day.string\n",
    "\n",
    "        elif soup.PubDate:\n",
    "            if soup.PubDate.Year is not None:\n",
    "                year = soup.PubDate.Year.string\n",
    "\n",
    "            if soup.PubDate.Month is not None:\n",
    "                month = soup.PubDate.Month.string\n",
    "\n",
    "            if soup.PubDate.Day is not None:\n",
    "                day = soup.PubDate.Day.string        \n",
    "    \n",
    "        \n",
    "        if year == '9999':\n",
    "            \"\"\"\n",
    "            <PubMedPubDate PubStatus=\"pubmed\">\n",
    "            <PubMedPubDate PubStatus=\"medline\">\n",
    "            <PubMedPubDate PubStatus=\"entrez\">\n",
    "            \"\"\"\n",
    "\n",
    "            if soup.find_all('PubMedPubDate', {'PubStatus': \"pubmed\"}):\n",
    "                pub_date_elements = soup.find_all('PubMedPubDate', {'PubStatus': \"pubmed\"})\n",
    "                for pub_date in pub_date_elements:\n",
    "                    if pub_date.find(\"Year\").text:\n",
    "                        year = pub_date.find(\"Year\").text\n",
    "                    if pub_date.find(\"Month\").text:\n",
    "                        month = pub_date.find(\"Month\").text\n",
    "                    if pub_date.find(\"Day\").text:\n",
    "                        day = pub_date.find(\"Day\").text\n",
    "                    \n",
    "            elif soup.find_all('PubMedPubDate', {'PubStatus': \"medline\"}):\n",
    "                pub_date_elements = soup.find_all('PubMedPubDate', {'PubStatus': \"medline\"})\n",
    "                for pub_date in pub_date_elements:\n",
    "                    if pub_date.find(\"Year\").text:\n",
    "                        year = pub_date.find(\"Year\").text\n",
    "                    if pub_date.find(\"Month\").text:\n",
    "                        month = pub_date.find(\"Month\").text\n",
    "                    if pub_date.find(\"Day\").text:\n",
    "                        day = pub_date.find(\"Day\").text\n",
    "\n",
    "            elif soup.find_all('PubMedPubDate', {'PubStatus': \"entrez\"}):\n",
    "                pub_date_elements = soup.find_all('PubMedPubDate', {'PubStatus': \"entrez\"})\n",
    "                for pub_date in pub_date_elements:\n",
    "                    if pub_date.find(\"Year\").text:\n",
    "                        year = pub_date.find(\"Year\").text\n",
    "                    if pub_date.find(\"Month\").text:\n",
    "                        month = pub_date.find(\"Month\").text\n",
    "                    if pub_date.find(\"Day\").text:\n",
    "                        day = pub_date.find(\"Day\").text        \n",
    "            \n",
    "        pub_year = f'{year}:{month}:{day}'\n",
    "\n",
    "\n",
    "            # extract title\n",
    "        if soup.ArticleTitle is not None:\n",
    "            title = soup.ArticleTitle.string\n",
    "                #print(title)\n",
    "\n",
    "            # extract journal title\n",
    "        if soup.Title is not None:\n",
    "            jour_title = soup.Title.string\n",
    "            \n",
    "            # extract journal title abbreviation\n",
    "        if soup.ISOAbbreviation is not None:\n",
    "            jour_abrv = soup.ISOAbbreviation.string\n",
    "\n",
    "\n",
    "       \n",
    "        #extract publication types\n",
    "        if soup.PublicationTypeList is not None:\n",
    "                pub_type = soup.PublicationTypeList.find_all()\n",
    "                pub_type = ';'.join([pub.string for pub in pub_type])\n",
    "\n",
    "        elif soup.PublicationType is not None:\n",
    "                pub_type = check_soup.PublicationType.string\n",
    "\n",
    "\n",
    "        # Checking Attribute 'RetractionOf' to if the PMID is a retraction notice\n",
    "        retraction_of = rn_soup.find('CommentsCorrections', attrs={'RefType': 'RetractionOf'})\n",
    "        pmid = retraction_of.find('PMID')\n",
    "        if pmid is not None:\n",
    "            retractionOf = pmid.text\n",
    "\n",
    "\n",
    "    except Exception as e: \n",
    "        pass\n",
    "        print(f'error at {pmid} with {doi}')\n",
    "\n",
    "\n",
    "    return [rn_pmid, doi,pub_year,title,pub_type,\n",
    "            jour_title,jour_abrv,retractionOf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_retractedpaper_metadata(soup:bs)->list:\n",
    "    \"\"\"\n",
    "    It extracts data from the XML for retracted publications\n",
    "    \n",
    "    :param soup: article in Beautifulsoup XML format\n",
    "    \n",
    "    :return: list of extracted data from the XML metadata\n",
    "    \"\"\" \n",
    "    #initialize all variables\n",
    "    pmid, doi = '',''   # retracted paper's pmid and doi\n",
    "    pub_year,year,month, day = '','9999','99','99'   # publication of the retracted paper\n",
    "    jour_abrv,jour_title, title = '','',''  # journal title, abbreviation and article title of the retracted paper\n",
    "    pub_type = '' # publication type of the retracted paper such as letter, article etc.\n",
    "    authors_names,authors_affils = '','' # authors' names & affiliations\n",
    "    retraction_notice_detail,retraction_notice_pmid,retractionOf=None,None,None # Default value if the attribute is not found\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # extract pmid\n",
    "        if soup.PMID: # pmid\n",
    "            pmid = soup.PMID.string\n",
    "        elif soup.ArticleIdList.find(IdType=\"pubmed\"):\n",
    "            pmid = soup.ArticleIdList.find(IdType=\"pubmed\").string\n",
    "            #print(pmid)\n",
    "\n",
    "            # extract doi\n",
    "        if soup.ArticleIdList.find(IdType=\"doi\") is not None:\n",
    "            doi = soup.ArticleIdList.find(IdType=\"doi\").string\n",
    "\n",
    "            # fetching publication year\n",
    "        if soup.ArticleDate:\n",
    "            if soup.ArticleDate.Year is not None:\n",
    "                year = soup.ArticleDate.Year.string\n",
    "\n",
    "            if soup.ArticleDate.Month is not None:\n",
    "                month = soup.ArticleDate.Month.string\n",
    "\n",
    "            if soup.ArticleDate.Day is not None:\n",
    "                day = soup.ArticleDate.Day.string\n",
    "\n",
    "        elif soup.PubDate:\n",
    "            if soup.PubDate.Year is not None:\n",
    "                year = soup.PubDate.Year.string\n",
    "\n",
    "            if soup.PubDate.Month is not None:\n",
    "                month = soup.PubDate.Month.string\n",
    "\n",
    "            if soup.PubDate.Day is not None:\n",
    "                day = soup.PubDate.Day.string        \n",
    "    \n",
    "        \n",
    "        if year == '9999':\n",
    "            \"\"\"\n",
    "            <PubMedPubDate PubStatus=\"pubmed\">\n",
    "            <PubMedPubDate PubStatus=\"medline\">\n",
    "            <PubMedPubDate PubStatus=\"entrez\">\n",
    "            \"\"\"\n",
    "\n",
    "            if soup.find_all('PubMedPubDate', {'PubStatus': \"pubmed\"}):\n",
    "                pub_date_elements = soup.find_all('PubMedPubDate', {'PubStatus': \"pubmed\"})\n",
    "                for pub_date in pub_date_elements:\n",
    "                    if pub_date.find(\"Year\").text:\n",
    "                        year = pub_date.find(\"Year\").text\n",
    "                    if pub_date.find(\"Month\").text:\n",
    "                        month = pub_date.find(\"Month\").text\n",
    "                    if pub_date.find(\"Day\").text:\n",
    "                        day = pub_date.find(\"Day\").text\n",
    "                    \n",
    "            elif soup.find_all('PubMedPubDate', {'PubStatus': \"medline\"}):\n",
    "                pub_date_elements = soup.find_all('PubMedPubDate', {'PubStatus': \"medline\"})\n",
    "                for pub_date in pub_date_elements:\n",
    "                    if pub_date.find(\"Year\").text:\n",
    "                        year = pub_date.find(\"Year\").text\n",
    "                    if pub_date.find(\"Month\").text:\n",
    "                        month = pub_date.find(\"Month\").text\n",
    "                    if pub_date.find(\"Day\").text:\n",
    "                        day = pub_date.find(\"Day\").text\n",
    "\n",
    "            elif soup.find_all('PubMedPubDate', {'PubStatus': \"entrez\"}):\n",
    "                pub_date_elements = soup.find_all('PubMedPubDate', {'PubStatus': \"entrez\"})\n",
    "                for pub_date in pub_date_elements:\n",
    "                    if pub_date.find(\"Year\").text:\n",
    "                        year = pub_date.find(\"Year\").text\n",
    "                    if pub_date.find(\"Month\").text:\n",
    "                        month = pub_date.find(\"Month\").text\n",
    "                    if pub_date.find(\"Day\").text:\n",
    "                        day = pub_date.find(\"Day\").text        \n",
    "            \n",
    "        pub_year = f'{year}:{month}:{day}'\n",
    "\n",
    "\n",
    "            # extract title\n",
    "        if soup.ArticleTitle is not None:\n",
    "            title = soup.ArticleTitle.string\n",
    "                #print(title)\n",
    "\n",
    "            # extract journal title\n",
    "        if soup.Title is not None:\n",
    "            jour_title = soup.Title.string\n",
    "            \n",
    "            # extract journal title abbreviation\n",
    "        if soup.ISOAbbreviation is not None:\n",
    "            jour_abrv = soup.ISOAbbreviation.string\n",
    "\n",
    "            # extract authors\n",
    "        authorlist = soup.AuthorList\n",
    "        if authorlist is not None:\n",
    "            authors= soup.AuthorList.find_all('Author')\n",
    "\n",
    "                # calling function to extract the authors names and affiliations\n",
    "            authors_names,authors_affils = get_authors_detail(authors)\n",
    "\n",
    "        #extract publication types\n",
    "        if soup.PublicationTypeList is not None:\n",
    "                pub_type = soup.PublicationTypeList.find_all()\n",
    "                pub_type = ';'.join([pub.string for pub in pub_type])\n",
    "\n",
    "        elif soup.PublicationType is not None:\n",
    "                pub_type = check_soup.PublicationType.string\n",
    "\n",
    "\n",
    "        # Call function to extract retraction notice \n",
    "        retraction_notice_detail,retraction_notice_pmid = get_retraction_notice(soup)\n",
    "\n",
    "\n",
    "\n",
    "        # Checking Attribute 'RetractionOf' to if the PMID is a retraction notice\n",
    "        retraction_of = soup.find('CommentsCorrections', attrs={'RefType': 'RetractionOf'})\n",
    "        if retraction_of is not None and retraction_of.PMID is not None:\n",
    "            retractionOf = retraction_of.PMID.string\n",
    "\n",
    "    except Exception as e: \n",
    "        pass\n",
    "        print(f'error at {pmid} with {doi}')\n",
    "\n",
    "\n",
    "    return [pmid, doi,pub_year,authors_names,authors_affils,title,pub_type,\n",
    "            jour_title,jour_abrv,retraction_notice_pmid,retraction_notice_detail,retractionOf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_retracted_paper_data_from_metadata(pmid:list):\n",
    "    \"\"\"\n",
    "    It retrieves XML of a given pmid\n",
    "    \n",
    "    :param pmid: the pmid of a given publication\n",
    "    :return: XML of the pmid of a given publication\n",
    "    \"\"\"\n",
    "    \n",
    "    #url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    efetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "    global my_email\n",
    "    \n",
    "    email = my_email       # Supply your email\n",
    "#   # Retrieve paper's XML details\n",
    "    params = {\n",
    "            \"db\": \"pubmed\",\n",
    "            \"id\": pmid,\n",
    "            \"retmode\": \"xml\"\n",
    "        }\n",
    "\n",
    "    # Process the XML response to extract the desired paper details \n",
    "    # (e.g., title, authors, abstract, etc.)\n",
    "    response = requests.get(efetch_url, params=params)\n",
    "    paper_xml = response.text\n",
    "    \n",
    "    return paper_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff70995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_detail(authors) -> list:\n",
    "    \"\"\"\n",
    "    It extracts authors' names and their affiliations\n",
    "    :param authors: authors' xml details - names and affiliations\n",
    "    :return: list values of authors and their affiliations\n",
    "    \"\"\"\n",
    "    au_name_list, au_affil_list = [], []\n",
    "    au_names, au_affils = '', ''\n",
    "\n",
    "    for author in authors:\n",
    "        # get forename/firstname\n",
    "        forename = ''\n",
    "        fname = author.ForeName\n",
    "\n",
    "        if fname is not None:\n",
    "            forename = fname.string\n",
    "        else:\n",
    "            forename = 'unknown'\n",
    "\n",
    "        # get lastname/surname\n",
    "        lastname = ''\n",
    "        lname = author.LastName\n",
    "\n",
    "        if lname is not None:\n",
    "            lastname = lname.string\n",
    "        else:\n",
    "            lastname = 'unknown'\n",
    "\n",
    "        au_name = f'{forename} {lastname}'\n",
    "        au_name_list.append(au_name)\n",
    "        au_names = ';'.join(au_name_list)\n",
    "\n",
    "        # get affiliation\n",
    "        au_affil = ''\n",
    "        if author.Affiliation is not None:\n",
    "            au_affil = author.Affiliation.string\n",
    "            if au_affil is not None:\n",
    "                au_affil_list.append(au_affil)\n",
    "\n",
    "    if au_affil_list:\n",
    "        au_affils = ';'.join(au_affil_list)\n",
    "\n",
    "    return au_names, au_affils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retraction_notice(soup:bs):\n",
    "    \"\"\"\n",
    "    It extracts journal title & retraction notice date, and retraction notice pmid\n",
    "    \n",
    "    :param soup: article in Beautifulsoup XML format\n",
    "    :return: journal title & retraction notice date, and retraction notice pmid\n",
    "    \"\"\"\n",
    "    retraction_notice_detail = None  # Default value if the attribute is not found\n",
    "    retraction_notice_pmid = None  # Default value if the attribute is not found\n",
    "\n",
    "    r_notice= soup.find_all('CommentsCorrections', attrs={'RefType': 'RetractionIn'})\n",
    "    #print(retractions)\n",
    "\n",
    "    result = [] \n",
    "    pmid_text =''\n",
    "    \n",
    "    # Getting the retraction notice details such date and PMID\n",
    "    for retraction in r_notice:\n",
    "        ref_source = retraction.find('RefSource')\n",
    "        if ref_source is not None:\n",
    "            ref_source_text = ref_source.text\n",
    "            result.append(ref_source_text)\n",
    "            \n",
    "\n",
    "        pmid = retraction.find('PMID')\n",
    "        if pmid is not None:\n",
    "            pmid_text = pmid.text\n",
    "\n",
    "\n",
    "    if (result is not None) or (len(result)!=0):\n",
    "        retraction_notice_detail = ','.join(result)\n",
    "        \n",
    "    retraction_notice_pmid = pmid_text\n",
    "    #print(retraction_notice_detail)\n",
    "    return retraction_notice_detail,retraction_notice_pmid    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc39985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_pmids(pmids:list, cut:int)-> list[list]:\n",
    "    \"\"\"\n",
    "    It divides the list pmids into batches for processing. \n",
    "    :param pmids: list of pmids \n",
    "    :param cut: maximum number of records to assign to a batch\n",
    "    \n",
    "    :return: list of list of batches of pmids\n",
    "    \"\"\"\n",
    "    pmids_batches=[]\n",
    "    \n",
    "    while len(pmids) >= cut:\n",
    "        selected_pmids= pmids[:cut]\n",
    "        pmids_batches.append(selected_pmids)\n",
    "#         print(selected_pmids)    \n",
    "        pmids = pmids[cut:]\n",
    "\n",
    "    if pmids:\n",
    "        pmids_batches.append(pmids)\n",
    "#         print(pmids)\n",
    "\n",
    "    return pmids_batches    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e375cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieves all the PMIDs of retracted publications in PubMed using function: fetch_all_pmids()\n",
    "# PubMed starts from 1951. Check https://pubmed.ncbi.nlm.nih.gov/?term=retracted+publication%5Bptype%5D\n",
    "\"\"\"\n",
    "\n",
    "start_year = 1950  # search \n",
    "end_year = datetime.date.today().year  \n",
    "interval_year = 10\n",
    "term = \"Retracted Publication[PT]\" #\"retracted publication[ptype]\"  \n",
    "\n",
    "\n",
    "total_retracted_publications, retracted_paper_pmids =\\\n",
    "                        fetch_all_pmids(term,start_year,end_year,interval_year)\n",
    "\n",
    "print(f'The total numbers of retracted publications between {start_year} and {end_year} to is \\\n",
    "{total_retracted_publications} records in PubMed as today {datetime.date.today()}')\n",
    "                                                \n",
    "\n",
    "print(f'After double check duplication, the confirm size of the all pubmids is {len(set(retracted_paper_pmids))} records')"
    "print('PLEASE NOTE: If any interval of years returned over 10K results, rerun this cell with a smaller interval_year value. PubMed can only return 10K items per request.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f36594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Break into chunks/batches the PMIDs of retracted publications using function: pmids_batches=batch_pmids()\n",
    "- Each batch contains 300pmids maximum\n",
    "NB:  300 records for works fine in our case. More than that may throw error \n",
    "\"\"\"\n",
    "\n",
    "no_records = 300\n",
    "pmids_batches=batch_pmids(retracted_paper_pmids,no_records) \n",
    "\n",
    "print(f'The pmids is divided into {len(pmids_batches)} batches')\n",
    "print(f'The pmids list is divided into lists in which each list contains {no_records} records maximum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c4c5d",
   "metadata": {},
   "source": [
    "#####  Writing the extracted data from retracted publication into file in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf4191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "header = ['PubMedID', 'DOI', 'Year', 'Author', 'Au_Affiliation', 'Title', 'PubType',\n",
    "       'Journal', 'JournalAbrv', 'RetractionPubMedID','RetractionNotice','RetractionOf']\n",
    "\n",
    "outfile = open(data_dir+\"pubmed/pubmed_retractedpublication_\"+today+\".csv\",\"w\",encoding = \"utf-8\", newline = \"\")\n",
    "csvout = csv.writer(outfile)\n",
    "csvout.writerow(header)\n",
    "\n",
    "result_per_paper = []\n",
    "count =1\n",
    "\n",
    "for selected_pmids in tqdm(pmids_batches):\n",
    "    all_results= []\n",
    "    print(f'batch {count}/{len(pmids_batches)}: {len(selected_pmids)} records')\n",
    "\n",
    "    retracted_papers_xml = retrieve_retracted_paper_data_from_metadata(selected_pmids)\n",
    "    \n",
    "    \n",
    "    soup = bs(retracted_papers_xml,'xml') \n",
    "    #print(soup)\n",
    "    papers_xml = soup.find_all('PubmedArticle') # <PubmedArticle> vs <PubmedBookArticle>\n",
    "     \n",
    "    time.sleep(10) # it sometimes failed at 8secs\n",
    "    \n",
    "    for per_paper_xml in papers_xml:\n",
    "        result_per_paper = extract_retractedpaper_metadata(per_paper_xml)\n",
    "        #print(result_per_paper)\n",
    "        all_results.append(result_per_paper)\n",
    "        #csvout.writerow(result_per_paper)\n",
    "    \n",
    "    csvout.writerows(all_results)\n",
    "    count+=1\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading PubMed retracted publications\n",
    "\"\"\"\n",
    "\n",
    "pubmed = pd.read_csv(data_dir+\"pubmed/pubmed_retractedpublication_\"+today+\".csv\")#.drop(['Unnamed: 0'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09bb320",
   "metadata": {},
   "source": [
    " #  Engineering Village API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643d50d",
   "metadata": {},
   "source": [
    "### Search terms in Engineering Village"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3eddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_ev(results):  \n",
    "    \"\"\"\n",
    "    This function extracts data from the metadata result from Engineering Village API\n",
    "    \n",
    "    :param results: results return from the Engineering Village search\n",
    "    :param store: list of extracted data from the metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    store = []\n",
    "    \n",
    "    for result in results:\n",
    "        metadata= result['EI-DOCUMENT'] # put in paper details\n",
    "        \n",
    "        doi= metadata['DOCUMENTPROPERTIES'].get('DO',\"\")\n",
    "        document_type=metadata['DOCUMENTPROPERTIES'].get('DT',\"\")\n",
    "        complete_year= metadata['DOCUMENTPROPERTIES'].get('SD',\"\")\n",
    "        year= metadata['DOCUMENTPROPERTIES'].get('YR',0)\n",
    "        title= metadata['DOCUMENTPROPERTIES'].get('TI',\"\")\n",
    "        journal_title= metadata['DOCUMENTPROPERTIES'].get('SO',\"\")\n",
    "\n",
    "        database = metadata['DOC']['DB'].get('DBNAME',\"\")\n",
    "        affiliation = metadata.get('AFS',\"\")\n",
    "        author= metadata.get('AFS',\"\")\n",
    "        \n",
    "        \n",
    "        store.append([doi,document_type,year,complete_year,title, journal_title, database,\n",
    "                      author, affiliation])\n",
    "        \n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b498d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ev_fetch_retractedpaper(database: str, filename: str, start_year:int, end_year:int):\n",
    "        # Engineering Village\n",
    "    \"\"\"\n",
    "    It retrieves results from the Engineering Village API search\n",
    "    \n",
    "    :param filename: name the output file and indicate the .csv\n",
    "    :param database: indicate the database to conduct the search\n",
    "        c - Compendex/EI Backfile\n",
    "        i - Inspec/Inspec Archive\n",
    "        n - NTIS\n",
    "        pc - Paperchem\n",
    "        cm - Chimica\n",
    "        cb - CBNB\n",
    "        el - EnCompassLIT\n",
    "        ep - EnCompassPAT\n",
    "        g - GEOBASE\n",
    "        f - GeoRef\n",
    "        u - US Patents\n",
    "        e - EP Patents\n",
    "        w - WO Patents\n",
    "        k - Knovel\n",
    "    \"\"\"    \n",
    "\n",
    "    header = ['DOI','Doc_type','Year','Full_year','Title','Journal','source','Author','Affiliation']\n",
    "\n",
    "    outfile = open(filename,\"w\",encoding = \"utf-8\", newline = \"\")\n",
    "\n",
    "    csvout = csv.writer(outfile)\n",
    "    csvout.writerow(header)\n",
    " \n",
    "    global elsevier_api_key\n",
    "    global elsevier_insttoken\n",
    "\n",
    "    # Set the base URL for the ScienceDirect API\n",
    "    base_url = 'https://api.elsevier.com/content/ev/results?' \n",
    "\n",
    "    # Set your API key\n",
    "    api_key = elsevier_api_key\n",
    "    elsevier_insttoken = elsevier_insttoken\n",
    "\n",
    "    # Set the query parameters \n",
    "    query = \"{tb} WN DT\" #\"RETRACTED\"\n",
    "\n",
    "    limit = 100  # Maximum number of results to retrieve per request \n",
    "    start = 0    # Starting index of the results (page)\n",
    "    total_counts =0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    # Set the request headers with the API key\n",
    "    headers = {\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-Insttoken': elsevier_insttoken\n",
    "    }\n",
    "\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # While loop for pagination\n",
    "    while True:\n",
    "\n",
    "        response = requests.get(\n",
    "                base_url,\n",
    "                headers=headers,\n",
    "                params={\n",
    "                    'query': query,\n",
    "                    'start': start,\n",
    "                    'database':database, #comment the database parameters to retrieve from the EV databases\n",
    "                    'startYear': start_year,\n",
    "                    'endYear': end_year,\n",
    "                    'sortField':'yr', # Sort by year\n",
    "                    'sortDirection':'dw', # Direction can be 'dw' or 'up'. Need to pull both directions if there are over 5000 records for one year (as in 2022) \n",
    "                    'offset':start,\n",
    "                    'pageSize':limit})\n",
    "        print(f'{response}')\n",
    "\n",
    "      # Check the status code of the response\n",
    "        if response.status_code == 200:\n",
    "#            print(response.text)\n",
    "            time.sleep(1)\n",
    "            result_per_page = response.json()\n",
    "#            print(result_per_page)\n",
    "            total = result_per_page['PAGE']['RESULTS-COUNT']\n",
    "            count = result_per_page['PAGE']['RESULTS-PER-PAGE']\n",
    "            \n",
    "            if count > 0:\n",
    "\n",
    "                results = result_per_page['PAGE']['PAGE-RESULTS']['PAGE-ENTRY']\n",
    "\n",
    "                store_result = extract_metadata_ev(results)\n",
    "\n",
    "                csvout.writerows(store_result) # write the result to file\n",
    "\n",
    "                total_counts += count\n",
    "                start += limit  # pagination: initializing page to the next page\n",
    "\n",
    "\n",
    "                print('count: ' ,count)\n",
    "                print('store_result: ',len(store_result))\n",
    "                print('total_counts:', total_counts)\n",
    "                all_results.extend(store_result)\n",
    "\n",
    "                if total_counts >= total:\n",
    "                    break\n",
    "            else:\n",
    "                print(f'Zero result returned')\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    outfile.close() \n",
    "   \n",
    "    return all_results # You can also return the result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b7227",
   "metadata": {},
   "source": [
    "###  Compendex: Fetching Retracted Publications from Via Engineering Village API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "\"database\": - specify the database i.e. 'c'\n",
    "\"start_year\": year to begin the search. # Earliest year for Compendex is 1996 (check the database, the search \n",
    "will return empty if 1900 is used)\n",
    "\"end_year\": year to end the search\n",
    "\"filename\": specify the directory & filename to save the file\n",
    "\n",
    "Output: ev_compendex_{year}.csv # filename specified\n",
    "\n",
    "\n",
    "Maximum record that can be fetch from the Engineering API is 5000 records.\n",
    "Hence Firstly consult the Database interface to verify maximum records across the years. This helps to define\n",
    "the start_year and the end_year using function: ev_fetch_retractedpaper(). \n",
    "Iterate for years of interest and merge the files together for further processing.\n",
    "\n",
    "Since there are over 5000 records for 2022, API call needs to be pulled multiple times: \n",
    "- 3 times with sortDirection: dw (once finished, manually rename file; \n",
    "after all are done manually change sortDirection in ev_fetch_retractedpaper() above)\n",
    "- 3 times with sortDirection: up (manually rename file) \n",
    "Then 2022 files need to be combined and deduplicated (see next cell) before combining with rest of Compendex files.\n",
    "\n",
    "Note: data from the API can be inconsistent. Repeat the request from the API till it tallies\n",
    "\"\"\"\n",
    "# Uncomment the filename from filename directory section and its corresponding  API request \n",
    "\n",
    "# Filename directory Section\n",
    "\n",
    "filename_comp1996_2009 = data_dir+\"engineeringvillage/ev_compendex_1996_2009.csv\"\n",
    "# filename_comp2010 = data_dir+\"engineeringvillage/ev_compendex_2010.csv\" #\n",
    "# filename_comp2011 = data_dir+\"engineeringvillage/ev_compendex_2011.csv\"\n",
    "# filename_comp2012_2020 = data_dir+\"engineeringvillage/ev_compendex_2012_2020.csv\"\n",
    "# filename_comp2021 = data_dir+\"engineeringvillage/ev_compendex_2021.csv\"\n",
    "# filename_comp2022 = data_dir+\"engineeringvillage/ev_compendex_2022.csv\"\n",
    "# filename_comp2023_2024 = data_dir+\"engineeringvillage/ev_compendex_2023_2024.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# # Compendex API Search\n",
    "# Function format: ev_fetch_retractedpaper(database,filename,start_year,end_year) \n",
    "\n",
    "ev_fetch_retractedpaper('c',filename_comp1996_2009,1996,2009) \n",
    "# ev_fetch_retractedpaper('c',filename_comp2010,2010,2010)\n",
    "# ev_fetch_retractedpaper('c',filename_comp2011,2011,2011)\n",
    "# ev_fetch_retractedpaper('c',filename_comp2012_2020,2012,2020)\n",
    "# ev_fetch_retractedpaper('c',filename_comp2021,2021,2021)\n",
    "# ev_fetch_retractedpaper('c',filename_comp2022,2022,2022)\n",
    "# ev_fetch_retractedpaper('c',filename_comp2023_2024,2023,2024)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effb0bb",
   "metadata": {},
   "source": [
    "##### Merging the Compendex files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merging and deduplicating 2022 Compendex retracted publications from API\n",
    "\"\"\"\n",
    "\n",
    "# selecting the downloaded Compendex files\n",
    "compendex_files_2022 = ['dw1_ev_compendex_2022.csv', 'dw2_ev_compendex_2022.csv', 'dw3_ev_compendex_2022.csv', 'up1_ev_compendex_2022.csv', 'up2_ev_compendex_2022.csv', 'up3_ev_compendex_2022.csv']\n",
    "compendex_2022 = pd.DataFrame()\n",
    "\n",
    "\n",
    "# iterating downloaded files, merging in compendex_2022, and deduplicating\n",
    "for compendex_file in compendex_files_2022: \n",
    "    print(compendex_file)\n",
    "    df = pd.read_csv(data_dir+'engineeringvillage/'+compendex_file)\n",
    "    compendex_2022= pd.concat([compendex_2022,df])\n",
    "compendex_2022.info()\n",
    "compendex_2022.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "compendex_2022.info()\n",
    "\n",
    "# Save to directory\n",
    "compendex_2022.to_csv(data_dir+'engineeringvillage/ev_compendex_2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeaa087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merging & Reading all Compendex retracted publications from API\n",
    "\"\"\"\n",
    "\n",
    "ev_geo_files = os.listdir(data_dir+'engineeringvillage/') \n",
    "\n",
    "#selecting the downloaded Compendex files\n",
    "compendex_files = [x for x in ev_geo_files if x.startswith('ev_compendex_') and x.endswith('.csv')]\n",
    "\n",
    "\n",
    "\n",
    "compendex = pd.DataFrame()\n",
    "\n",
    "# iterating downloaded files & merging in compendex\n",
    "for compendex_file in compendex_files: \n",
    "    print(compendex_file)\n",
    "    df_comp= pd.read_csv(data_dir+'engineeringvillage/'+compendex_file)\n",
    "    compendex= pd.concat([compendex,df_comp])\n",
    "    \n",
    "# Save to directory\n",
    "compendex.to_csv(data_dir+'engineeringvillage/compendex_retractedpublication_'+today+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278fa7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading Compendex retracted publications\n",
    "\"\"\"\n",
    "\n",
    "compendex= pd.read_csv(data_dir+'engineeringvillage/compendex_retractedpublication_'+today+'.csv')# .drop(['Unnamed: 0'],1)\n",
    "compendex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For Manual Download from the Compendex database\n",
    "\n",
    "You can download records manually from the database UIUC Library:\n",
    "https://www-engineeringvillage-com.proxy2.library.illinois.edu/search/quick.url\n",
    "Use Expert Search and search with: \"{tb} WN DT\" to retrieve retracted publication and filter to 'Compendex'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Merging & Reading in Compendex retracted publications\n",
    "\"\"\"\n",
    "\n",
    "# ev_geo_files = os.listdir(data_dir+'engineeringvillage/') \n",
    "\n",
    "#selecting the downloaded Compendex files\n",
    "# compendex_files = [x for x in ev_geo_files if x.startswith('ev_compendex_') and x.endswith('.csv')]\n",
    "\n",
    "# Selecting the columns to select from the files\n",
    "# selected_columns = ['DOI','Title','Publication year','Author','Author affiliation',\n",
    "# 'Source','Document type','Language','Classification code','Database']\n",
    "\n",
    "# compendex = pd.DataFrame()\n",
    "\n",
    "# iterating downloaded files & merging in compendex\n",
    "# for compendex_file in compendex_files: \n",
    "#     print(compendex_file)\n",
    "#     df_geo= pd.read_csv(data_dir+'engineeringvillage/'+compendex_file, usecols=selected_columns)\n",
    "#     compendex= pd.concat([compendex,df_geo])\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fc3a6",
   "metadata": {},
   "source": [
    "### GEOBASE: Fetching Retracted Publications from Via Engineering Village API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eee26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    \"database\": - specify the database i.e. 'g'\n",
    "    \"start_year\": year to begin the search. # Earliest year for GEOBASE is 1973 \n",
    "                    (Confirm earliest retracted publication year from the database)\n",
    "    \"end_year\": year to end the search\n",
    "    \"geo_filename\": specify the directory & filename to save the file\n",
    "\n",
    "Output: 'geobase_retractedpublication_{today}'.csv # filename specified\n",
    "\n",
    "\n",
    "NB: Maximum record that can be fetch from the Engineering Village API is 5000 records. Though GEOBASE is approximately \n",
    "1k records as at Jul 2024.\n",
    "\"\"\"\n",
    "\n",
    "# # Fetching records for Geobase via Engineering Village API\n",
    "# Uncomment the filename from filename directory section and its corresponding  API request for GEOBASE\n",
    "\n",
    "# Filename\n",
    "geo_filename = data_dir+\"engineeringvillage/geobase_retractedpublication_\"+today+\".csv\"\n",
    "\n",
    "# Fetch retracted publications from GEOBASE API Search\n",
    "ev_fetch_retractedpaper('g',geo_filename,2001,2024) \n",
    "\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be958040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NB: Our preliminary finding as at Feb 13 2023, shows that GEOBASE records via Engineering Village API can be incomplete and inconsistent\n",
    "For instance:\n",
    "- 935 unique DOIs records from API  instead of 952  as shown in the database interface search. \n",
    "After a retry from the API, the result is complete\n",
    "\"\"\"\n",
    "\n",
    "# If API result does not tally, we can manually download from the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ed8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading GEOBASE retracted publications\n",
    "\"\"\"\n",
    "\n",
    "geobase = pd.read_csv(data_dir+\"engineeringvillage/geobase_retractedpublications_\"+today+\".csv\")\n",
    "geobase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e3fb0",
   "metadata": {},
   "source": [
    "# Scopus API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed861cc",
   "metadata": {},
   "source": [
    "#### Searching Scopus API by search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd92009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scopus_metadata(publications:list)->list:\n",
    "    \"\"\"\n",
    "    It extracts data from the Scopus API results\n",
    "    \n",
    "    :params publications: are json file of publications - the result from the Scopus API search\n",
    "    :return: list of extracted metadata of publications\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for publication in publications:\n",
    "        title = publication.get('dc:title','')\n",
    "        eid = publication.get('eid','')\n",
    "        DOI = publication.get('prism:doi','')\n",
    "        publication_date = publication.get('prism:coverDate','')\n",
    "        # author = publication.get('dc:creator', [])\n",
    "        authors_raw = publication.get('author','')\n",
    "        authors= [\",\".join([author.get('authname') for author in authors_raw])]\n",
    "\n",
    "        affiliations= publication.get('affiliation','')\n",
    "        journal = publication.get('prism:publicationName',['']) \n",
    "        PubMedID = publication.get('pubmed-id','')\n",
    "        pub_type = publication.get('prism:aggregationType','')\n",
    "        pub_type2 = publication.get('subtypeDescription','')\n",
    "        \n",
    "        results.append([DOI, eid,title,publication_date,authors,affiliations,journal,PubMedID, pub_type, pub_type2])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5761c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scopus_get_retracted_pubs(filename: str):\n",
    "        \n",
    "    \"\"\"\n",
    "    Gets the Scopus search results for a given query on retracted publications, using cursor - pagination approach.\n",
    "    It writes the result into a specified directory file.\n",
    "    Check API documentation: https://dev.elsevier.com/documentation/ScopusSearchAPI.wadl\n",
    "    \n",
    "    :params filename: the directory filename to save the result of the search query\n",
    "    \"\"\"\n",
    "\n",
    "    header = ['DOI', 'Eid','Title','Publication_date','Authors','Affiliations',\n",
    "          'Journal','PubMedID', 'Pub_type', 'Pub_type2']\n",
    "\n",
    "    outfile = open(filename,\"a\",encoding = \"utf-8\", newline = \"\")\n",
    "    \n",
    "    csvout = csv.writer(outfile)\n",
    "    csvout.writerow(header)\n",
    "\n",
    "    global elsevier_api_key\n",
    "    global elsevier_insttoken\n",
    "\n",
    "    # Set the base URL for the Scopus API\n",
    "    base_url = \"https://api.elsevier.com/content/search/scopus\" \n",
    "\n",
    "    # Set your API key\n",
    "    api_key = elsevier_api_key\n",
    "    elsevier_insttoken = elsevier_insttoken\n",
    "\n",
    "    # Set the query parameters \n",
    "    query = \"DOCTYPE(tb)\" #\"RETRACTED\"\n",
    "\n",
    "\n",
    "    start = 0    # Starting index of the results (page)\n",
    "    total_counts =0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    # Set the request headers with the API key\n",
    "    headers = {\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-Insttoken':elsevier_insttoken\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    #####\n",
    "    cursor = \"*\"\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # While loop for pagination\n",
    "    while True:\n",
    "\n",
    "        response = requests.get(\n",
    "                base_url,\n",
    "                headers=headers,\n",
    "                params={\n",
    "                    'query': query,\n",
    "                    'start': start,\n",
    "   #                  'date': date,    # if batch needed, in this format: 2000-2015\n",
    "                    'cursor': cursor,\n",
    "                    'view': 'COMPLETE',\n",
    "                                        })\n",
    "\n",
    "\n",
    "      # Check the status code of the response\n",
    "        if response.status_code == 200:\n",
    "            result_per_page = response.json()\n",
    "\n",
    "            total = result_per_page['search-results']['opensearch:totalResults']\n",
    "            page_count = result_per_page['search-results']['opensearch:itemsPerPage'] #'25' str maximum items on a page \n",
    "            \n",
    "            publications = result_per_page['search-results']['entry']\n",
    "\n",
    "            results = get_scopus_metadata(publications)\n",
    "\n",
    "\n",
    "            csvout.writerows(results) # write the result to file\n",
    "\n",
    "            total_counts += int(page_count)\n",
    "            cursor = result_per_page['search-results']['cursor']['@next'] #start += limit  # pagination: initializing page to the next page\n",
    "            \n",
    "            all_results.extend(results)\n",
    "            \n",
    "            \n",
    "            if total_counts >= int(total):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    outfile.close() \n",
    "    \n",
    "    return all_results # You can also return the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68567f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fetching Retracted Publications From Scopus API. Result as at Feb 28  # Put date of download\n",
    "\n",
    "Output: 'scopus_retractedpublication_{today}.csv'  # Use today's date to name the file\n",
    "\n",
    "# uncomment line 10: scopus_get_retracted_pubs() to run the code\n",
    "\"\"\"\n",
    "\n",
    "# It fetches retracted publication from Scopus API\n",
    "scopus_get_retracted_pubs(data_dir+\"scopus/scopus_retractedpublication_\"+today+\".csv\",) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ceab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading Scopus retracted publications\n",
    "\"\"\"\n",
    "\n",
    "scopus = pd.read_csv(data_dir+\"scopus/scopus_retractedpublication_\"+today+\".csv\")\n",
    "\n",
    "# Converting 'Publication_date' to Date to extract the year\n",
    "scopus['Publication_date'] = pd.to_datetime(scopus['Publication_date'])\n",
    "scopus['Year'] = scopus['Publication_date'].dt.year\n",
    "scopus #.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a9ead",
   "metadata": {},
   "source": [
    "# Web of Science API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbba3d8",
   "metadata": {},
   "source": [
    "#### Searching Web of Science API by search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ab581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wos_metadata(publications:list)->list:\n",
    "    results = []\n",
    "    for publication in publications:\n",
    "        \n",
    "        title = publication.get('title','')\n",
    "        doc_type = publication.get('sourceTypes',[])\n",
    "        authors = publication.get('names', '')\n",
    "        uid= publication.get('uid','')\n",
    "        \n",
    "        journal, year, month = '','',''\n",
    "        if publication.get('source'):\n",
    "            source = publication.get('source', '')\n",
    "            journal = source.get('sourceTitle', '')\n",
    "            year = source.get('publishYear','')\n",
    "            month = source.get('publishMonth','')\n",
    "        \n",
    "        pmid, DOI= '',''\n",
    "        if publication.get('identifiers'):\n",
    "            identifiers = publication.get('identifiers','')\n",
    "            pmid = identifiers.get('pmid','')\n",
    "            DOI= identifiers.get('doi','')\n",
    "        \n",
    "        results.append([DOI, uid,title,year,month,authors,journal,doc_type,pmid])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a66187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wos_get_retracted_pubs(filename: str,year: str, db: str):\n",
    "        \n",
    "    \"\"\"\n",
    "    Gets the Web of Science search results for a given query on retracted publications,via its API - pagination approach.\n",
    "    It writes the result into a specified directory file.\n",
    "    Check API documentation: http://help.incites.clarivate.com/wosWebServicesLite/WebServicesLiteOverviewGroup/Introduction.html\n",
    "    \n",
    "    Allowable databases: \n",
    "    ARCI,BCI,BIOABS,BIOSIS,CABI,CCC,CSCD,DIIDW,DRCI,FSTA,INSPEC,KJD,MEDLINE,PPRN,RSCI,SCIELO,WOK,WOS,ZOOREC\"\n",
    "    :params filename: the directory filename to save the result of the search query\n",
    "    \n",
    "     \n",
    "    Resource:\n",
    "    https://api.clarivate.com/swagger-ui/?url=https%3A%2F%2Fdeveloper.clarivate.com%2Fapis%2Fwos-starter%2Fswagger\n",
    "     \n",
    "        WOS - Web of Science Core collection\n",
    "        BIOABS - Biological Abstracts\n",
    "        BCI - BIOSIS Citation Index\n",
    "        BIOSIS - BIOSIS Previews\n",
    "        CCC - Current Contents Connect\n",
    "        DIIDW - Derwent Innovations Index\n",
    "        DRCI - Data Citation Index\n",
    "        MEDLINE - MEDLINE The U.S. National Library of Medicine® (NLM®) premier life sciences database.\n",
    "        ZOOREC - Zoological Records\n",
    "        PPRN - Preprint Citation Index\n",
    "        WOK - All databases\n",
    "        Available values : BCI, BIOABS, BIOSIS, CCC, DIIDW, DRCI, MEDLINE, PPRN, WOK, WOS, ZOOREC\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    header = ['DOI', 'Uid','Title','Year','Month','Authors','Journal','Pub_type','PubMedID']\n",
    "\n",
    "    outfile = open(filename,\"w\",encoding = \"utf-8\", newline = \"\")\n",
    "    \n",
    "    csvout = csv.writer(outfile)\n",
    "    csvout.writerow(header)\n",
    "\n",
    "    global wos_api_key\n",
    "\n",
    "\n",
    "    # Set the base URL for the WoS API\n",
    "    base_url = 'https://api.clarivate.com/apis/wos-starter/v1/documents'  # Uses WoS Starter API\n",
    "    \n",
    "    # Set your API key\n",
    "    api_key = wos_api_key\n",
    "    #print(api_key)\n",
    "\n",
    "    # Set the query parameters \n",
    "    query =   f'(DT=\"Retracted Publication\") AND (PY={year})'#'DT=\"Retracted Publication\"'\n",
    "\n",
    "    page_no = 1    # Starting index of the results (page)\n",
    "    total_counts =0\n",
    "    total = 0\n",
    "    page_size = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    # Set the request headers with the API key\n",
    "    headers = {\n",
    "    'X-ApiKey': api_key,\n",
    "    'charset': 'UTF-8',\n",
    "    'Encoding': 'UTF-8'\n",
    "    }\n",
    "\n",
    "    \n",
    "    #####\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # While loop for pagination\n",
    "    while True:\n",
    "        \n",
    "        params = {\n",
    "                'db': db, #'WOK', # WOK - retrieves items for all the databases\n",
    "                'q': query,       \n",
    "                'limit': 50, # maximum number of result on the page\n",
    "                'page':page_no }\n",
    "\n",
    "        response = requests.get(\n",
    "                base_url,\n",
    "                headers=headers,\n",
    "                params= params)\n",
    "        \n",
    "\n",
    "\n",
    "      # Check the status code of the response\n",
    "    \n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Extract the response content as JSON\n",
    "            data = response.json()\n",
    "            \n",
    "            count = data['metadata']['limit'] # counting number of items on a page\n",
    "            publications = data['hits']\n",
    "            results= get_wos_metadata(publications)              \n",
    "\n",
    "            csvout.writerows(results) # write the result to file\n",
    "            \n",
    "            #all_results.extend(results)\n",
    "\n",
    "            total_counts += int(len(publications)) # counting total number of items retrieved so far\n",
    "            \n",
    "            \n",
    "            # Calculating number of pages to iterate\n",
    "            total  = data['metadata']['total']\n",
    "            rem = 0 if total % 50 == 0 else 1\n",
    "            page_size = total//50 + rem\n",
    "                   \n",
    "            \n",
    "            page_no += 1 # next paging\n",
    "            if page_no > page_size:\n",
    "                break\n",
    "            #time.sleep(0.035)    \n",
    "        else:\n",
    "            # If the request was not successful, print the error message\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            break\n",
    "    print(f'The total number of items from {db} is {total_counts}')\n",
    "    outfile.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634bbaa8",
   "metadata": {},
   "source": [
    "### BCI - BIOSIS Citation Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063cbc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting BCI retracted publications from API\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "\n",
    "bci_year= '1900-2024' # years period to get from the API\n",
    "\n",
    "# Fetch retracted publications and write to file -> 'bci_retractedpublication...'\n",
    "wos_get_retracted_pubs(data_dir+f'webofscience/bci_retractedpublication_{today}.csv',bci_year,'BCI')\n",
    "\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdeba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loding in indexed retracted publications from Biological & Chemical Information\n",
    "\"\"\"\n",
    "\n",
    "bci= pd.read_csv(data_dir+f'webofscience/bci_retractedpublication_{today}.csv')\n",
    "bci.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a7183a",
   "metadata": {},
   "source": [
    "### BIOABS - Biological Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting BIOABS retracted publications from API\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "\n",
    "bioabs_year= '1900-2024' # years period to get from the API\n",
    "\n",
    "# Fetch retracted publications and write to file -> 'bioabs_retractedpublication...'\n",
    "wos_get_retracted_pubs(data_dir+f'webofscience/bioabs_retractedpublication_{today}.csv',bioabs_year,'BIOABS')\n",
    "\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading in indexed retracted publications from Biological Abstracts\n",
    "\"\"\"\n",
    "\n",
    "bioabs= pd.read_csv(data_dir+f'webofscience/bioabs_retractedpublication_{today}.csv')\n",
    "bioabs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2315358",
   "metadata": {},
   "source": [
    "### CCC - Current Contents Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fe6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting CCC retracted publications from API\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "\n",
    "ccc_year= '1900-2024'  # years period to get from the API\n",
    "\n",
    "# Fetch retracted publications and write to file -> 'ccc_retractedpublication...'\n",
    "wos_get_retracted_pubs(data_dir+f'webofscience/CCC_retractedpublication_{today}.csv',ccc_year,'CCC')\n",
    "\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading in indexed retracted publications from Current Contents Connect\n",
    "\"\"\"\n",
    "\n",
    "ccc= pd.read_csv(data_dir+f'webofscience/ccc_retractedpublication_{today}.csv')\n",
    "ccc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1223c",
   "metadata": {},
   "source": [
    "### MEDLINE - Medical Literature Analysis and Retrieval System Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e76955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting MEDLINE retracted publications from API\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "\n",
    "medline_year= '1900-2024' # years period to get from the API\n",
    "\n",
    "# Fetch retracted publications and write to file -> 'medline_retractedpublication...'\n",
    "wos_get_retracted_pubs(data_dir+f'webofscience/medline_retractedpublication_{today}.csv',medline_year,'MEDLINE')\n",
    "\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading in indexed retracted publications from MEDLINE\n",
    "\"\"\"\n",
    "\n",
    "medline= pd.read_csv(data_dir+f'webofscience/medline_retractedpublication_{today}.csv')\n",
    "medline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce548e84",
   "metadata": {},
   "source": [
    "### WoS_Core - Web of Science Core Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa367621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# As at Feb 28 2024\n",
    "Getting Web of Science Core retracted publications from API\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "\n",
    "wos_core_year = '1920-2024' #'1920-2000' '2000-2024'\n",
    "\n",
    "# Fetch retracted publications and write to file -> 'webofsciencecore_retractedpublication...'\n",
    "wos_get_retracted_pubs(data_dir+f'webofscience/webofsciencecore_retractedpublication_{today}.csv',wos_core_year,'WOS')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec730e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading in indexed retracted publications from Web of Science Core\n",
    "\"\"\"\n",
    "\n",
    "wos_core= pd.read_csv(data_dir+f'webofscience/webofsciencecore_retractedpublication_{today}.csv')\n",
    "wos_core.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030e2ea",
   "metadata": {},
   "source": [
    "# Crossref API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e93d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function to collect data from crossrefapi\n",
    "\n",
    "def get_crossref_metadata(x):     \n",
    "    \"\"\"\n",
    "    This function fetches publication data from the Crossref API based on document type.\n",
    "    Data includes DOI, publication date, author, titles, and URL of the publication.\n",
    "    \n",
    "    :param x: specify the document type e.g. 'retraction', 'withdrawal'\n",
    "    :return: dataframe of specific records\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    metadata = {'DOI':[],'issued':[],'URL':[],'title':[],'author':[],'container-title':[]}\n",
    "    \n",
    "    for i in work.filter(update_type = x).select('DOI','issued','URL','title','author','container-title'):\n",
    "        metadata['DOI'].append(i['DOI'])\n",
    "        metadata['issued'].append(i['issued'])\n",
    "        metadata['URL'].append(i['URL'])      \n",
    "        try:\n",
    "            metadata['author'].append(i['author'])\n",
    "            \n",
    "        except:\n",
    "            metadata['author'].append('null')\n",
    "        \n",
    "        try:\n",
    "            metadata['title'].append(i['title'])\n",
    "        except:\n",
    "            metadata['title'].append('null')\n",
    "        \n",
    "        try:\n",
    "            metadata['container-title'].append(i['container-title'])\n",
    "        except:\n",
    "            metadata['container-title'].append('null')\n",
    "            \n",
    "\n",
    "\n",
    "    df = pd.DataFrame(metadata)\n",
    "    if x.lower().strip() =='withdrawal':\n",
    "        df['Doc_type']= 'Withdrawal'\n",
    "    elif x.lower().strip() =='removal':\n",
    "        df['Doc_type']= 'Removal'\n",
    "    else:\n",
    "        df['Doc_type']= 'Retraction'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4776b9",
   "metadata": {},
   "source": [
    "#### Searching Retracted Publication From Crossref API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34762f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Crossref\n",
    "#!pip install crossrefapi\n",
    "\n",
    "my_etiquette = Etiquette('My Project Name', 'My Project version', 'My Project URL', 'My contact email')\n",
    "print(str(my_etiquette))\n",
    "\n",
    "works = Works(etiquette=my_etiquette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f48cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check document type\n",
    "# Find all retraction indexing wordings\n",
    "\n",
    "work = Works()\n",
    "\n",
    "update_type = works.facet('update-type')\n",
    "update_type_df = pd.DataFrame(update_type['update-type']['values'], index=['count']).transpose().reset_index().rename(columns={'index':'document_type'})\n",
    "\n",
    "update_type_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of retraction document type\n",
    "\n",
    "update_type_retraction = update_type_df.loc[update_type_df['document_type'].isin(['retraction','Retraction', 'retracion', 'retration', 'partial_retraction','withdrawal','removal'])].set_index(['document_type'])\n",
    "\n",
    "\n",
    "retracted_tags = update_type_retraction.index.to_list()\n",
    "\n",
    "update_type_retraction.loc['Total Count'] = update_type_retraction['count'].sum()\n",
    "update_type_retraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59450575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fetching all items of retraction document type in Crossref\n",
    "\n",
    "#Uncomment line 9 & 10 to fetch retracted items from Crossref API\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "\n",
    "crossref_df= pd.DataFrame()\n",
    "for x in update_type_retraction.index:\n",
    "     crossref_df = pd.concat([crossref_df, get_crossref_metadata(x)], ignore_index=True) \n",
    "    \n",
    "end = time.time()\n",
    "   \n",
    "print(f\"Time taken: {end-start} seconds\")\n",
    "crossref_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing the data fetched from the Crossref API\n",
    "\n",
    "\"\"\"\n",
    "crossref_df = crossref_df.rename(columns={'issued':'Year', 'container-title': 'Journal','title': 'Title', 'author': 'Author'})\n",
    "\n",
    "year=[]\n",
    "title=[]\n",
    "journal=[]\n",
    "author=[]\n",
    "\n",
    "# Extracting the year title,journal title of \n",
    "for i in range(len(crossref_df)):\n",
    "    year.append(crossref_df['Year'][i]['date-parts'][0][0])\n",
    "    title.append(crossref_df['Title'][i][0])\n",
    "    journal.append(crossref_df['Journal'][i][0])\n",
    "    author.append(crossref_df['Author'][i])\n",
    "\n",
    "    \n",
    "crossref_df['Year'] = year\n",
    "crossref_df['Title'] = title\n",
    "crossref_df['Journal'] = journal\n",
    "crossref_df['Author'] = author\n",
    "\n",
    "crossref_df.Year.unique()\n",
    "\n",
    "crossref_df.fillna(0)\n",
    "crossref_df.replace('null','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save retracted items to File\n",
    "crossref_df.to_csv(data_dir+f'crossref/crossref_retractedpublication_{today}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading in indexed retracted publications from Crossref\n",
    "\"\"\"\n",
    "\n",
    "crossref = pd.read_csv(data_dir+f\"crossref/crossref_retractedpublication_{today}.csv\").drop('Unnamed: 0',axis=1)\n",
    "crossref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387b36b",
   "metadata": {},
   "source": [
    "# Step 1b: Creating Unionlist of Retracted Items\n",
    "\n",
    "This step created the unionlist of retracted items from these sources:\n",
    "-   BCI, BIOABS, CCC, Compendex, Crossref, GEOBASE, PubMed, Retraction Watch, Scopus, and Web of Science\n",
    "\n",
    "####  Input File:  \n",
    "Retracted publication records collected from Step 1a from API EXCEPT for Retraction Watch. \n",
    "<br> *Retraction Watch: download retraction records via this Crossref downlink link: https://api.labs.crossref.org/data/retractionwatch?name@email.org  This link may change. Check this Crossref webpage for update: https://doi.org/10.13003/c23rw1d9 . Afterwards, save the file as 'retractionwatch_{date}.csv' \n",
    "<br> Input files are as follows:\n",
    "   - engineeringvillage/compendex_retractedpublication_{date}.csv\n",
    "   - engineeringvillage/geobase_retractedpublication_{date}.csv\n",
    "   - crossref/crossref_retractedpublication_{date}.csv\n",
    "   - pubmed/pubmed_retractedpublication_{date}.csv\n",
    "   - retractionwatch/retractionwatch_{date}.csv\n",
    "   - scopus/scopus_retractedpublication_{date}.csv\n",
    "   - webofscience/bci_retractedpublication_{date}.csv\n",
    "   - webofscience/bioabs_retractedpublication_{date}.csv\n",
    "   - webofscience/ccc_retractedpublication_{date}.csv\n",
    "   - webofscience/medline_retractedpublication_{date}.csv\n",
    "   - webofscience/webofsciencecore_retractedpublication_{date}.csv\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "#### Output File: \n",
    "   - Union list of retracted publications\n",
    "       - unionlist/unionlist_{date}.csv\n",
    "   - Three file forms:\n",
    "       - source_recordswithdoi_ e.g. scopus_recordswithdoi_{date}.csv\n",
    "       - source_recordsnodoi_ e.g. scopus_recordsnodoi_{date}.csv\n",
    "       - source_duplicatedrecord_ e.g. scopus_duplicatedrecord_{date}.csv\n",
    "\n",
    "###### Uncomment the line of code \"....to_csv(..)\"  to save file to your local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unicode(string: str) -> str:\n",
    "    \"\"\"\n",
    "    It takes a string and passes it through different encoding parameter phases\n",
    "    E.g. '10.\\u200b1105/\\u200btpc.\\u200b010357' ->  '10.1105/tpc.010357'\n",
    "    \n",
    "    :param string: variable to be encoded\n",
    "    :return: the actual string value devoided of encoded character\n",
    "    \"\"\"\n",
    "    \n",
    "    string = unicodedata.normalize('NFKD', string).encode('iso-8859-1', 'ignore').decode('iso-8859-1')\n",
    "    string = unicodedata.normalize('NFKD', string).encode('latin1', 'ignore').decode('latin1')\n",
    "    string = unicodedata.normalize('NFKD', string).encode('cp1252', 'ignore').decode('cp1252')\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fc24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input the date you retrieve retraction publications for each database\n",
    "\n",
    "Update the date for each database format: YYYY-MM-DD e.g. 2024-02-13\n",
    "\"\"\"\n",
    "\n",
    "getdate = {'scopus': '2024-07-05',\n",
    "            'crossref':'2024-07-03',\n",
    "            'retractionwatch': '2024-07-03',\n",
    "            'pubmed': '2024-07-03',\n",
    "            'geobase': '2024-07-05',\n",
    "            'compendex': '2024-07-09',\n",
    "                \n",
    "            'bci': '2024-07-03',\n",
    "            'bioabs': '2024-07-03',\n",
    "            'ccc': '2024-07-03',\n",
    "            'medline': '2024-07-03',\n",
    "            'webofsciencecore': '2024-07-03'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe3ff8",
   "metadata": {},
   "source": [
    "### Reading in Retracted Publications from the Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ecf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in PubMed file and renaming some columns\n",
    "- Extract the publication year & retracted year and convert to 'int' type. \n",
    "\"\"\"\n",
    "pubmed = pd.read_csv(data_dir+\"/pubmed/pubmed_retractedpublication_\"+\\\n",
    "                     getdate['pubmed']+\".csv\").rename(\n",
    "    columns={'doi':'DOI',\n",
    "            'au_names':'Author',\n",
    "            'title':'Title',\n",
    "            'journal_title':'Journal',\n",
    "            'year':'Year',\n",
    "            'pmid': 'PubMedID',\n",
    "            'retraction_notice_pmid':'RetractionPubMedID',\n",
    "            'rn_doi':'RetractionDOI',\n",
    "            'retracted_year':'RetractionDate'}) #.drop(['Unnamed: 0'],axis=1\n",
    "\n",
    "pubmed['source']='PubMed'\n",
    "pubmed['Year'] = pubmed['Year'].str.split(':').str[0].astype(int)\n",
    "# pubmed['RetractionDate'] = pubmed['RetractionDate'].str.split(':').str[0].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "pubmed['DOI']= pubmed['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "pubmed['PubMedID']= pubmed['PubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str).str.strip()\n",
    "\n",
    "pubmed['RetractionPubMedID']= pubmed['RetractionPubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str)\n",
    "\n",
    "pubmed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in Retraction Watch items\n",
    "\"\"\"\n",
    "\n",
    "retractionwatch = pd.read_csv(data_dir+'/retractionwatch/retractionwatch_'+\\\n",
    "                     getdate['retractionwatch']+'.csv', encoding='latin1').rename(\n",
    "    columns={'OriginalPaperDOI':'DOI', \n",
    "             'OriginalPaperPubMedID': 'PubMedID', \n",
    "             'OriginalPaperDate': 'Year'})\n",
    "\n",
    "retractionwatch['source']='Retraction Watch'\n",
    "\n",
    "\n",
    "retractionwatch['PubMedID']= retractionwatch['PubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str).str.strip()\n",
    "\n",
    "retractionwatch['RetractionPubMedID']= retractionwatch['RetractionPubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str)\n",
    "\n",
    "#rerun\n",
    "retractionwatch.loc[retractionwatch['Year']=='1/1/1753 12:00:00 AM', 'Year'] = '1/1/1753 00:00' # 'Year' column for publication from 1753 was entered differently, causing dt.year to fail\n",
    "retractionwatch['Year']=  pd.to_datetime(retractionwatch['Year']).dt.year\n",
    "retractionwatch['RetractionYear']=  pd.to_datetime(retractionwatch['RetractionDate'], format='%m/%d/%Y', exact=False).dt.strftime(\"%Y\").fillna(0).astype(int)\n",
    "# pd.to_datetime(retractionwatch['Year']).dt.year\n",
    "\n",
    "# A row in Retraction Watch contains '|' from a row in \n",
    "retractionwatch['DOI']= retractionwatch['DOI'].str.replace(r'\\|', '') # \"10.1038/embor.2009.88 |\"\n",
    "\n",
    "retractionwatch['DOI'] = retractionwatch['DOI'].str.lower().fillna('').str.strip().apply(convert_unicode)\n",
    "\n",
    "retractionwatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8beae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in Compendex file and renaming some columns\n",
    "\"\"\"\n",
    "compendex = pd.read_csv(data_dir+\"engineeringvillage/compendex_retractedpublication_\"+\\\n",
    "                     getdate['compendex']+\".csv\")\n",
    "#            .drop('Unnamed: 0',axis=1)\n",
    "\n",
    "compendex['DOI']=  compendex['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "compendex['source']='Compendex'                        \n",
    "compendex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in GEOBASE file and renaming some columns\n",
    "\"\"\"\n",
    "geobase = pd.read_csv(data_dir+\"engineeringvillage/geobase_retractedpublication_\"+\\\n",
    "                     getdate['geobase']+\".csv\")\n",
    "\n",
    "\n",
    "geobase['DOI']=  geobase['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "geobase['source']='GEOBASE'\n",
    "geobase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7534132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in Scopus file and renaming some columns\n",
    "\"\"\"\n",
    "scopus = pd.read_csv(data_dir+'scopus/scopus_retractedpublication_'+\\\n",
    "                     getdate['scopus']+'.csv').rename(\n",
    "    columns={'Authors':'Author',\n",
    "             'Source':'source',\n",
    "             'Titles':'Title',\n",
    "             'Source title':'Journal',\n",
    "             'PubMed ID': 'PubMedID'})\n",
    "\n",
    "\n",
    "\n",
    "# Converting 'Publication_date' to Date to extract the year\n",
    "scopus['Publication_date'] = pd.to_datetime(scopus['Publication_date'])\n",
    "scopus['Year'] = scopus['Publication_date'].dt.year\n",
    "#scopus['Year'] = pd.to_numeric(scopus['Year']).fillna(0).astype(int)\n",
    "\n",
    "scopus['DOI']= scopus['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "\n",
    "scopus['PubMedID']= scopus['PubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str).str.strip()\n",
    "\n",
    "scopus['source'] = 'Scopus'\n",
    "\n",
    "scopus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in Crossref file and renaming some columns\n",
    "\"\"\"\n",
    "crossref = pd.read_csv(data_dir+'crossref/crossref_retractedpublication_'+\\\n",
    "                     getdate['crossref']+'.csv').drop('Unnamed: 0',axis=1)\n",
    "crossref['source'] = 'Crossref'\n",
    "\n",
    "crossref['DOI']= crossref['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "crossref.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in indexed retracted publications from Biological & Chemical Information\n",
    "\"\"\"\n",
    "\n",
    "bci= pd.read_csv(data_dir+f'webofscience/bci_retractedpublication_'+ \\\n",
    "                     getdate['bci']+'.csv')\n",
    "\n",
    "bci['source'] = 'BCI'\n",
    "\n",
    "bci['DOI']= bci['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "\n",
    "# Remove decimal part\n",
    "bci['PubMedID'] = bci['PubMedID'].fillna('').astype(str).str.split('.').str[0]\n",
    "\n",
    "bci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39859af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Reading in indexed retracted publications from Biological Abstracts\n",
    "\"\"\"\n",
    "bioabs= pd.read_csv(data_dir+f'webofscience/bioabs_retractedpublication_'+ \\\n",
    "                     getdate['bioabs']+'.csv')\n",
    "\n",
    "bioabs['source'] = 'BIOABS'\n",
    "\n",
    "bioabs['DOI']= bioabs['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "\n",
    "# Remove decimal part\n",
    "bioabs['PubMedID'] = bioabs['PubMedID'].fillna('').astype(str).str.split('.').str[0]\n",
    "\n",
    "bioabs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988294c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in indexed retracted publications from Current Contents Connect\n",
    "\"\"\"\n",
    "\n",
    "ccc= pd.read_csv(data_dir+f'webofscience/ccc_retractedpublication_'+ \\\n",
    "                     getdate['ccc']+'.csv')\n",
    "\n",
    "ccc['source'] = 'CCC'\n",
    "\n",
    "ccc['DOI']= ccc['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "\n",
    "# Remove decimal part\n",
    "ccc['PubMedID'] = ccc['PubMedID'].fillna('').astype(str).str.split('.').str[0]\n",
    "\n",
    "ccc.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in indexed retracted publications from Medline\n",
    "\"\"\"\n",
    "\n",
    "medline= pd.read_csv(data_dir+f'webofscience/medline_retractedpublication_'+ \\\n",
    "                     getdate['medline']+'.csv')\n",
    "\n",
    "medline['source'] = 'Medline'\n",
    "\n",
    "medline['DOI']= medline['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "\n",
    "# Remove decimal part\n",
    "medline['PubMedID'] = medline['PubMedID'].fillna('').astype(str).str.split('.').str[0]\n",
    "\n",
    "medline.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35200208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading in indexed retracted publications from Web of Science Core\n",
    "\"\"\"\n",
    "\n",
    "webofsciencecore= pd.read_csv(data_dir+f'webofscience/webofsciencecore_retractedpublication_'+\\\n",
    "                              getdate['webofsciencecore']+'.csv')\n",
    "\n",
    "webofsciencecore['source'] = 'WoS_Core'\n",
    "\n",
    "webofsciencecore['DOI']= webofsciencecore['DOI'].str.lower().str.strip().astype(str).apply(convert_unicode)\n",
    "\n",
    "# Remove decimal part\n",
    "webofsciencecore['PubMedID'] = webofsciencecore['PubMedID'].fillna('').astype(str).str.split('.').str[0]\n",
    "\n",
    "webofsciencecore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee12a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkindividualdataset(x):\n",
    "    \n",
    "    # Input dataframe names as x\n",
    "    \n",
    "    '''\n",
    "    Clean and deduplicate records based on DOIs.\n",
    "    After removing duplicates, we will return the count and the list of records with DOI, \n",
    "    those without DOIs and duplicated records that will be dropped.\n",
    "    '''\n",
    "    \n",
    "    # Fixing '0.1080/20430795.2021.1894544' DOI error instead of '10.1080/20430795.2021.1894544', \n",
    "    # a peculiarity of Retraction Watch\n",
    "    \n",
    "    # Getting the DataFrame name\n",
    "    df_name = [name for name, obj in globals().items() if obj is x][0]\n",
    "\n",
    "    if df_name == 'retractionwatch':\n",
    "            if '10.1038/embor.2009.88 |' in x['DOI'].tolist():\n",
    "                x['DOI'] = x['DOI'].replace('10.1038/embor.2009.88 |', '10.1038/embor.2009.88')\n",
    "\n",
    "#             if '0.1080/20430795.2021.1894544' in x['DOI'].tolist():\n",
    "#                 x['DOI'] = x['DOI'].replace('10.7705/biomedica.v38i0.3546)', '10.7705/biomedica.v38i0.3546')\n",
    "\n",
    "\n",
    "    \n",
    "    # Step 1: We identify the unique records of each dataset based on DOI.\n",
    "    # 'records_withDOI_hasdup': Identify records that have a valid DOI which should start with '10.'\n",
    "    # 'records_withDOI': Drop the duplicates from the previous line.\n",
    "    \n",
    "    records_withDOI_hasdup= x.loc[x['DOI'].str.startswith('10.', na=False)]\n",
    "    \n",
    "    if df_name == 'pubmed':\n",
    "        records_withDOI = records_withDOI_hasdup.drop_duplicates(subset=['DOI'], keep='last')\n",
    "    else:\n",
    "        records_withDOI = records_withDOI_hasdup.drop_duplicates(subset=['DOI'], keep='first')\n",
    "\n",
    "    \n",
    "    # Step 2: We create two duplicate lists.\n",
    "    # 'duplicated_records_all': Identify ALL duplicated records for reference and download for checking manually.\n",
    "    # 'duplicated_records': Identify duplicated records to drop but keep only the first occurrence of each group of duplicates.\n",
    "    duplicate_records_all = records_withDOI_hasdup.loc[records_withDOI_hasdup.duplicated(subset=['DOI'],keep=False), :]\n",
    "    \n",
    "    \n",
    "    # To check anamolies - to remove record with PubMedID 28202934 and leave that 26511294 with correct date\n",
    "    if df_name == 'pubmed':\n",
    "        duplicate_records = records_withDOI_hasdup.loc[records_withDOI_hasdup.duplicated(subset=['DOI'],keep='last'), :]\n",
    "    else:\n",
    "        duplicate_records = records_withDOI_hasdup.loc[records_withDOI_hasdup.duplicated(subset=['DOI'],keep='first'), :]\n",
    "\n",
    "\n",
    "    \n",
    "    # Step 3: We get the count of records without DOI. Duplicates may exist since we could not use DOI to identify duplicate records\n",
    "    records_withoutDOI = x.loc[~x['DOI'].str.startswith('10.', na=False)]    \n",
    "\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if len(x) == len(records_withDOI)+len(records_withoutDOI)+len(duplicate_records):\n",
    "            return[len(x), len(records_withDOI),len(records_withoutDOI),len(duplicate_records), records_withDOI, records_withoutDOI, duplicate_records, duplicate_records_all]\n",
    "            # return the count and items of each group\n",
    "    \n",
    "    except: \n",
    "        return('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfea489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_DOI_n_PubMedID(df, source)-> list:\n",
    "    \"\"\"\n",
    "    :param df: DataFrame to work on\n",
    "    :param source: source to lookup to determine number of count\n",
    "    \n",
    "    :return: source, # DOI, # PubMedID, # Duplicated record -> list    \n",
    "    \"\"\"  \n",
    "    \n",
    "    df_DOI= df[(df['DOI'].str.startswith('10')) & (df['source'].str.contains(source))]\n",
    "    \n",
    "    df_nodupDOI= df_DOI.drop_duplicates(subset=['DOI'], keep='first') # DF that has no duplicated DOI\n",
    "    \n",
    "    df_duplicatedDOI= df_DOI[df_DOI.duplicated(subset=['DOI'],keep='last')] # DF that are duplicated\n",
    "    \n",
    "    \n",
    "    df_noDOI= df[~(df['DOI'].str.startswith('10')) & (df['source'].str.contains(source))] # DF that has no DOI\n",
    "    \n",
    "    \n",
    "    nDOI= len(df_nodupDOI) # Numbers of items with unique DOI\n",
    "    nDuplicatedDOI= len(df_duplicatedDOI) # Numbers of items that has duplicated DOI removed\n",
    "    nNoDOI= len(df_noDOI) # Numbers of items with without DOI\n",
    "    \n",
    "    \n",
    "    \n",
    "    if 'PubMedID' in df.columns:\n",
    "        \n",
    "        df_PMID= df_DOI[((df_DOI['PubMedID'] != \"\") | ~df_DOI['PubMedID'].isna()) & (df_DOI['source'].str.contains(source))]\n",
    "        \n",
    "        df_nodupPMID= df_PMID.drop_duplicates(subset=['DOI'], keep='first') # DF that has no duplicated PMID\n",
    "        \n",
    "        df_duplicatedPMID= df_PMID[df_PMID.duplicated(subset=['DOI'],keep='last')] # DF that are duplicated\n",
    "        \n",
    "        df_noPMID= df[~(((df['PubMedID'] != \"\") | ~df['PubMedID'].isna()) & (df['source'].str.contains(source)))]\n",
    "        \n",
    "        \n",
    "        nPMID= len(df_nodupPMID) # Numbers of items with unique PMID\n",
    "        nDuplicatedPMID= len(df_duplicatedPMID)  # Numbers of items that has duplicated PMID\n",
    "        nNoPMID= len(df_noPMID)  # Numbers of items with without PMID\n",
    "        \n",
    "    else:\n",
    "        nPMID,nDuplicatedPMID,nNoPMID= 0,0,0\n",
    "        \n",
    "        \n",
    "    Total= len(df)\n",
    "        \n",
    "    return  source,Total, nDOI,nNoDOI,nDuplicatedDOI,nPMID,nNoPMID,nDuplicatedPMID #nDuplicateDOI,nPubMedID,nDuplicatePubMedID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452712a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample of Indexing of retracted item and retraction note in BCI:\n",
    "10.1080/02640414.2013.792948 [Retracted] vs. 10.1080/02640414.2013.805885 [Retraction notice]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Counting # DOIs, PubMedID\n",
    "#  return: source,Total, nDOI,nNoDOI,nDuplicatedDOI,nPMID,nNoPMID,nDuplicatedPMID \n",
    "\"\"\"\n",
    "nBCI= count_DOI_n_PubMedID(bci,'BCI')\n",
    "nBIOADS= count_DOI_n_PubMedID(bioabs,'BIOABS')\n",
    "nCCC= count_DOI_n_PubMedID(ccc,'CCC')\n",
    "nCompendex= count_DOI_n_PubMedID(compendex,'Compendex')\n",
    "nCrossref= count_DOI_n_PubMedID(crossref,'Crossref')\n",
    "nGeobase= count_DOI_n_PubMedID(geobase,'GEOBASE')\n",
    "nMedline= count_DOI_n_PubMedID(medline,'Medline')\n",
    "nPubMed=count_DOI_n_PubMedID(pubmed,'PubMed')\n",
    "nRW= count_DOI_n_PubMedID(retractionwatch,'Retraction Watch')\n",
    "nScopus= count_DOI_n_PubMedID(scopus,'Scopus')\n",
    "nWoS=count_DOI_n_PubMedID(webofsciencecore,'WoS_Core')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aggregate the items from all the sources\n",
    "\"\"\"\n",
    "dbtable = [] # A nested list which stores the records of each group in each source\n",
    "ovtable = [] # Store the count of each group from each source and create a table for viewing\n",
    "\n",
    "dblist= [nBCI, nBIOADS, nCCC, nCompendex, nCrossref,nGeobase, nMedline, nPubMed, nRW, nScopus, nWoS]\n",
    "\n",
    "#uery results retrieved\tRecords with DOI\tRecords without DOI removed\tDuplicate records removed\n",
    "# source,Total, nDOI,nNoDOI,nDuplicatedDOI,nPMID,nNoPMID,nDuplicatedPMID\n",
    "for result in dblist:\n",
    "    dbtable.append(result)\n",
    "    \n",
    "    np_results= np.array(dbtable)\n",
    "    \n",
    "# Create a table showing the count of each group\n",
    "overview = pd.DataFrame(np_results[:,[1,2,3,4,5]])\n",
    "overview.columns =['Query_result', 'Records_withDOI', 'Records_withoutDOI', 'Duplicate_DOI_removed', 'DOI_records_withPubMedID']\n",
    "overview['source']= ['BCI', 'BIOABS','CCC','Compendex','Crossref', 'GEOBASE', 'Medline', 'PubMed','Retraction Watch','Scopus','Web of Science Core']\n",
    "\n",
    "# overview.loc[len(overview)] = [overview.Query_result.sum(), overview.Records_withDOI.sum(), overview.Duplicate_DOI_removed.sum(), overview.DOI_records_withPubMedID.sum(),''] \n",
    "\n",
    "\n",
    "# Re-order columns\n",
    "overview = overview[['source', 'Query_result', 'Records_withDOI', 'Records_withoutDOI', 'Duplicate_DOI_removed', 'DOI_records_withPubMedID']]\n",
    "\n",
    "\n",
    "# Aggregating items in each column\n",
    "overview.loc[len(overview)] = ['Total',overview.Query_result.astype(int).sum(), overview.Records_withDOI.astype(int).sum(),\n",
    "                               overview.Records_withoutDOI.astype(int).sum(), \n",
    "                               overview.Duplicate_DOI_removed.astype(int).sum(), overview.DOI_records_withPubMedID.astype(int).sum(),] \n",
    "\n",
    "\n",
    "\n",
    "overview.to_csv(result_dir+'datasources_overview.csv')\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99de30",
   "metadata": {},
   "source": [
    "### Exporting Records to Folder\n",
    "- These include DOIs, deduplicated DOIs, and No DOI records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets_from_sources(source_list: list, dbtable: list,source_names:list):\n",
    "    \"\"\"\n",
    "    This function filter records with records with doi,without doi, and those with duplicated doi\n",
    "    for each source and save each file\n",
    "    \n",
    "    :param source_list: list of dataframe of sources. This is dblist. It is initiated later\n",
    "    :return: It will for each sources their: \n",
    "            - records with doi (saved as in this format: 'source_name_recordswithdoi_date')\n",
    "            - records without doi (saved as in this format: 'source_name_recordsnodoi_date')\n",
    "            - duplicated doi records (saved as in this format: 'source_name_duplicatedrecords_date')\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for i in range(len(source_list[:])):\n",
    "#         source_name = [name for name, obj in globals().items() if obj is source_list[i]][0]\n",
    "        source_name= source_names[i]\n",
    "#         if source_name== 'wos_core':\n",
    "#             source_name= 'webofsciencecore'\n",
    "#         print(source_name)\n",
    "\n",
    "        date = getdate[source_name]\n",
    "        \n",
    "        print(f'{source_name} was last updated on {date}')\n",
    "        \n",
    "        if source_name in ['compendex','geobase']:\n",
    "            dbtable[i][4].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+ 'engineeringvillage'+'/'+source_name+'_recordswithdoi_'+date + '.csv')\n",
    "            dbtable[i][5].to_csv(data_dir+ 'engineeringvillage'+'/'+source_name+'_recordsnodoi_' + date + '.csv')\n",
    "            dbtable[i][6].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+ 'engineeringvillage'+'/'+source_name+ '_duplicatedrecords_' + date + '.csv')\n",
    "\n",
    "        \n",
    "        elif source_name in ['bci','bioabs','ccc','medline','webofsciencecore']:\n",
    "            dbtable[i][4].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+ 'webofscience'+'/'+source_name+'_recordswithdoi_'+date + '.csv')\n",
    "            dbtable[i][5].to_csv(data_dir+ 'webofscience'+'/'+source_name+'_recordsnodoi_' + date + '.csv')\n",
    "            dbtable[i][6].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+ 'webofscience'+'/'+source_name+ '_duplicatedrecords_' + date + '.csv')\n",
    "\n",
    "      \n",
    "        else:\n",
    "            dbtable[i][4].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+ source_name+'/'+source_name+'_recordswithdoi_'+date + '.csv')\n",
    "            dbtable[i][5].to_csv(data_dir+ source_name+'/'+source_name+'_recordsnodoi_' + date + '.csv')\n",
    "            dbtable[i][6].sort_values(by=['DOI'], ascending=False).to_csv(data_dir+ source_name+'/'+source_name+ '_duplicatedrecords_' + date + '.csv')\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving records of items with DOIs, without DOIs, and duplicated records in each of the sources\n",
    "\n",
    "dbtable2 = [] # A nested list which stores the records of each group in each source\n",
    "dblist2 = [bci, bioabs, ccc,compendex, crossref,geobase, medline, pubmed, retractionwatch, scopus, webofsciencecore,]\n",
    "source_names= ['bci', 'bioabs', 'ccc','compendex', 'crossref','geobase', 'medline', 'pubmed', 'retractionwatch', 'scopus', 'webofsciencecore']\n",
    "for x in dblist2[:]:\n",
    "    dbtable2.append(checkindividualdataset(x))\n",
    "\n",
    "export_datasets_from_sources(dblist2[:], dbtable2,source_names) # [-1] is to exclude '' in dblist2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c5a5e",
   "metadata": {},
   "source": [
    "### Formation of Unionlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89296f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying retracted items with DOI for each source\n",
    "\n",
    "bci_retracted = pd.read_csv(data_dir+'webofscience/bci_recordswithdoi_'+getdate['bci']+'.csv')\n",
    "\n",
    "bioabs_retracted = pd.read_csv(data_dir+'webofscience/bioabs_recordswithdoi_'+getdate['bioabs']+'.csv')\n",
    "\n",
    "ccc_retracted = pd.read_csv(data_dir+'webofscience/ccc_recordswithdoi_'+getdate['ccc']+'.csv')\n",
    "\n",
    "compendex_retracted = pd.read_csv(data_dir+'engineeringvillage/compendex_recordswithdoi_'+getdate['compendex']+'.csv')\n",
    "\n",
    "crossref_retracted= pd.read_csv(data_dir+'crossref/crossref_recordswithdoi_'+getdate['crossref'] +'.csv')\n",
    "\n",
    "pubmed_retracted= pd.read_csv(data_dir+'pubmed/pubmed_recordswithdoi_'+getdate['pubmed'] +'.csv')\n",
    "\n",
    "geobase_retracted = pd.read_csv(data_dir+'engineeringvillage/'+'geobase_recordswithdoi_'+getdate['geobase']+'.csv')\n",
    "\n",
    "medline_retracted = pd.read_csv(data_dir+'webofscience/medline_recordswithdoi_'+getdate['ccc']+'.csv')\n",
    "\n",
    "retractionwatch_retracted= pd.read_csv(data_dir+'retractionwatch/retractionwatch_recordswithdoi_'+getdate['retractionwatch'] +'.csv')\n",
    "\n",
    "scopus_retracted = pd.read_csv(data_dir+'scopus/scopus_recordswithdoi_'+getdate['scopus']+'.csv')\n",
    "\n",
    "webofsciencecore_retracted = pd.read_csv(data_dir+'webofscience/webofsciencecore_recordswithdoi_'+getdate['webofsciencecore'] +'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e87c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged into one full list 'The Union List'\n",
    "\n",
    "\"\"\"\n",
    "Select PubMed with doi and \n",
    "Select the columns: 'DOI', 'Author','Title', 'Year', 'Journal', 'source', 'PubMedID', -> 'union_list'\n",
    "\"\"\"\n",
    "retracted_sources= [bci_retracted, bioabs_retracted, ccc_retracted, compendex_retracted, \n",
    "                    crossref_retracted, pubmed_retracted, geobase_retracted, medline_retracted,\n",
    "                    retractionwatch_retracted, scopus_retracted, webofsciencecore_retracted]\n",
    "\n",
    "merged_withdoi = pd.concat(retracted_sources)\n",
    "merged_withdoi = merged_withdoi[['DOI', 'Author','Title', 'Year', 'Journal', 'source', 'PubMedID']].sort_values(by='DOI')\n",
    "\n",
    "# Check if the number of records are consistent before and after merging.\n",
    "if len(merged_withdoi) == (len(bci_retracted) + len(bioabs_retracted) + len(ccc_retracted) + len(compendex_retracted)+\\\n",
    "                        len(crossref_retracted) + len(pubmed_retracted)+ len(geobase_retracted) + len(medline_retracted)+\\\n",
    "                        len(retractionwatch_retracted)+len(scopus_retracted)+ len(webofsciencecore_retracted)):\n",
    "\n",
    "    print('full record count:', len(merged_withdoi))\n",
    "\n",
    "else:\n",
    "    print('ERROR: Inconsistent Counts') #185513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_withdoi['source']=merged_withdoi['source'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e2dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It merges file from all the sources using their DOIs, and joins the source that of the items\n",
    "E.g. DOI: 10.1001/archdermatol.2012.418 Source: PubMed; Web of Science; Retraction Watch\n",
    "Output File: One merged CSV file as final output - \n",
    "\n",
    "Uses date as of today (last update): '2023-07-09 to indicate last update the items were merged\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "unionlist = merged_withdoi.groupby('DOI').agg({'Author':'first', \n",
    "                              'Title': 'last',\n",
    "                              'Year': 'first', #lambda x: x.min(), #'Year': 'first'\n",
    "                              'Journal': 'last',\n",
    "                              'source':'; '.join, \n",
    "                              'PubMedID':'first'}).reset_index()\n",
    "\n",
    "\n",
    "unionlist['PubMedID']= unionlist['PubMedID'].fillna(0).astype(int).replace(0,'').astype(str)\n",
    "unionlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d29e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saving output File\n",
    "Output saved as: 'union_list.csv_{today}' # merged CSV file from all sources\n",
    "\"\"\"\n",
    "unionlist.to_csv(data_dir+'unionlist/unionlist_2024-07-09.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
