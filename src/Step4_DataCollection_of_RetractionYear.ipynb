{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82180fef",
   "metadata": {},
   "source": [
    "### STEP 4: Data Collection of Retraction Year\n",
    "This file collects retraction years for retracted items for the following sources:\n",
    "1. Retraction Watch\n",
    "2. PubMed\n",
    "3. Web of Science platform: i. BIOABS ii. BCI iii. CCC and iv. Web of Science Core\n",
    "4. Crossref \n",
    "\n",
    "Input Files:\n",
    "- unionlist/unionlist_with_nodoi_{date}.csv\n",
    "- webofscience/bci_retractedpublication_{date}.csv\n",
    "- webofscience/bioabs_retractedpublication_{date}.csv\n",
    "- webofscience/ccc_retractedpublication_{date}.csv\n",
    "- webofscience/webofsciencecore_retractedpublication_{date}.csv\n",
    "- crossref/crossref_recordswithdoi_{date}.csv\n",
    "- retractionwatch/retractionwatch_{date}.csv\n",
    "\n",
    "Output File:\n",
    "- pubmed/pubmed_retractionyear_{date}.csv\n",
    "- webofscience/bci_retractionyear_{date}.csv\n",
    "- webofscience/bioabs_retractionyear_{date}.csv\n",
    "- webofscience/ccc_retractionyear_{date}.csv\n",
    "- webofscience/webofsciencecore_retractionyear_{date}.csv\n",
    "- crossref/crossref_retractionyear_{date}.csv\n",
    "- unionlist/unionlist_with_retractionyear_{date}.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time,datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b185ace",
   "metadata": {},
   "source": [
    "### Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targeting the retraction_index_path\n",
    "retraction_index_path = os.path.abspath('./.')\n",
    "retraction_index_path\n",
    "\n",
    "data_dir = retraction_index_path+'/data/' # data directory\n",
    "result_dir = retraction_index_path+'/result/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97592a5",
   "metadata": {},
   "source": [
    "### Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d76d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "con_file = open(retraction_index_path+\"/config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()\n",
    "\n",
    "# Initializing variable for configuration file\n",
    "my_email = config['my_email']\n",
    "elsevier_api_key = config['Elsevier_APIKEY']\n",
    "elsevier_insttoken = config['insttoken']\n",
    "ieee_xplore_api_key = config['IEEEXplore_APIKEY']\n",
    "wos_api_key = config['WoS_APIKEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global initializatiion\n",
    "global my_email\n",
    "global elsevier_api_key\n",
    "global elsevier_insttoken\n",
    "global ieee_xplore_api_key\n",
    "global wos_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unicode(string: str) -> str:\n",
    "    \"\"\"\n",
    "    It takes a string and passes it through different encoding parameter phases\n",
    "    E.g. '10.\\u200b1105/\\u200btpc.\\u200b010357' ->  '10.1105/tpc.010357'\n",
    "    \n",
    "    :param string: variable to be encoded\n",
    "    :return: the actual string value devoided of encoded character\n",
    "    \"\"\"\n",
    "    \n",
    "    string = unicodedata.normalize('NFKD', string).encode('iso-8859-1', 'ignore').decode('iso-8859-1')\n",
    "    string = unicodedata.normalize('NFKD', string).encode('latin1', 'ignore').decode('latin1')\n",
    "    string = unicodedata.normalize('NFKD', string).encode('cp1252', 'ignore').decode('cp1252')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25962372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_items(pmids:list, cut:int)-> list[list]:\n",
    "    \"\"\"\n",
    "    It divides the list pmids into batches for processing. \n",
    "    :param pmids: list of pmids \n",
    "    :param cut: maximum number of records to assign to a batch\n",
    "    \n",
    "    :return: list of list of batches of pmids\n",
    "    \"\"\"\n",
    "    pmids_batches=[]\n",
    "    \n",
    "    while len(pmids) >= cut:\n",
    "        selected_pmids= pmids[:cut]\n",
    "        pmids_batches.append(selected_pmids)\n",
    "#         print(selected_pmids)    \n",
    "        pmids = pmids[cut:]\n",
    "\n",
    "    if pmids:\n",
    "        pmids_batches.append(pmids)\n",
    "#         print(pmids)\n",
    "\n",
    "    return pmids_batches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8149cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Inputting last date when the retraction publication were collected each source as stated in STEP 1 notebook\n",
    "\"\"\"\n",
    "getdate = {'scopus': '2024-07-05',\n",
    "            'crossref':'2024-07-03',\n",
    "            'retractionwatch': '2024-07-03',\n",
    "            'pubmed': '2024-07-03',\n",
    "           \n",
    "            'geobase': '2024-07-05',\n",
    "            'compendex': '2024-07-09',\n",
    "                \n",
    "            'bci': '2024-07-03',\n",
    "            'bioabs': '2024-07-03',\n",
    "            'ccc': '2024-07-03',\n",
    "            'medline': '2024-07-03',\n",
    "            'webofsciencecore': '2024-07-03',\n",
    "          \n",
    "            'unionlist':'2024-07-09'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(datetime.date.today())\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a42c71",
   "metadata": {},
   "source": [
    "# PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fcfe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pmids(term:str,mindate:int, maxdate:int)->list:\n",
    "    \"\"\"\n",
    "    It retrieves pmids for a given search term.\n",
    "    \n",
    "    :param term: search term\n",
    "    :param mindate: the year to start the search\n",
    "    :param maxdate: the year to end the search\n",
    "    \n",
    "    :return: list of all pmids of the records retrieved\n",
    "    \"\"\"\n",
    "    \n",
    "    api_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    \n",
    "    # Step 1: Search for retracted papers\n",
    "    \n",
    "    email = my_email # Supply your email\n",
    "    \n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": term,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": 10000,  # Maximum number of results per request\n",
    "        \"mindate\": mindate,\n",
    "        \"maxdate\": maxdate\n",
    "    }\n",
    "\n",
    "    # Step 2: Send a GET request to the PubMed API to search for retracted papers\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    total_results = int(data[\"esearchresult\"][\"count\"])\n",
    "    pmids = data[\"esearchresult\"][\"idlist\"]\n",
    "    return total_results, pmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_pmids(term:str, start_year:int, end_year:int, interval_year:int)->list:\n",
    "    \"\"\"\n",
    "    It retrieves pmids for a given search term over a period of time using retrieve_pmids function. \n",
    "    It re-iterates at a defined interval year because up to 10,000 records maximum can retrieved \n",
    "    from PubMed at a time. Check: https://www.ncbi.nlm.nih.gov/books/NBK25501/ for details\n",
    "    \n",
    "    :param term: search term\n",
    "    :param start_year: the year to start the search\n",
    "    :param end_year: the year to end the search\n",
    "    :param interval_year: interval between year batches\n",
    "    \n",
    "    :return: list of all pmids of the records retrieved\n",
    "    \"\"\"\n",
    "    all_pmids = []\n",
    "    total_pmids_count = 0\n",
    "    current_year = end_year\n",
    "    \n",
    "    # Iterate over the years with a stipulated year interval of 10,000 records maximum limitation\n",
    "    for year in range(start_year, end_year +1, interval_year):\n",
    "        end_year = year + interval_year-1\n",
    "        if (current_year - year) < interval_year:\n",
    "            end_year = current_year\n",
    "        \n",
    "        count, pmids_per_interval = retrieve_pmids(term,year,end_year)\n",
    "        total_pmids_count+=count\n",
    "        all_pmids+=pmids_per_interval\n",
    "        \n",
    "        print(f'{year} - {end_year}: {count} total number of retrieved pmids')\n",
    "        \n",
    "    return total_pmids_count,all_pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_retracted_paper_data_from_metadata(pmid:list):\n",
    "    \"\"\"\n",
    "    It retrieves XML of a given pmid\n",
    "    \n",
    "    :param pmid: the pmid of a given publication\n",
    "    :return: XML of the pmid of a given publication\n",
    "    \"\"\"\n",
    "    \n",
    "    #api_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    efetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "\n",
    "    email = my_email       # Supply your email\n",
    "    \n",
    "#   Retrieve paper's XML details\n",
    "    params = {\n",
    "            \"db\": \"pubmed\",\n",
    "            \"id\": pmid,\n",
    "            \"retmode\": \"xml\"\n",
    "        }\n",
    "\n",
    "    # Process the XML response to extract the desired paper details\n",
    "    # (e.g., title, authors, abstract, etc.)\n",
    "    response = requests.get(efetch_url, params=params)\n",
    "    paper_xml = response.text\n",
    "    \n",
    "    return paper_xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_retraction_notice_metadata(soup:bs)->list:\n",
    "    \"\"\"\n",
    "    It extracts data from the XML metadata for retraction notice\n",
    "    \n",
    "    :param soup: article in Beautifulsoup XML format\n",
    "    \n",
    "    :return: list of extracted data from the XML metadata\n",
    "    \"\"\" \n",
    "    # Initialize all variables\n",
    "    rn_pmid, doi = '',''   # retraction notice paper's pmid and doi\n",
    "    pub_year, year, month, day = '','9999','99','99'   # publication of the retracted paper\n",
    "    jour_abrv,jour_title, title = '','',''  # journal title, abbreviation and article title of the retracted paper\n",
    "    pub_type = '' # publication type of the retracted paper such as letter, article etc.\n",
    "    retractionOf = None\n",
    "     \n",
    "    try:\n",
    "\n",
    "        # extract pmid\n",
    "        if soup.PMID: # pmid\n",
    "            rn_pmid = soup.PMID.string\n",
    "        elif soup.ArticleIdList.find(IdType=\"pubmed\"):\n",
    "            rn_pmid = soup.ArticleIdList.find(IdType=\"pubmed\").string\n",
    "            #print(pmid)\n",
    "\n",
    "        # extract doi\n",
    "        if soup.ArticleIdList.find(IdType=\"doi\") is not None:\n",
    "            doi = soup.ArticleIdList.find(IdType=\"doi\").string\n",
    "\n",
    "        # fetching publication year\n",
    "        if soup.ArticleDate:\n",
    "            if soup.ArticleDate.Year is not None:\n",
    "                year = soup.ArticleDate.Year.string\n",
    "\n",
    "            if soup.ArticleDate.Month is not None:\n",
    "                month = soup.ArticleDate.Month.string\n",
    "\n",
    "            if soup.ArticleDate.Day is not None:\n",
    "                day = soup.ArticleDate.Day.string\n",
    "\n",
    "        elif soup.PubDate:\n",
    "            if soup.PubDate.Year is not None:\n",
    "                year = soup.PubDate.Year.string\n",
    "\n",
    "            if soup.PubDate.Month is not None:\n",
    "                month = soup.PubDate.Month.string\n",
    "\n",
    "            if soup.PubDate.Day is not None:\n",
    "                day = soup.PubDate.Day.string        \n",
    "    \n",
    "        \n",
    "        if year == '9999':\n",
    "            \"\"\"\n",
    "            <PubMedPubDate PubStatus=\"pubmed\">\n",
    "            <PubMedPubDate PubStatus=\"medline\">\n",
    "            <PubMedPubDate PubStatus=\"entrez\">\n",
    "            \"\"\"\n",
    "\n",
    "            if soup.find_all('PubMedPubDate', {'PubStatus': \"pubmed\"}):\n",
    "                pub_date_elements = soup.find_all('PubMedPubDate', {'PubStatus': \"pubmed\"})\n",
    "                for pub_date in pub_date_elements:\n",
    "                    if pub_date.find(\"Year\").text:\n",
    "                        year = pub_date.find(\"Year\").text\n",
    "                    if pub_date.find(\"Month\").text:\n",
    "                        month = pub_date.find(\"Month\").text\n",
    "                    if pub_date.find(\"Day\").text:\n",
    "                        day = pub_date.find(\"Day\").text\n",
    "                    \n",
    "            elif soup.find_all('PubMedPubDate', {'PubStatus': \"medline\"}):\n",
    "                pub_date_elements = soup.find_all('PubMedPubDate', {'PubStatus': \"medline\"})\n",
    "                for pub_date in pub_date_elements:\n",
    "                    if pub_date.find(\"Year\").text:\n",
    "                        year = pub_date.find(\"Year\").text\n",
    "                    if pub_date.find(\"Month\").text:\n",
    "                        month = pub_date.find(\"Month\").text\n",
    "                    if pub_date.find(\"Day\").text:\n",
    "                        day = pub_date.find(\"Day\").text\n",
    "\n",
    "            elif soup.find_all('PubMedPubDate', {'PubStatus': \"entrez\"}):\n",
    "                pub_date_elements = soup.find_all('PubMedPubDate', {'PubStatus': \"entrez\"})\n",
    "                for pub_date in pub_date_elements:\n",
    "                    if pub_date.find(\"Year\").text:\n",
    "                        year = pub_date.find(\"Year\").text\n",
    "                    if pub_date.find(\"Month\").text:\n",
    "                        month = pub_date.find(\"Month\").text\n",
    "                    if pub_date.find(\"Day\").text:\n",
    "                        day = pub_date.find(\"Day\").text        \n",
    "            \n",
    "        pub_year = f'{year}:{month}:{day}'\n",
    "\n",
    "\n",
    "        # extract title\n",
    "        if soup.ArticleTitle is not None:\n",
    "            title = soup.ArticleTitle.string\n",
    "                #print(title)\n",
    "\n",
    "        # extract journal title\n",
    "        if soup.Title is not None:\n",
    "            jour_title = soup.Title.string\n",
    "            \n",
    "        # extract journal title abbreviation\n",
    "        if soup.ISOAbbreviation is not None:\n",
    "            jour_abrv = soup.ISOAbbreviation.string\n",
    "\n",
    "\n",
    "       \n",
    "        #extract publication types\n",
    "        if soup.PublicationTypeList is not None:\n",
    "                pub_type = soup.PublicationTypeList.find_all()\n",
    "                pub_type = ';'.join([pub.string for pub in pub_type])\n",
    "\n",
    "        elif soup.PublicationType is not None:\n",
    "                pub_type = check_soup.PublicationType.string\n",
    "\n",
    "\n",
    "        # Checking Attribute 'RetractionOf' to see if the PMID is a retraction notice\n",
    "        retraction_of = soup.find('CommentsCorrections', attrs={'RefType': 'RetractionOf'})\n",
    "        #print(retraction_of)\n",
    "        pmid = retraction_of.find('PMID')\n",
    "        if pmid is not None:\n",
    "            retractionOf = pmid.text\n",
    "\n",
    "\n",
    "    except Exception as e: \n",
    "        pass\n",
    "        print(f'Error at {rn_pmid} with {doi}')\n",
    "\n",
    "\n",
    "    return [rn_pmid, doi,pub_year,title,pub_type,\n",
    "            jour_title,jour_abrv,retractionOf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in PubMed file and renaming some columns\n",
    "- Extract the publication year & retracted year and convert to 'int' type. \n",
    "\"\"\"\n",
    "pubmed = pd.read_csv(data_dir+\"/pubmed/pubmed_retractedpublication_\"+getdate['pubmed']+\".csv\").rename(\n",
    "    columns={'doi':'DOI',\n",
    "            'au_names':'Author',\n",
    "            'title':'Title',\n",
    "            'journal_title':'Journal',\n",
    "            'year':'Year',\n",
    "            'pmid': 'PubMedID',\n",
    "            'retraction_notice_pmid':'RetractionPubMedID',\n",
    "            'rn_doi':'RetractionDOI',\n",
    "            'retracted_year':'RetractionDate'}) #.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "pubmed['source']='PubMed'\n",
    "#pubmed['Year'] = pubmed['Year'].str.split(':').str[0].astype(int)\n",
    "#pubmed['RetractionDate'] = pubmed['RetractionDate'].str.split(':').str[0].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "pubmed['DOI']= pubmed['DOI'].str.strip().astype(str).apply(convert_unicode)\n",
    "pubmed['PubMedID']= pubmed['PubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str).str.strip()\n",
    "\n",
    "pubmed['Year'] = pubmed['Year'].str.split(':').str[0].astype(int)\n",
    "\n",
    "pubmed['RetractionPubMedID']= pubmed['RetractionPubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str).str.strip()\n",
    "\n",
    "pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f41c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get retraction PubMedID, in which will be used to fetch retraction year\n",
    "\"\"\"\n",
    "retraction_notice_pmids= pubmed['RetractionPubMedID'].to_list()\n",
    "no_records = 300\n",
    "retraction_notice_pmids_batches=batch_items(retraction_notice_pmids,no_records) \n",
    "\n",
    "print(f'The retraction notice pmids are divided into {len(retraction_notice_pmids_batches)} batches')\n",
    "print(f'There are {len(pubmed)} pmids divided into lists in which each list contains {no_records} records maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61781da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this cell to get this cell to get data from the API. \n",
    "\n",
    "Running the pipeline for the 'retracion_notice_pmids' and save as \n",
    "'pubmed_retractiondetail.csv'\n",
    "\"\"\"\n",
    "\n",
    "rn_header = ['retraction_notice_pmid', 'rn_doi', 'retracted_year', 'title', 'pub_type',\n",
    "       'journal_title', 'journal_abrv', 'retractionOf_pmid']\n",
    "\n",
    "outfile = open(data_dir+\"pubmed/pubmed_retractiondetail.csv\",\"w\",encoding = \"utf-8\", newline = \"\")\n",
    "csvout = csv.writer(outfile)\n",
    "csvout.writerow(rn_header)\n",
    "\n",
    "result_per_paper = []\n",
    "count =1\n",
    "\n",
    "for selected_pmids in tqdm(retraction_notice_pmids_batches):\n",
    "    all_results= []\n",
    "    print(f'batch {count}/{len(retraction_notice_pmids_batches)}: {len(selected_pmids)} records')\n",
    "\n",
    "    retraction_notice_papers_xml = retrieve_retracted_paper_data_from_metadata(selected_pmids)\n",
    "    \n",
    "    \n",
    "    rn_soup = bs(retraction_notice_papers_xml,'xml') \n",
    "    #print(soup)\n",
    "    rn_papers_xml = rn_soup.find_all('PubmedArticle') # <PubmedArticle> \n",
    "     \n",
    "    time.sleep(10)\n",
    "    \n",
    "    for per_paper_xml in rn_papers_xml:\n",
    "        result_per_paper = extract_retraction_notice_metadata(per_paper_xml)\n",
    "        #print(result_per_paper)\n",
    "        all_results.append(result_per_paper)\n",
    "        csvout.writerow(result_per_paper)\n",
    "    \n",
    "#     rn_csvout.writerows(all_results)\n",
    "    count+=1\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a8e9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load already-gotten retraction data from PubMed in the above cell\n",
    "\"\"\"\n",
    "\n",
    "pubmed_retraction = pd.read_csv(data_dir+f\"pubmed/pubmed_retractiondetail.csv\",encoding= 'utf-8').rename(    \n",
    "            columns={\n",
    "            'title':'Title',\n",
    "            'journal_title':'Journal',\n",
    "            'year':'Year',\n",
    "            'pmid': 'PubMedID',\n",
    "            'retraction_notice_pmid':'RetractionPubMedID', \n",
    "            'rn_doi':'RetractionDOI',\n",
    "            'retracted_year':'RetractionYear'})\n",
    "\n",
    "pubmed_retraction['RetractionYear'] = pubmed_retraction['RetractionYear'].str.split(':').str[0].astype(int)\n",
    "pubmed_retraction['RetractionPubMedID']= pubmed_retraction['RetractionPubMedID'].astype(str).str.strip()\n",
    "\n",
    "pubmed_retraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_retraction_pmids = [int(x) for x in pubmed_retraction['RetractionPubMedID'].to_list()]\n",
    "\n",
    "retraction_pmid_undone = set(retraction_notice_pmids) - set(pubmed_retraction_pmids)\n",
    "len(retraction_pmid_undone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5329bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save RetractionYear for PubMed\n",
    "\n",
    "ToDo:\n",
    "Uncomment \".to_csv...\" to save the file\n",
    "\"\"\"\n",
    "# Deduplicate the pubmed_retraction\n",
    "pubmed_retraction.drop_duplicates(subset=['RetractionPubMedID'], keep='first', inplace= True)\n",
    "\n",
    "# Merge the PubMed retraction (retraction notice item) with the PubMed (retracted item)\n",
    "pubmed_updated = pd.merge(pubmed,pubmed_retraction.iloc[:,:3], on='RetractionPubMedID', how='left')\n",
    "\n",
    "# Replace RetractionYear with NaN value with zero - 0\n",
    "pubmed_updated['RetractionYear']= pubmed_updated['RetractionYear'].fillna(0).astype(int)\n",
    "\n",
    "pubmed_updated\\\n",
    "#     .to_csv(data_dir+'pubmed/pubmed_retractionyear_'+getdate['pubmed']+'.csv')\n",
    "\n",
    "pubmed_updated[['PubMedID','DOI','Year','RetractionPubMedID','RetractionYear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74572b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_updated[['PubMedID','DOI','Year','RetractionPubMedID','RetractionYear']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56fa94",
   "metadata": {},
   "source": [
    "# Web of Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following databases: i. BIOABS ii. BCI iii. CCC and iv. Web of Science Core of Web of Science platform \n",
    "have  patterns in item title that say:\n",
    "\".... (Retracted article. See vol. 122, 2021\".  It will contain \"Retracted article\" and \"See\".\n",
    "Hence we are extracting the retraction year from the title with pattern recognition.\n",
    "\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readin_wosDBs_files(df_dir: str):  \n",
    "    df = pd.read_csv(df_dir).rename(\n",
    "    columns={'Authors':'Author', \n",
    "             'Article Title': 'Title', \n",
    "             'Source Title': 'Journal', \n",
    "             'Publication Year': 'Year', \n",
    "             'Pubmed Id': 'PubMedID'})#.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "\n",
    "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce') \n",
    "    df['Year'] = df['Year'].fillna(0)\n",
    "    df['Year'] = df['Year'].astype(float).astype(int)\n",
    "\n",
    "    df['DOI']= df['DOI'].str.strip().astype(str).apply(convert_unicode)\n",
    "\n",
    "    #df['PubMedID'] = df['PubMedID'].fillna('').replace('nan','').str.replace('.0', '',regex=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63907bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wos_retractionyear(title: str):\n",
    "    \"\"\"\n",
    "    It gets retraction year from the title of the article\n",
    "    \n",
    "    :param title: the title of the article (input data)\n",
    "    :return: retracted year\n",
    "    \"\"\"\n",
    "    \n",
    "    if title=='':\n",
    "        return '0'\n",
    "    \n",
    "    pattern = r'(?i)Retracted.*'# r'Retracted.*?see.*?(\\d{4})'  ?(\\d{4})\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        match = re.search(pattern, title)\n",
    "        new_text= re.sub(r'\\s+', ' ', match[0])\n",
    "        new_text= re.sub(r'[,().]', '', new_text)\n",
    "        retractionyear= new_text.split(' ')[-1]\n",
    "        if len(retractionyear)==4:\n",
    "            return retractionyear\n",
    "        else:\n",
    "            return '0'\n",
    "        \n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def wosDBs_get_retractionyear(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Extracting the retraction year from the title with pattern:\n",
    "    \".... (Retracted article. See vol. 122, 2021\".  It will contain \"Retracted article\" and \"See\", thereafter,\n",
    "    extract the four digits that represents the retracted year\n",
    "    \"\"\"\n",
    "\n",
    "    # Removing excessive whitespace from title\n",
    "    df['Title']= df['Title'].fillna('')\n",
    "\n",
    "    df['Title']= df['Title'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "\n",
    "\n",
    "    df_updated= df.copy()\n",
    "\n",
    "    # initialize row with no retraction year value to zero (0)\n",
    "    df_updated['RetractionYear']= df_updated['Title'].apply(get_wos_retractionyear)\n",
    "    \n",
    "    df_updated['RetractionYear']= df_updated['RetractionYear'].fillna('0')\n",
    "    \n",
    "    df_updated['RetractionYear']= df_updated['RetractionYear'].astype(int)\n",
    "\n",
    "    df_withRetractionYear = len(df_updated[df_updated['RetractionYear']!=0])\n",
    "\n",
    "    print(f'The total number of records with retraction year is {df_withRetractionYear} items')\n",
    "\n",
    "    return df_updated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load BCI retracted dataset and extract retraction year from the Title\n",
    "\"\"\"\n",
    "\n",
    "bci_dir = data_dir+f\"webofscience/bci_retractedpublication_{getdate['bci']}.csv\"\n",
    "bci = readin_wosDBs_files(bci_dir)\n",
    "\n",
    "bci_updated= wosDBs_get_retractionyear(bci)\n",
    "\n",
    "bci_updated\\\n",
    "#     .to_csv(data_dir+'webofscience/bci_retractionyear_'+getdate['bci']+'.csv')\n",
    "bci_updated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9d1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load BIOABS retracted dataset and extract retraction year from the Title\n",
    "\"\"\"\n",
    "\n",
    "bioabs_dir = data_dir+f\"webofscience/bioabs_retractedpublication_{getdate['bioabs']}.csv\"\n",
    "bioabs = readin_wosDBs_files(bioabs_dir)\n",
    "\n",
    "bioabs_updated= wosDBs_get_retractionyear(bioabs)\n",
    "bioabs_updated\\\n",
    "     .to_csv(data_dir+'webofscience/bioabs_retractionyear_'+getdate['bioabs']+'.csv')\n",
    "\n",
    "bioabs_updated.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fcd77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load CCC retracted dataset and extract retraction year from the Title\n",
    "\"\"\"\n",
    "ccc_dir = data_dir+f\"webofscience/ccc_retractedpublication_{getdate['ccc']}.csv\"\n",
    "ccc = readin_wosDBs_files(ccc_dir)\n",
    "\n",
    "ccc_updated= wosDBs_get_retractionyear(ccc)\n",
    "ccc_updated\\\n",
    "#     .to_csv(data_dir+'webofscience/ccc_retractionyear_'+getdate['ccc']+'.csv')\n",
    "\n",
    "ccc_updated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load Web of Science Core retracted dataset and extract retraction year from the Title\n",
    "\"\"\"\n",
    "woscore_dir = data_dir+f\"webofscience/webofsciencecore_retractedpublication_{getdate['webofsciencecore']}.csv\"\n",
    "\n",
    "woscore = readin_wosDBs_files(woscore_dir) #.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "woscore_updated= wosDBs_get_retractionyear(woscore)\n",
    "woscore_updated\\\n",
    "#    .to_csv(data_dir+'webofscience/webofsciencecore_retractionyear_'+getdate['webofsciencecore']+'.csv')\n",
    "\n",
    "woscore_updated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8054105",
   "metadata": {},
   "source": [
    "# Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Reading in Crossred file and extract its retracted item DOIs in which will be use to search their\n",
    "  retraction year\n",
    "\"\"\"\n",
    "\n",
    "crossref = pd.read_csv(data_dir+f\"crossref/crossref_recordswithdoi_{getdate['crossref']}.csv\").drop('Unnamed: 0',axis=1)\n",
    "crossref.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cffe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracting the Crossref retracted item DOIs for RetractionYear request from its API\n",
    "\"\"\"\n",
    "\n",
    "crossref_dois = crossref[~crossref['DOI'].isna()]['DOI']\n",
    "crossref_dois= list(set(crossref_dois))\n",
    "print(len(crossref_dois))\n",
    "\n",
    "# Split the extracted DOIs in 10 batches\n",
    "crossref_dois_chunks = np.array_split(list(crossref_dois), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Searching for the retraction detail via Crossref API: retraction year and reason of retracted items\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "crossref_retraction= []\n",
    "base_url = \"https://api.crossref.org\"\n",
    "\n",
    "for crossref_doi_batch in crossref_dois_chunks[:]:\n",
    "\n",
    "    for doi in tqdm(crossref_doi_batch):\n",
    "        url = f\"{base_url}/works/{doi}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "#             print(data)\n",
    "\n",
    "            error_type, retractionyear= '',0\n",
    "            \n",
    "            try:\n",
    "                if data['message']['update-to']:\n",
    "                    try:\n",
    "                        if data['message']['update-to'][0]['type']:\n",
    "                            error_type= data['message']['update-to'][0]['type']\n",
    "                    except Exception as e: \n",
    "                        print(f'error at {doi}  at index: {c_doi}. Error: {e}')\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        if  data['message']['update-to'][0]['updated']['date-parts'][0]:\n",
    "                            retractionyear=  data['message']['update-to'][0]['updated']['date-parts'][0]   \n",
    "                    except Exception as e: \n",
    "                        print(f'error at {doi}  at index: {c_doi}. Error: {e}')\n",
    "                        continue\n",
    "                        \n",
    "                    crossref_retraction.append([doi,retractionyear,error_type])\n",
    "  \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        time.sleep(.10)\n",
    "    \n",
    "                \n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf787f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merger data from Crossref API and save the file\n",
    "\n",
    "ToDo:\n",
    "Uncomment (line 15):\" crossref_retractionyear.to_csv...\" to save the file to drive\n",
    "\"\"\"\n",
    "# If running in multiple batches, concatenate the batches\n",
    "# crossref_retractionyear= pd.concat([cross_tempo, crossref_retraction], axis=0, ignore_index=True).rename(\n",
    "#     columns={0:'DOI', 1: 'RetractionYear', 2:'Reason'})\n",
    "\n",
    "crossref_retractionyear= pd.DataFrame(crossref_retraction).rename(\n",
    "    columns={0:'DOI', 1: 'RetractionYear', 2:'Reason'})\n",
    "crossref_retractionyear['RetractionYear']= crossref_retractionyear['RetractionYear'].apply(lambda x: int(x[0]))\n",
    "\n",
    "crossref_retractionyear \\\n",
    "#    .to_csv(data_dir+'crossref/crossref_retractionyear_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c358ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load already-gotten retraction year data from Crossref in the  above cell\n",
    "\"\"\"\n",
    "crossref_retractionyear= pd.read_csv(data_dir+'crossref/crossref_retractionyear_only.csv').drop(['Unnamed: 0'],axis=1)\n",
    "crossref_retractionyear.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237893bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merging RetractionYear to Crossref retracted items\n",
    "\n",
    "ToDo:\n",
    "Uncomment (line 12): \".to_csv(data_dir....\" to save the complete Crossref data with retraction year\n",
    "\"\"\"\n",
    "\n",
    "crossref_updated= pd.merge(crossref,crossref_retractionyear[['DOI','RetractionYear','Reason']], on='DOI', how='left')\n",
    "\n",
    "crossref_updated['RetractionYear']= crossref_updated['RetractionYear'].fillna(0).astype(int)\n",
    "crossref_updated \\\n",
    "#    .to_csv(data_dir+f'crossref/crossref_retractionyear_'+getdate['crossref']+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53840179",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossref_updated.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420884d",
   "metadata": {},
   "source": [
    "# Retraction Watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07550332",
   "metadata": {},
   "outputs": [],
   "source": [
    "retractionwatch = pd.read_csv(data_dir+f\"retractionwatch/retractionwatch_{getdate['retractionwatch']}.csv\",\\\n",
    "                             encoding='latin1').rename(\n",
    "    columns={'OriginalPaperDOI':'DOI', \n",
    "             'OriginalPaperPubMedID': 'PubMedID', \n",
    "             'OriginalPaperDate': 'Year'})\n",
    "\n",
    "retractionwatch['PubMedID']= retractionwatch['PubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str).str.strip()\n",
    "\n",
    "retractionwatch['RetractionPubMedID']= retractionwatch['RetractionPubMedID'].fillna(0).astype(int)\\\n",
    "                .replace(0,'').astype(str)\n",
    "\n",
    "retractionwatch['Year']=  pd.to_datetime(retractionwatch['Year'], exact=False).dt.year\n",
    "retractionwatch['RetractionYear']=  pd.to_datetime(retractionwatch['RetractionDate'], exact=False).dt.strftime(\"%Y\").fillna(0).astype(int)\n",
    "\n",
    "retractionwatch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4495fe",
   "metadata": {},
   "source": [
    "### Update the Unionlist with RetractionYear Values from Data Sources: \n",
    "####   Retraction Watch, PubMed, BCI, BIOABS, CCC, Web of Science Core, and Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Unionlist_retractionyear(df_source: pd.DataFrame, col_name: str):\n",
    "    \"\"\"\n",
    "    It updates the unionlist with retraction year from PubMed, BCI, BIOABS, Web of Science Core, \n",
    "    and Crossref. Retraction Watch does not use this function.\n",
    "    \n",
    "    :param df_source: The DataFrame to use to update unionlist with Retractionyear\n",
    "    :param col_name: the column ID to use as reference i.e PubMedID/DOI\n",
    "    :return: the updated unionlist with retraction year assigned\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    unionlist_updated['RetractionYear']= unionlist_updated['RetractionYear'].fillna(0).astype(int)\n",
    "\n",
    "    for index, row in unionlist_updated.iterrows():\n",
    "        doi,pmid,retractionyear= row[0].strip(), row[6].strip(), row[7]\n",
    "        \n",
    "        if retractionyear == 0:\n",
    "            if col_name=='PubMedID': \n",
    "                if pmid in df_source['PubMedID'].values: \n",
    "                    df_source_retractionyear = list(df_source.loc[df_source['PubMedID'] == pmid, 'RetractionYear'])[0]\n",
    "                    unionlist_updated.at[index, 'RetractionYear'] = df_source_retractionyear\n",
    "            \n",
    "            if col_name=='DOI': \n",
    "#                 print('Here')\n",
    "                \n",
    "                if doi in df_source['DOI'].values: \n",
    "#                     print('Y')\n",
    "#                     print(doi,pmid,retractionyear)\n",
    "\n",
    "                    df_source_retractionyear = list(df_source.loc[df_source['DOI'] == doi, 'RetractionYear'])[0]\n",
    "#                     print(df_source_retractionyear,doi)\n",
    "                    \n",
    "                    unionlist_updated.at[index, 'RetractionYear'] = df_source_retractionyear\n",
    "\n",
    "\n",
    "    without_rn_year = len(unionlist_updated[unionlist_updated['RetractionYear']==0])\n",
    "  \n",
    "\n",
    "    print(f'While the total items with no retraction year is {without_rn_year} in the unionlist')\n",
    "\n",
    "    return unionlist_updated    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba87c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading the unionlist\n",
    "\"\"\"\n",
    "unionlist= pd.read_csv(data_dir +'unionlist/unionlist_with_nodoi_'+getdate['unionlist']+'.csv', encoding='utf-8').drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# convert String type\n",
    "unionlist['PubMedID']= unionlist['PubMedID'].fillna(0).astype(int).replace(0,'').astype(str)\n",
    "\n",
    "unionlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21875ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confirm no duplicate DOIs in unionlist\n",
    "\"\"\"\n",
    "len(set(unionlist['DOI']))\n",
    "\n",
    "unionlist[unionlist['DOI'].duplicated('keep'==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aeda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First Pass:\n",
    "Updating the unionlist with RetractionYear from Retraction Watch for items with no retraction year\n",
    "\"\"\"\n",
    "\n",
    "rw_deduplicated= retractionwatch.drop_duplicates(subset=['DOI'], keep='last')\n",
    "\n",
    "unionlist_updated = pd.merge(unionlist,rw_deduplicated[['DOI','RetractionYear']], on='DOI', how='left')\n",
    "\n",
    "with_rn_year1= len(unionlist_updated[~unionlist_updated['RetractionYear'].isna()])\n",
    "without_rn_year1= len(unionlist_updated[unionlist_updated['RetractionYear'].isna()])\n",
    "\n",
    "unionlist_updated['RetractionYear']= unionlist_updated['RetractionYear'].fillna(0.0).astype(int)\n",
    "\n",
    "print(f'In First Pass: \\nUsing Retraction Watch: the total items with retraction year is {with_rn_year1} ')\n",
    "\n",
    "print(f'While the total items with no retraction year in the unionlist is {without_rn_year1}  ')\n",
    "\n",
    "unionlist_updated#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ddd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Second Pass:\n",
    "Updating the unionlist with RetractionYear from PubMed for items with no retraction year after first pass\n",
    "\"\"\"\n",
    "print(f'In Second Pass: Update the remaining items in the Unionlist with retraction year from PubMed')\n",
    "\n",
    "update_Unionlist_retractionyear(pubmed_updated,'PubMedID') #.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4295ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Third Pass:\n",
    "Updating the unionlist with RetractionYear from BCI for items with no retraction year after second pass\n",
    "\"\"\"\n",
    "print(f'In Third Pass: Update the remaining items in the Unionlist with retraction year from BCI')\n",
    "\n",
    "update_Unionlist_retractionyear(bci_updated,'DOI') #.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fourth Pass:\n",
    "Updating the unionlist with RetractionYear from BIOABS for items with no retraction year after third pass\n",
    "\"\"\"\n",
    "print(f'In Fourth Pass: Update the remaining items in the Unionlist with retraction year from BIOABS')\n",
    "\n",
    "update_Unionlist_retractionyear(bioabs_updated,'DOI') #.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03eeb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fifth Pass:\n",
    "Updating the unionlist with RetractionYear from CCC for items with no retraction year after fourth pass\n",
    "\"\"\"\n",
    "print(f'In Fifth Pass: Update the remaining items in the Unionlist with retraction year from CCC')\n",
    "\n",
    "update_Unionlist_retractionyear(ccc_updated,'DOI') #.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sixth Pass:\n",
    "Updating the unionlist with RetractionYear from Web of Science Core for items with no retraction year after fifth pass\n",
    "\"\"\"\n",
    "print(f'In Sixth Pass: Update the remaining items in the Unionlist with retraction year from Web of Science Core')\n",
    "\n",
    "update_Unionlist_retractionyear(woscore_updated,'DOI') #.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35bef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Seventh Pass:\n",
    "Updating the unionlist with RetractionYear from Crossref for items with no retraction year after sixth pass\n",
    "\"\"\"\n",
    "print(f'In Seventh Pass: Update the remaining items in the Unionlist with retraction year from Crossref')\n",
    "\n",
    "update_Unionlist_retractionyear(crossref_updated,'DOI')#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save The update Unionlist with RetractionYear\n",
    "\n",
    "ToDo:\n",
    "Uncomment (line 8): \".to_csv(data_dir+'unionlist/....\" to save items in union list with retraction year\n",
    "\"\"\"\n",
    "unionlist_updated\\\n",
    "#     .to_csv(data_dir+'unionlist/unionlist_with_retractionyear_'+getdate['unionlist']+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31528d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
