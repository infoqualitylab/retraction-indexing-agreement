{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Collect Items that are Not Indexed as Retracted in the Sources\n",
    "\n",
    "Publications on the Known Retraction List were found in 1-4 sources from step 1 (Crossref, Retraction Watch, Scopus, WOS). For publications found in fewer than 4 sources, we need to distinguish two situations: (1) the publication is not covered by a given source (e.g., no record of the publication exists in that source); and (2) the publication is covered but not listed as retracted in a given source (e.g., record exists but is not retracted in that source). To do this we use the Crossref, Scopus, and WOS APIs (no need for Retraction Watch because everything they cover is retracted) to find out whether a record exists in a source. We export the list of items covered but not indexed as retracted as the output of the code notebook. \n",
    "\n",
    "We split each function into three parts:\n",
    "- Part 1: find retractions that are not covered in each source (Crossref, Scopus, WOS) \n",
    "- Part 2: test individual APIs using a small set of dataset from each source\n",
    "- Part 3: run individual APIs to identify if the records exist in each sources based on their DOIs\n",
    "- Part 4: return and export the records that exist in each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install elsapy\n",
    "!pip install crossrefapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from crossref.restful import Works, Etiquette\n",
    "my_etiquette = Etiquette('My Project Name', 'My Project version', 'My Project URL', 'My contact email')\n",
    "works = Works(etiquette=my_etiquette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "today = str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set path --- Link to the box folder with your name\n",
    "# Download Box Desktop to copy the pathname\n",
    "\n",
    "# Input\n",
    "# Folder name: step1-outputfile\n",
    "box_path_1 = {enterdirectorytofolder}\n",
    "\n",
    "# Output\n",
    "# Folder name: step2-outputfile\n",
    "box_path_2 = {enterdirectorytofolder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input File: One CSV file of the known retraction list \n",
    "knownretraction = pd.read_csv(box_path_1 + '2023-04-05-knownretractionlist.csv').drop(['Unnamed: 0'], axis=1).sort_values('Year', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(knownretraction.shape())\n",
    "print(knownretraction.info())\n",
    "knownretraction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# find retractions that are not covered in each source (Crossref, Scopus, WOS)\n",
    "\n",
    "sp = knownretraction[~knownretraction['source'].str.contains('Scopus', na=False)]\n",
    "doilist_sp = sp['DOI']\n",
    "\n",
    "wos = knownretraction[~knownretraction['source'].str.contains('Web of Science', na=False)]\n",
    "doilist_wos = wos['DOI']\n",
    "\n",
    "cr = knownretraction[~knownretraction['source'].str.contains('Crossref', na=False)]\n",
    "doilist_cr = cr['DOI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# test APIs using a small set of dataset \n",
    "\n",
    "new = Works()\n",
    "\n",
    "testing_start_sp = timer()\n",
    "\n",
    "for i in doilist_cr[0:10]:\n",
    "    try:\n",
    "        \n",
    "        for j in new.filter(doi = i).select('DOI'):\n",
    "            find = j['DOI']\n",
    "            if i == find:\n",
    "                print(i)\n",
    "\n",
    "    except:\n",
    "        print('error')\n",
    "\n",
    "testing_end_sp = timer()\n",
    "print(testing_end_sp - testing_start_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 3 -- do not run\n",
    "# run APIs to find out whether a record exists in a source\n",
    "\n",
    "included_cr=[]\n",
    "\n",
    "start_cr = timer()\n",
    "\n",
    "\n",
    "for i in doilist_cr:\n",
    "    try:\n",
    "        for j in new.filter(doi = i).select('DOI'):\n",
    "            find = j['DOI']\n",
    "            if i == find:\n",
    "                included_cr.append(i)\n",
    "\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "        \n",
    "end_cr = timer()\n",
    "print(end_cr - start_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# test APIs using a small set of dataset\n",
    "\n",
    "testing_start_sp = timer()\n",
    "\n",
    "\n",
    "for i in doilist_sp[0:10]:\n",
    "    try:\n",
    "        resp = requests.get(\"https://api.elsevier.com/content/abstract/doi/\" + i + \"?field=doi\",\n",
    "                            headers={'Accept':'application/json', \n",
    "                                     'X-ELS-APIKey': {enterapikey}})\n",
    "\n",
    "        results = resp.json()\n",
    "        #response = requests.get(resp)\n",
    "        print(results.get('abstracts-retrieval-response').get('coredata').get('prism:doi'))\n",
    "    \n",
    "    except:\n",
    "        print('pass')\n",
    "        #print(i, results, resp.headers)    \n",
    "\n",
    "        \n",
    "testing_end_sp = timer()\n",
    "print(testing_end_sp - testing_start_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 3 -- do not run\n",
    "# run APIs to find out whether a record exists in a source\n",
    "\n",
    "included_sp =[]\n",
    "\n",
    "start_sp = timer()\n",
    "\n",
    "\n",
    "for i in doilist_sp:\n",
    "    try:\n",
    "        resp = requests.get(\"https://api.elsevier.com/content/abstract/doi/\" + i + \"?field=doi\",\n",
    "                            headers={'Accept':'application/json', \n",
    "                                     'X-ELS-APIKey': {enterapikey}})\n",
    "\n",
    "        results = resp.json()\n",
    "        results.get('abstracts-retrieval-response').get('coredata').get('prism:doi')\n",
    "        included_sp.append(i)\n",
    "\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "        \n",
    "end_sp = timer()\n",
    "print(end_sp - start_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web of Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# test APIs using a small set of dataset \n",
    "\n",
    "headers = {'X-APIKey': {enterapikey}}\n",
    "baseurl = \"https://wos-api.clarivate.com/api/wos\"  # this is the base URL for WoS Expanded API\n",
    "\n",
    "testing_start_sp = timer()\n",
    "\n",
    "\n",
    "for i in doilist_wos[0:10]:\n",
    "\n",
    "    search_query = 'DO=(\"' + i + '\")'\n",
    "\n",
    "    try:\n",
    "        \n",
    "        initial_response = requests.get(\n",
    "            'https://wos-api.clarivate.com/api/wos?databaseId=WOS&usrQuery=' + search_query + '&count=0&firstRecord=1',\n",
    "            headers=headers)\n",
    "\n",
    "        data = initial_response.json()\n",
    "\n",
    "        if data['QueryResult']['RecordsFound'] == 1: \n",
    "            print(i)   \n",
    "\n",
    "    except:\n",
    "        print(data)   \n",
    "    \n",
    "    \n",
    "testing_end_sp = timer()\n",
    "print(testing_end_sp - testing_start_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 3 -- do not run\n",
    "# run APIs to find out whether a record exists in a source\n",
    "\n",
    "included_wos =[]\n",
    "\n",
    "start_wos = timer()\n",
    "\n",
    "\n",
    "for i in doilist_wos:\n",
    "    \n",
    "    search_query = 'DO=(\"' + i + '\")'\n",
    "    \n",
    "    try:\n",
    "\n",
    "        initial_response = requests.get(\n",
    "            'https://wos-api.clarivate.com/api/wos?databaseId=WOS&usrQuery=' + search_query + '&count=0&firstRecord=1',\n",
    "            headers=headers)\n",
    "\n",
    "        data = initial_response.json()\n",
    "\n",
    "        if data['QueryResult']['RecordsFound'] == 1: \n",
    "            included_wos.append(i)\n",
    "\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "        \n",
    "end_wos = timer()\n",
    "print(end_wos - start_wos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 4 -- do not run\n",
    "# return the retracted items are found in each source\n",
    "\n",
    "# Output File: Three CSV files (one for each source) \n",
    "# of the items from the known retraction list \n",
    "# that are covered by a given source but not indexed as retracted in that source \n",
    "\n",
    "\n",
    "# Crossref\n",
    "included_cr_df = pd.DataFrame()\n",
    "\n",
    "for i in included_cr:\n",
    "    included_cr_df = pd.concat([cr[cr['DOI']== i], included_cr_df.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "if len(included_cr) == len(included_cr_df):\n",
    "    included_cr_df.to_csv(box_path_2 + today + 'notindexedasretracted-crossref.csv')\n",
    "    print(len(included_cr))\n",
    "\n",
    "else:\n",
    "    print('error: wrong length' + len(included_wos),len(included_wos_df))   \n",
    "    \n",
    "    \n",
    "# Scopus\n",
    "included_sp_df = pd.DataFrame()\n",
    "\n",
    "for i in included_sp:\n",
    "    included_sp_df = pd.concat([sp[sp['DOI']== i], included_sp_df.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "if len(included_sp) == len(included_sp_df):\n",
    "    print(len(included_sp))\n",
    "    included_sp_df.to_csv(box_path_2 + today + '-notindexedasretracted-scopus.csv')\n",
    "\n",
    "else:\n",
    "    print('error: wrong length' + len(included_sp),len(included_sp_df))\n",
    "\n",
    "    \n",
    "# Web of science\n",
    "included_wos_df = pd.DataFrame()\n",
    "\n",
    "for i in included_wos:\n",
    "    included_wos_df = pd.concat([wos[wos['DOI']== i], included_wos_df.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "if len(included_wos) == len(included_wos_df):\n",
    "    included_wos_df.to_csv(box_path_2 + today + '-notindexedasretracted-webofscience.csv')\n",
    "    print(len(included_wos))\n",
    "\n",
    "else:\n",
    "    print('error: wrong length' + len(included_wos),len(included_wos_df))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
