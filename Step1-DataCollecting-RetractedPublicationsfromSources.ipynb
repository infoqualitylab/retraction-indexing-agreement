{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import and Clean Dataset from Sources\n",
    "We import retraction data that we collected via queries from Crossref, Scopus, and WOS, and received directly from Retraction Watch. The code cleans and deduplicates the records. Then the final output of this code notebook is one merged file called the \"Known Retraction List\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install crossrefapi\n",
    "!pip install --upgrade xlrd\n",
    "!pip install openpyxl\n",
    "!pip install dataframe-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Crossref API\n",
    "\n",
    "from crossref.restful import Works, Etiquette\n",
    "my_etiquette = Etiquette('My Project Name', 'My Project version', 'My Project URL', 'My contact email')\n",
    "print(str(my_etiquette))\n",
    "\n",
    "works = Works(etiquette=my_etiquette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "\n",
    "from datetime import date\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path --- Link to the box folder with your name\n",
    "# Download Box Desktop to copy the pathname\n",
    "\n",
    "# Input\n",
    "# Folder name: step1-inputfile\n",
    "box_path_1 = {enterdirectorytofolder}\n",
    "\n",
    "# Output\n",
    "# Folder name: step1-outputfile\n",
    "box_path_2 = {enterdirectorytofolder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Merge multiple downloaded files from the same source (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Files: All downloaded files from the same source  \n",
    "\n",
    "# Scopus\n",
    "file_list = []\n",
    "\n",
    "for file_num in range(1,3):\n",
    "    path = box_path_1 + \"scopus (\" + str(file_num) + \").csv\"\n",
    "    file_list.append(path)\n",
    "\n",
    "    \n",
    "sp = pd.DataFrame()\n",
    "\n",
    "for file_name in file_list:\n",
    "    sp = sp.append(pd.read_csv(file_name), ignore_index=True)\n",
    "\n",
    "    \n",
    "# Output File: One merged CSV file for a source \n",
    "sp.to_csv(box_path_1 + today +'-scopus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web of Science\n",
    "\n",
    "# specify the path\n",
    "file_list = []\n",
    "\n",
    "for file_num in range(9,26):\n",
    "    path = box_path_1 + 'WOS/savedrecs(' + str(file_num) + ').xls'\n",
    "    file_list.append(path)\n",
    "\n",
    "wos = pd.DataFrame()\n",
    "\n",
    "for file_name in file_list:\n",
    "    wos = wos.append(pd.read_excel(file_name), ignore_index=True)\n",
    "\n",
    "wos['Database'] = 'Web of Science'\n",
    "\n",
    "# Output File: One merged CSV file for a source \n",
    "wos.to_csv(box_path_1 + today + '-webofscience.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect retraction data from Crossref\n",
    "\n",
    "- Get the count of each document type and identify which categories are related to retracted publication\n",
    "- Get the count of document type that retracted publication\n",
    "- Collect data using crossrefapi\n",
    "- Clean dataframe and export into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check document type\n",
    "# Find all retraction indexing wordings\n",
    "\n",
    "work = Works()\n",
    "\n",
    "update_type = works.facet('update-type')\n",
    "update_type_df = pd.DataFrame(update_type['update-type']['values'], index=['count']).transpose().reset_index().rename(columns={'index':'document_type'})\n",
    "\n",
    "update_type_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of retraction document type\n",
    "\n",
    "update_type_retraction = update_type_df.loc[update_type_df['document_type'].isin(['retraction','Retraction', 'retracion', 'retration', 'withdrawal','removal', 'partial_retraction'])].set_index(['document_type'])\n",
    "\n",
    "\n",
    "retracted_tags = update_type_retraction.index.to_list()\n",
    "\n",
    "update_type_retraction.loc['Total Count'] = update_type_retraction['count'].sum()\n",
    "update_type_retraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function to collect data from crossrefapi\n",
    "\n",
    "def getmetadata(x):\n",
    "        \n",
    "    \"\"\"\n",
    "    This function fetches publication data from the Crossref API based on document type.\n",
    "    Data includes DOI, publicatio date, author, titles, and URL of the publication.\n",
    "    \n",
    "    :param x: specify the document type e.g. 'retraction', 'withdrawal'\n",
    "    :return: dataframe of specific records\n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer() # set timer\n",
    "\n",
    "    metadata = {'DOI':[],'issued':[],'URL':[],'title':[],'author':[],'container-title':[]}\n",
    "    \n",
    "    for i in work.filter(update_type = x).select('DOI','issued','URL','title','author','container-title'):\n",
    "        metadata['DOI'].append(i['DOI'])\n",
    "        metadata['issued'].append(i['issued'])\n",
    "        metadata['URL'].append(i['URL'])      \n",
    "        try:\n",
    "            metadata['author'].append(i['author'])\n",
    "            \n",
    "        except:\n",
    "            metadata['author'].append('null')\n",
    "            \n",
    "        metadata['title'].append(i['title'])\n",
    "        metadata['container-title'].append(i['container-title'])\n",
    "     \n",
    "    end = timer()\n",
    "    print(end - start)\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metadate from Crossref\n",
    "\n",
    "crossref_df= pd.DataFrame()\n",
    "\n",
    "for x in update_type_retraction:\n",
    "    crossref_df = crossref_df.append(getmetadata(x), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataframe\n",
    "\n",
    "crossref_df = crossref_df.rename(columns={'issued':'Year', 'container-title': 'Journal','title': 'Title', 'author': 'Author'})\n",
    "\n",
    "year=[]\n",
    "title=[]\n",
    "journal=[]\n",
    "author=[]\n",
    "\n",
    "for i in range(len(crossref_df)):\n",
    "    year.append(crossref_df['Year'][i]['date-parts'][0][0])\n",
    "    title.append(crossref_df['Title'][i][0])\n",
    "    journal.append(crossref_df['Journal'][i][0])\n",
    "    author.append(crossref_df['Author'][i])\n",
    "\n",
    "    \n",
    "crossref_df['Year'] = year\n",
    "crossref_df['Title'] = title\n",
    "crossref_df['Journal'] = journal\n",
    "crossref_df['Author'] = author\n",
    "\n",
    "crossref_df.Year.unique()\n",
    "\n",
    "crossref_df.fillna(0)\n",
    "crossref_df.replace('null','')\n",
    "\n",
    "\n",
    "# Output File: One CSV files listing retracted publication\n",
    "crossref_df.to_csv(box_path_1 + today + '-crossref.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get additional data from Crossref -- Category\n",
    "\n",
    "url_base = 'https://api.crossref.org/works/{}'\n",
    "\n",
    "crossref_cat = crossref_retracted['DOI'].tolist()\n",
    "crossref_cat_chunks = np.array_split(crossref_cat, 147)\n",
    "\n",
    "\n",
    "output = box_path_2 + 'crossrefcategory_2.tsv'\n",
    "with open(output, 'a') as fout:\n",
    "    line = ['DOI', 'Category']\n",
    "    line = '\\t'.join(line) + '\\n'\n",
    "    fout.write(line)\n",
    "    \n",
    "for chunk in crossref_cat_chunks:\n",
    "    start_time = time.time()\n",
    "    for doi in chunk:\n",
    "        try:\n",
    "            url = url_base.format(doi)\n",
    "            result = requests.get(url)\n",
    "            result = result.json()\n",
    "\n",
    "            cat = result['message']['subject']\n",
    "\n",
    "            new_line = [doi, str(cat)]\n",
    "            new_line = '\\t'.join(new_line) + '\\n'\n",
    "            with open(output, 'a') as fout:\n",
    "                fout.write(new_line)\n",
    "        except:\n",
    "            print(doi)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "    process_time = end_time - start_time\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "print(process_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossrefcat = pd.read_table(box_path_2 + 'crossrefcategory_2.tsv').fillna('')\n",
    "crossrefcat['Category_str'] = [''.join(map(str, l)) for l in crossrefcat['Category']]\n",
    "\n",
    "crossrefcat['Category_str'] = crossrefcat.Category_str.str.replace('[','').str.replace(']','')\n",
    "crossrefcat.Category_str.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean and deduplicate retraction data of each source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imput Files: Four files (one from each source) listing retracted publication.\n",
    "\n",
    "\n",
    "# Crossref\n",
    "crossref = pd.read_csv(box_path_1 + '2023-04-05-crossref.csv')\n",
    "crossref['source'] = 'Crossref'\n",
    "\n",
    "print(crossref.shape)\n",
    "print(crossref.info())\n",
    "print(crossref.head())\n",
    "               \n",
    "               \n",
    "# Retraction Watch\n",
    "retractionwatch = pd.read_excel(box_path_1 + '2023-03-27-RetractionWatch.xlsx', engine='openpyxl').rename(\n",
    "    columns={'OriginalPaperDOI':'DOI', \n",
    "             'OriginalPaperPubMedID': 'PubMedID', \n",
    "             'OriginalPaperDate': 'Year'})\n",
    "\n",
    "retractionwatch['source'] = 'Retraction Watch'\n",
    "\n",
    "retractionwatch['PubMedID'] = pd.to_numeric(retractionwatch['PubMedID']).fillna(0).astype(str)\n",
    "retractionwatch[\"Year\"] = pd.to_datetime(retractionwatch[\"Year\"]).dt.strftime(\"%Y\").fillna(0).astype(int)\n",
    "retractionwatch['RetractionYear'] = pd.to_datetime(date['RetractionDate']).dt.strftime(\"%Y\").fillna(0).astype(int)\n",
    "retractionwatch['DOI'] = retractionwatch['DOI'].fillna(0)\n",
    "\n",
    "print(retractionwatch.shape)\n",
    "print(retractionwatch.info())\n",
    "print(retractionwatch.head())\n",
    "\n",
    "\n",
    "# Scopus\n",
    "scopus = pd.read_csv(box_path_1 + '2023-04-05-scopus.csv').rename(\n",
    "    columns={'Authors':'Author',\n",
    "             'Source':'source',\n",
    "             'Titles':'Title',\n",
    "             'Source title':'Journal',\n",
    "             'PubMed ID': 'PubMedID'})\n",
    "\n",
    "scopus['Year'] = pd.to_numeric(scopus['Year']).fillna(0).astype(int)\n",
    "scopus['PubMedID'] = pd.to_numeric(scopus['PubMedID']).fillna(0).astype(str)\n",
    "\n",
    "\n",
    "print(scopus.shape)\n",
    "print(scopus.info())\n",
    "print(scopus.head())\n",
    "\n",
    "               \n",
    "# Web of Science\n",
    "wos = pd.read_csv(box_path_1 + '2023-04-05-webofscience.csv').rename(\n",
    "    columns={'Authors':'Author', \n",
    "             'Article Title': 'Title', \n",
    "             'Source Title': 'Journal', \n",
    "             'Publication Year': 'Year', \n",
    "             'Pubmed Id': 'PubMedID'})\n",
    "\n",
    "wos['PubMedID'] = pd.to_numeric(wos['PubMedID']).fillna(0).astype(str)\n",
    "wos['source'] = 'Web of Science'\n",
    "\n",
    "print(wos.shape)\n",
    "print(wos.info())\n",
    "print(wos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to clean and deduplicate retraction data of each source\n",
    "\n",
    "def checkindividualdataset(x):\n",
    "    \n",
    "    # Input dataframe names as x\n",
    "    \n",
    "    '''\n",
    "    Clean and deplicate records based on DOIs.\n",
    "    After removing duplicates, we will return the count and the list of records with DOI, \n",
    "    those without DOIs and duplicated records that will be dropped.\n",
    "    '''\n",
    "    \n",
    "    # Step 1: We identify the unique records of each dataset based on DOI.\n",
    "    # 'records_withDOI_hasdup': Identify records that have a valid DOI which should start with '10.'\n",
    "    # 'records_withDOI': Drop the duplicates from the previous line.\n",
    "    records_withDOI_hasdup= x.loc[x['DOI'].str.startswith('10.', na=False)]\n",
    "    records_withDOI = records_withDOI_hasdup.drop_duplicates(subset=['DOI'], keep='first')\n",
    "\n",
    "    \n",
    "    # Step 2: We create two duplicate lists.\n",
    "    # 'duplicated_records_all': Identify ALL duplicated records for reference and download for checking manually.\n",
    "    # 'duplicated_records': Identify duplicated records to drop but keep only the first occurrence of each group of duplicates.\n",
    "    duplicate_records_all = records_withDOI_hasdup.loc[records_withDOI_hasdup.duplicated(subset=['DOI'],keep=False), :]\n",
    "    duplicate_records = records_withDOI_hasdup.loc[records_withDOI_hasdup.duplicated(subset=['DOI'],keep='first'), :]\n",
    "\n",
    "    \n",
    "    # Step 3: We get the count of records without DOI. Duplicates may exist since we could not use DOI to identify duplicate records\n",
    "    records_withoutDOI = x.loc[~x['DOI'].str.startswith('10.', na=False)]    \n",
    "\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if len(x) == len(records_withDOI)+len(records_withoutDOI)+len(duplicate_records):\n",
    "            return[len(x), len(records_withDOI),len(records_withoutDOI),len(duplicate_records), records_withDOI, records_withoutDOI, duplicate_records, duplicate_records_all]\n",
    "            # return the count and items of each group\n",
    "    \n",
    "    except: \n",
    "        return('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function checkindividualdataset(x) \n",
    "# Create two lists to store the count and items\n",
    "\n",
    "dbtable = [] # A nested list which stores the records of each group in each source\n",
    "ovtable = [] # Store the count of each group from each source and create a table for viewing\n",
    "dblist = [scopus, wos, retractionwatch, crossref]\n",
    "\n",
    "for x in dblist:\n",
    "    dbtable.append(checkindividualdataset(x))\n",
    "\n",
    "for x in range(0,len(dbtable)):\n",
    "    ovtable.append(dbtable[x][0:4])\n",
    "\n",
    "\n",
    "# Create a table showing the count of each group\n",
    "overview = pd.DataFrame(ovtable)\n",
    "overview.columns =['total', 'records_withDOI', 'records_withoutDOI', 'duplicate_records']\n",
    "overview['source']= ['Scopus','Web of Science', 'Retraction Watch', 'Crossref']\n",
    "overview.loc[len(overview)] = [overview.total.sum(), overview.records_withDOI.sum(), overview.records_withoutDOI.sum(), overview.duplicate_records.sum(),''] \n",
    "\n",
    "overview.insert(0, 'source', overview.pop('source'))\n",
    "overview['source'][4]= 'Total'\n",
    "\n",
    "print(overview)\n",
    "\n",
    "\n",
    "## export table to png\n",
    "dfi.export(overview, 'source_overview.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the nested list 'dblist' and export each group in each source individually.\n",
    "\n",
    "def exportthreedatasets(x):\n",
    "    \n",
    "    if x == 'scopus':\n",
    "        dbtable[0][4].sort_values(by=['DOI'], ascending=False).to_csv(box_path_2 + today + '-recordswithdoi-' + x + '.csv')\n",
    "        dbtable[0][5].to_csv(box_path_2 + today + '-recordsnodoi-' + x + '.csv')\n",
    "        dbtable[0][7].sort_values(by=['DOI'], ascending=False).to_csv(box_path_2 + today + '-duplicatedrecords-' + x + '.csv')\n",
    "\n",
    "    if x == 'wos':\n",
    "        dbtable[1][4].sort_values(by=['DOI'], ascending=False).to_csv(box_path_2 + today + '-recordswithdoi-' + x + '.csv')\n",
    "        dbtable[1][5].to_csv(box_path_2 + today + '-recordsnodoi-' + x + '.csv')\n",
    "        dbtable[1][7].sort_values(by=['DOI'], ascending=False).to_csv(box_path_2 + today + '-duplicatedrecords-' + x + '.csv')\n",
    "\n",
    "    if x == 'retractionwatch':\n",
    "        dbtable[2][4].sort_values(by=['DOI'], ascending=False).to_csv(box_path_2 + today + '-recordswithdoi-' + x + '.csv')\n",
    "        dbtable[2][5].to_csv(box_path_2 + today + '-recordsnodoi-' + x + '.csv')\n",
    "        dbtable[2][7].sort_values(by=['DOI'], ascending=False).to_csv(box_path_2 + today + '-duplicatedrecords-' + x + '.csv')\n",
    "    \n",
    "    if x == 'crossref':\n",
    "        dbtable[3][4].sort_values(by=['DOI'], ascending=False).to_csv(box_path_2 + today + '-recordswithdoi-' + x + '.csv')\n",
    "        dbtable[3][5].to_csv(box_path_2 + today + '-recordsnodoi-' + x + '.csv')\n",
    "        dbtable[3][7].sort_values(by=['DOI'], ascending=False).to_csv(box_path_2 + today + '-duplicatedrecords-' + x + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Files: Three CSV files of cleaned data for each source listing:\n",
    "# the records with DOI\n",
    "# the records with no DOI\n",
    "# all the duplicate records\n",
    "\n",
    "for x in ['wos', 'scopus', 'retractionwatch', 'crossref']:\n",
    "    exportthreedatasets(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a known retraction list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Files: Four CSV files (one for each source) of the records with DOI. (Produced in step #3 above) \n",
    "\n",
    "scopus_retracted = pd.read_csv(box_path_2 + '2023-04-09-recordswithdoi-scopus.csv')\n",
    "wos_retracted = pd.read_csv(box_path_2 + '2023-04-09-recordswithdoi-wos.csv')\n",
    "retractionwatch_retracted = pd.read_csv(box_path_2 + '2023-04-09-recordswithdoi-retractionwatch.csv')\n",
    "crossref_retracted = pd.read_csv(box_path_2 + '2023-04-09-recordswithdoi-crossref.csv')\n",
    "\n",
    "print(scopus_retracted[['DOI', 'Author','Title', 'Year', 'Journal', 'source', 'PubMedID']].dtypes)\n",
    "print(wos_retracted[['DOI', 'Author','Title', 'Year', 'Journal', 'source', 'PubMedID']].dtypes)\n",
    "print(retractionwatch_retracted[['DOI', 'Author','Title', 'Year', 'Journal', 'source', 'PubMedID']].dtypes)\n",
    "print(crossref_retracted[['DOI', 'Author','Title', 'Year', 'Journal', 'source']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged into one full list 'The Known Retraction List'\n",
    "\n",
    "merged_withdoi = pd.concat([wos_retracted, scopus_retracted, retractionwatch_retracted, crossref_retracted])\n",
    "merged_withdoi = merged_withdoi[['DOI', 'Author','Title', 'Year', 'Journal', 'source', 'PubMedID']].sort_values(by='DOI')\n",
    "\n",
    "# Check if the number of records are consistent before and after merging.\n",
    "if len(merged_withdoi) == len(wos_retracted)+len(scopus_retracted)+len(retractionwatch_retracted)+len(crossref_retracted): \n",
    "    print('full record count:', len(merged_withdoi))\n",
    "\n",
    "else:\n",
    "    print('ERROR: Inconsistent Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Format the dataframe\n",
    "\n",
    "knownretractionlist = merged_withdoi.groupby('DOI').agg({'Author':'first', \n",
    "                              'Title': 'last',\n",
    "                              'Year': 'first', \n",
    "                              'Journal': 'last',\n",
    "                              'source':'; '.join, \n",
    "                              'PubMedID':'first'}).reset_index()\n",
    "\n",
    "print(knownretractionlist.shape())\n",
    "print(knownretractionlist.info())\n",
    "knownretractionlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output File: One merged CSV file as final output \n",
    "knownretractionlist.to_csv(box_path_2 + today + '-knownretractionlist-0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table showing the number of matched records in each group based on DOI.\n",
    "\n",
    "df = pd.merge(\n",
    "        pd.merge(\n",
    "            pd.merge(\n",
    "            scopus_retracted, wos_retracted,how='inner', on='DOI')\n",
    "            , retractionwatch_retracted, how='inner', on='DOI')\n",
    "            , crossref_retracted, how='inner', on='DOI')\n",
    "\n",
    "df1 = pd.merge(scopus_retracted, wos_retracted,how='inner', on='DOI')\n",
    "df2 = pd.merge(scopus_retracted, retractionwatch_retracted,how='inner', on='DOI')\n",
    "df3 = pd.merge(wos_retracted, retractionwatch_retracted,how='inner', on='DOI')\n",
    "df4 = pd.merge(crossref_retracted, scopus_retracted,how='inner', on='DOI')\n",
    "df5 = pd.merge(crossref_retracted, wos_retracted,how='inner', on='DOI')\n",
    "df6 = pd.merge(crossref_retracted, retractionwatch_retracted,how='inner', on='DOI')\n",
    "\n",
    "\n",
    "crm = [len(crossref_retracted), len(df6), len(df4), len(df5), '-']\n",
    "rwm = ['-', len(retractionwatch_retracted),len(df2), len(df3), '-']\n",
    "spm = ['-', '-', len(scopus_retracted), len(df1), '-']\n",
    "wosm = ['-', '-', '-', len(wos_retracted), '-']\n",
    "\n",
    "allm = ['-','-','-','-', len(df)]\n",
    "\n",
    "mtable = [crm, rwm, spm, wosm, allm]\n",
    "match = pd.DataFrame(mtable)\n",
    "match.columns = ['Crossref','Retraction Watch', 'Scopus', 'Web of Science', 'All Matched']\n",
    "match['source']= ['Crossref','Retraction Watch', 'Scopus','Web of Science', 'All Matched']\n",
    "\n",
    "# shift column 'database' to first position\n",
    "match.insert(0, 'source', match.pop('source'))\n",
    "match = match.set_index(match.columns[0])\n",
    "\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export table\n",
    "dfi.export(match, box_path_1 + today + 'matchingmatrix_overview.png', table_conversion=\"matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Step1.5 Notebook and return to the steps below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add field of study tp each item\n",
    "journallist = pd.read_csv(box_path_1 + '2023-04-13-journalcategory-knownretractionlist.csv').drop(['Unnamed: 0'], axis=1)\n",
    "journallist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knownretractionlist['journal'] = knownretractionlist['Journal'].str.lower()\n",
    "\n",
    "knownretractionlist = knownretractionlist.merge(journallist, left_on='journal', right_on='JournalandConferenceProceedings', how='left').drop(columns=['journal','JournalandConferenceProceedings'], axis=1)\n",
    "\n",
    "print(knownretractionlist.shape())\n",
    "print(knownretractionlist.info())\n",
    "knownretractionlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output File: One merged CSV file as final output \n",
    "\n",
    "knownretractionlist.to_csv(box_path_2 + today + '-knownretractionlist-1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
